2025-09-03 05:30:19,752 - INFO - Connected to database successfully
2025-09-03 05:30:19,752 - INFO - ============================================================
2025-09-03 05:30:19,752 - INFO - STARTING FEATURE ENGINEERING PIPELINE
2025-09-03 05:30:19,752 - INFO - ============================================================
2025-09-03 05:30:19,752 - INFO - Loading telemetry data from 02_raw_telemetry_transformed table...
2025-09-03 05:36:58,909 - INFO - Connected to database with high-performance configuration
2025-09-03 05:36:58,909 - INFO - ======================================================================
2025-09-03 05:36:58,909 - INFO - STARTING HIGH-PERFORMANCE DATABASE-CENTRIC FEATURE ENGINEERING
2025-09-03 05:36:58,909 - INFO - ======================================================================
2025-09-03 05:36:58,909 - INFO - 🚀 Leveraging 100 CPUs and 300GB RAM through PostgreSQL parallel processing
2025-09-03 05:36:58,909 - INFO - 🔗 Using raw_event_hash_id for optimal join performance
2025-09-03 05:36:58,909 - INFO - ✓ Loaded SQL script: generate_features.sql
2025-09-03 05:36:58,909 - INFO - Executing comprehensive feature engineering query...
2025-09-03 05:36:58,909 - INFO - This leverages all available CPU cores and memory for parallel processing
2025-09-03 05:36:58,921 - INFO - ✓ Database session optimized for high-performance processing
2025-09-03 05:36:58,923 - ERROR - SQL execution failed: (psycopg2.errors.WindowingError) window function calls cannot be nested
LINE 77:       ) - LAG(AVG(load_weight) OVER (
                       ^

[SQL: -- High-Performance Feature Engineering Pipeline
-- AHS Analytics Engine - DataMine V2
-- This SQL script generates all features directly in the database using parallel processing
-- Optimized with raw_event_hash_id joins for maximum performance

-- Enable parallel execution and optimize memory
SET max_parallel_workers_per_gather = 8;
SET work_mem = '4GB';
SET effective_cache_size = '200GB';

-- Drop the target table to ensure a fresh build
DROP TABLE IF EXISTS "03_primary_feature_table";

-- Create comprehensive feature table in a single parallel operation
CREATE TABLE "03_primary_feature_table" AS
WITH
  -- Step 1: Base data with geospatial features
  base_features AS (
    SELECT
      t.*,
      -- Extract altitude directly from PostGIS geometry
      ST_Z(t.current_position::geometry) AS altitude,
      -- Simple geospatial classification (can be enhanced with actual operational_zones table)
      CASE
        WHEN ST_Z(t.current_position::geometry) < 200 THEN 'Loading Zone A'
        WHEN ST_Z(t.current_position::geometry) > 250 THEN 'Dump Zone B' 
        ELSE 'Haul Road / Other'
      END AS location_type
    FROM "02_raw_telemetry_transformed" t
    WHERE current_position IS NOT NULL
  ),
  
  -- Step 2: Kinematic features using optimized window functions
  kinematic_features AS (
    SELECT
      *,
      -- A. is_stationary
      (current_speed < 0.5) AS is_stationary,
      
      -- B. altitude_rate_of_change using LAG window function
      altitude - LAG(altitude, 1) OVER (
        PARTITION BY device_id
        ORDER BY timestamp
      ) AS altitude_rate_of_change,
      
      -- C. speed_rolling_avg_5s using optimized window
      AVG(current_speed) OVER (
        PARTITION BY device_id
        ORDER BY timestamp 
        ROWS BETWEEN 2 PRECEDING AND 2 FOLLOWING
      ) AS speed_rolling_avg_5s
    FROM base_features
  ),
  
  -- Step 3: Payload features with smoothing
  payload_features AS (
    SELECT
      *,
      -- load_weight_smoothed using 5-point rolling average
      AVG(load_weight) OVER (
        PARTITION BY device_id
        ORDER BY timestamp 
        ROWS BETWEEN 2 PRECEDING AND 2 FOLLOWING
      ) AS load_weight_smoothed
    FROM kinematic_features
  ),
  
  -- Step 4: Add payload rate of change
  payload_with_rate AS (
    SELECT
      *,
      -- load_weight_rate_of_change on smoothed values
      AVG(load_weight) OVER (
        PARTITION BY device_id
        ORDER BY timestamp 
        ROWS BETWEEN 2 PRECEDING AND 2 FOLLOWING
      ) - LAG(AVG(load_weight) OVER (
        PARTITION BY device_id
        ORDER BY timestamp 
        ROWS BETWEEN 2 PRECEDING AND 2 FOLLOWING
      ), 1) OVER (
        PARTITION BY device_id
        ORDER BY timestamp
      ) AS load_weight_rate_of_change
    FROM payload_features
  ),
  
  -- Step 5: Sensor health flags using device-level aggregation
  sensor_health_features AS (
    SELECT
      *,
      -- has_reliable_payload flag based on load_weight variance
      (STDDEV(load_weight) OVER (PARTITION BY device_id) > 1000) AS has_reliable_payload
    FROM payload_with_rate
  ),
  
  -- Step 6: PRNDL one-hot encoding
  prndl_features AS (
    SELECT
      *,
      (prndl = 'park') AS prndl_park,
      (prndl = 'reverse') AS prndl_reverse, 
      (prndl = 'neutral') AS prndl_neutral,
      (prndl = 'drive') AS prndl_drive,
      (prndl = 'unknown') AS prndl_unknown
    FROM sensor_health_features
  ),
  
  -- Step 7: Duration features - identify stationary state changes
  duration_prep AS (
    SELECT
      *,
      -- Create unique IDs for consecutive stationary blocks
      SUM(CASE
        WHEN is_stationary != LAG(is_stationary, 1, is_stationary) OVER (
          PARTITION BY device_id ORDER BY timestamp
        ) THEN 1
        ELSE 0
      END) OVER (
        PARTITION BY device_id ORDER BY timestamp
      ) AS stationary_block_id,
      
      -- Calculate time delta between consecutive records (in seconds)
      COALESCE(
        EXTRACT(EPOCH FROM (
          timestamp - LAG(timestamp, 1) OVER (
            PARTITION BY device_id ORDER BY timestamp
          )
        )), 0
      ) AS time_delta
    FROM prndl_features
  ),
  
  -- Step 8: Calculate cumulative time within stationary blocks
  final_features AS (
    SELECT
      *,
      -- Cumulative time within each stationary block
      SUM(time_delta) OVER (
        PARTITION BY device_id, stationary_block_id
        ORDER BY timestamp
      ) AS time_in_block
    FROM duration_prep
  )

-- Final selection with all original and engineered features
SELECT
  -- === ORIGINAL COLUMNS ===
  timestamp,
  ingested_at,
  raw_event_hash_id,  -- Primary key for fast joins
  device_id,
  device_date,
  system_engaged,
  parking_brake_applied,
  current_position,
  current_speed,
  load_weight,
  state,
  software_state,
  prndl,
  extras,
  
  -- === ENGINEERED FEATURES ===
  
  -- Geospatial Features
  location_type,
  
  -- Kinematic Features
  is_stationary,
  altitude,
  altitude_rate_of_change,
  speed_rolling_avg_5s,
  
  -- System State Features (PRNDL one-hot)
  prndl_park,
  prndl_reverse,
  prndl_neutral,
  prndl_drive,
  prndl_unknown,
  
  -- Payload Features
  load_weight_smoothed,
  load_weight_rate_of_change,
  
  -- Sensor Health Features
  has_reliable_payload,
  
  -- Duration Features
  CASE
    WHEN is_stationary THEN time_in_block
    ELSE 0
  END AS time_in_stationary_state,
  
  -- Interaction Features (composite)
  (is_stationary AND prndl = 'neutral' AND parking_brake_applied) AS is_ready_for_load,
  (location_type = 'Haul Road / Other' AND NOT is_stationary) AS is_hauling,
  (is_stationary AND location_type LIKE '%%Loading%%') AS is_loading_position,
  (is_stationary AND location_type LIKE '%%Dump%%') AS is_dumping_position,
  (load_weight_smoothed > 50000) AS is_heavy_load

FROM final_features
ORDER BY device_id, timestamp;

-- === PERFORMANCE OPTIMIZATIONS ===

-- Create primary key on raw_event_hash_id for fastest possible joins
ALTER TABLE "03_primary_feature_table" ADD PRIMARY KEY (raw_event_hash_id);

-- Create optimized indexes for common query patterns
CREATE INDEX CONCURRENTLY idx_03_primary_device_timestamp 
ON "03_primary_feature_table" (device_id, timestamp);

CREATE INDEX CONCURRENTLY idx_03_primary_stationary 
ON "03_primary_feature_table" (is_stationary) 
WHERE is_stationary = true;

CREATE INDEX CONCURRENTLY idx_03_primary_location 
ON "03_primary_feature_table" (location_type);

CREATE INDEX CONCURRENTLY idx_03_primary_heavy_load
ON "03_primary_feature_table" (is_heavy_load)
WHERE is_heavy_load = true;

-- Update table statistics for optimal query planning
ANALYZE "03_primary_feature_table";

-- Log completion
SELECT 
    COUNT(*) as total_records,
    COUNT(DISTINCT device_id) as unique_devices,
    MIN(timestamp) as earliest_record,
    MAX(timestamp) as latest_record
FROM "03_primary_feature_table";]
(Background on this error at: https://sqlalche.me/e/20/f405)
2025-09-03 05:36:58,923 - ERROR - High-performance pipeline failed: (psycopg2.errors.WindowingError) window function calls cannot be nested
LINE 77:       ) - LAG(AVG(load_weight) OVER (
                       ^

[SQL: -- High-Performance Feature Engineering Pipeline
-- AHS Analytics Engine - DataMine V2
-- This SQL script generates all features directly in the database using parallel processing
-- Optimized with raw_event_hash_id joins for maximum performance

-- Enable parallel execution and optimize memory
SET max_parallel_workers_per_gather = 8;
SET work_mem = '4GB';
SET effective_cache_size = '200GB';

-- Drop the target table to ensure a fresh build
DROP TABLE IF EXISTS "03_primary_feature_table";

-- Create comprehensive feature table in a single parallel operation
CREATE TABLE "03_primary_feature_table" AS
WITH
  -- Step 1: Base data with geospatial features
  base_features AS (
    SELECT
      t.*,
      -- Extract altitude directly from PostGIS geometry
      ST_Z(t.current_position::geometry) AS altitude,
      -- Simple geospatial classification (can be enhanced with actual operational_zones table)
      CASE
        WHEN ST_Z(t.current_position::geometry) < 200 THEN 'Loading Zone A'
        WHEN ST_Z(t.current_position::geometry) > 250 THEN 'Dump Zone B' 
        ELSE 'Haul Road / Other'
      END AS location_type
    FROM "02_raw_telemetry_transformed" t
    WHERE current_position IS NOT NULL
  ),
  
  -- Step 2: Kinematic features using optimized window functions
  kinematic_features AS (
    SELECT
      *,
      -- A. is_stationary
      (current_speed < 0.5) AS is_stationary,
      
      -- B. altitude_rate_of_change using LAG window function
      altitude - LAG(altitude, 1) OVER (
        PARTITION BY device_id
        ORDER BY timestamp
      ) AS altitude_rate_of_change,
      
      -- C. speed_rolling_avg_5s using optimized window
      AVG(current_speed) OVER (
        PARTITION BY device_id
        ORDER BY timestamp 
        ROWS BETWEEN 2 PRECEDING AND 2 FOLLOWING
      ) AS speed_rolling_avg_5s
    FROM base_features
  ),
  
  -- Step 3: Payload features with smoothing
  payload_features AS (
    SELECT
      *,
      -- load_weight_smoothed using 5-point rolling average
      AVG(load_weight) OVER (
        PARTITION BY device_id
        ORDER BY timestamp 
        ROWS BETWEEN 2 PRECEDING AND 2 FOLLOWING
      ) AS load_weight_smoothed
    FROM kinematic_features
  ),
  
  -- Step 4: Add payload rate of change
  payload_with_rate AS (
    SELECT
      *,
      -- load_weight_rate_of_change on smoothed values
      AVG(load_weight) OVER (
        PARTITION BY device_id
        ORDER BY timestamp 
        ROWS BETWEEN 2 PRECEDING AND 2 FOLLOWING
      ) - LAG(AVG(load_weight) OVER (
        PARTITION BY device_id
        ORDER BY timestamp 
        ROWS BETWEEN 2 PRECEDING AND 2 FOLLOWING
      ), 1) OVER (
        PARTITION BY device_id
        ORDER BY timestamp
      ) AS load_weight_rate_of_change
    FROM payload_features
  ),
  
  -- Step 5: Sensor health flags using device-level aggregation
  sensor_health_features AS (
    SELECT
      *,
      -- has_reliable_payload flag based on load_weight variance
      (STDDEV(load_weight) OVER (PARTITION BY device_id) > 1000) AS has_reliable_payload
    FROM payload_with_rate
  ),
  
  -- Step 6: PRNDL one-hot encoding
  prndl_features AS (
    SELECT
      *,
      (prndl = 'park') AS prndl_park,
      (prndl = 'reverse') AS prndl_reverse, 
      (prndl = 'neutral') AS prndl_neutral,
      (prndl = 'drive') AS prndl_drive,
      (prndl = 'unknown') AS prndl_unknown
    FROM sensor_health_features
  ),
  
  -- Step 7: Duration features - identify stationary state changes
  duration_prep AS (
    SELECT
      *,
      -- Create unique IDs for consecutive stationary blocks
      SUM(CASE
        WHEN is_stationary != LAG(is_stationary, 1, is_stationary) OVER (
          PARTITION BY device_id ORDER BY timestamp
        ) THEN 1
        ELSE 0
      END) OVER (
        PARTITION BY device_id ORDER BY timestamp
      ) AS stationary_block_id,
      
      -- Calculate time delta between consecutive records (in seconds)
      COALESCE(
        EXTRACT(EPOCH FROM (
          timestamp - LAG(timestamp, 1) OVER (
            PARTITION BY device_id ORDER BY timestamp
          )
        )), 0
      ) AS time_delta
    FROM prndl_features
  ),
  
  -- Step 8: Calculate cumulative time within stationary blocks
  final_features AS (
    SELECT
      *,
      -- Cumulative time within each stationary block
      SUM(time_delta) OVER (
        PARTITION BY device_id, stationary_block_id
        ORDER BY timestamp
      ) AS time_in_block
    FROM duration_prep
  )

-- Final selection with all original and engineered features
SELECT
  -- === ORIGINAL COLUMNS ===
  timestamp,
  ingested_at,
  raw_event_hash_id,  -- Primary key for fast joins
  device_id,
  device_date,
  system_engaged,
  parking_brake_applied,
  current_position,
  current_speed,
  load_weight,
  state,
  software_state,
  prndl,
  extras,
  
  -- === ENGINEERED FEATURES ===
  
  -- Geospatial Features
  location_type,
  
  -- Kinematic Features
  is_stationary,
  altitude,
  altitude_rate_of_change,
  speed_rolling_avg_5s,
  
  -- System State Features (PRNDL one-hot)
  prndl_park,
  prndl_reverse,
  prndl_neutral,
  prndl_drive,
  prndl_unknown,
  
  -- Payload Features
  load_weight_smoothed,
  load_weight_rate_of_change,
  
  -- Sensor Health Features
  has_reliable_payload,
  
  -- Duration Features
  CASE
    WHEN is_stationary THEN time_in_block
    ELSE 0
  END AS time_in_stationary_state,
  
  -- Interaction Features (composite)
  (is_stationary AND prndl = 'neutral' AND parking_brake_applied) AS is_ready_for_load,
  (location_type = 'Haul Road / Other' AND NOT is_stationary) AS is_hauling,
  (is_stationary AND location_type LIKE '%%Loading%%') AS is_loading_position,
  (is_stationary AND location_type LIKE '%%Dump%%') AS is_dumping_position,
  (load_weight_smoothed > 50000) AS is_heavy_load

FROM final_features
ORDER BY device_id, timestamp;

-- === PERFORMANCE OPTIMIZATIONS ===

-- Create primary key on raw_event_hash_id for fastest possible joins
ALTER TABLE "03_primary_feature_table" ADD PRIMARY KEY (raw_event_hash_id);

-- Create optimized indexes for common query patterns
CREATE INDEX CONCURRENTLY idx_03_primary_device_timestamp 
ON "03_primary_feature_table" (device_id, timestamp);

CREATE INDEX CONCURRENTLY idx_03_primary_stationary 
ON "03_primary_feature_table" (is_stationary) 
WHERE is_stationary = true;

CREATE INDEX CONCURRENTLY idx_03_primary_location 
ON "03_primary_feature_table" (location_type);

CREATE INDEX CONCURRENTLY idx_03_primary_heavy_load
ON "03_primary_feature_table" (is_heavy_load)
WHERE is_heavy_load = true;

-- Update table statistics for optimal query planning
ANALYZE "03_primary_feature_table";

-- Log completion
SELECT 
    COUNT(*) as total_records,
    COUNT(DISTINCT device_id) as unique_devices,
    MIN(timestamp) as earliest_record,
    MAX(timestamp) as latest_record
FROM "03_primary_feature_table";]
(Background on this error at: https://sqlalche.me/e/20/f405)
2025-09-03 05:36:58,923 - ERROR - Check the logs above for specific error details
2025-09-03 05:36:58,923 - ERROR - ❌ Pipeline execution failed: (psycopg2.errors.WindowingError) window function calls cannot be nested
LINE 77:       ) - LAG(AVG(load_weight) OVER (
                       ^

[SQL: -- High-Performance Feature Engineering Pipeline
-- AHS Analytics Engine - DataMine V2
-- This SQL script generates all features directly in the database using parallel processing
-- Optimized with raw_event_hash_id joins for maximum performance

-- Enable parallel execution and optimize memory
SET max_parallel_workers_per_gather = 8;
SET work_mem = '4GB';
SET effective_cache_size = '200GB';

-- Drop the target table to ensure a fresh build
DROP TABLE IF EXISTS "03_primary_feature_table";

-- Create comprehensive feature table in a single parallel operation
CREATE TABLE "03_primary_feature_table" AS
WITH
  -- Step 1: Base data with geospatial features
  base_features AS (
    SELECT
      t.*,
      -- Extract altitude directly from PostGIS geometry
      ST_Z(t.current_position::geometry) AS altitude,
      -- Simple geospatial classification (can be enhanced with actual operational_zones table)
      CASE
        WHEN ST_Z(t.current_position::geometry) < 200 THEN 'Loading Zone A'
        WHEN ST_Z(t.current_position::geometry) > 250 THEN 'Dump Zone B' 
        ELSE 'Haul Road / Other'
      END AS location_type
    FROM "02_raw_telemetry_transformed" t
    WHERE current_position IS NOT NULL
  ),
  
  -- Step 2: Kinematic features using optimized window functions
  kinematic_features AS (
    SELECT
      *,
      -- A. is_stationary
      (current_speed < 0.5) AS is_stationary,
      
      -- B. altitude_rate_of_change using LAG window function
      altitude - LAG(altitude, 1) OVER (
        PARTITION BY device_id
        ORDER BY timestamp
      ) AS altitude_rate_of_change,
      
      -- C. speed_rolling_avg_5s using optimized window
      AVG(current_speed) OVER (
        PARTITION BY device_id
        ORDER BY timestamp 
        ROWS BETWEEN 2 PRECEDING AND 2 FOLLOWING
      ) AS speed_rolling_avg_5s
    FROM base_features
  ),
  
  -- Step 3: Payload features with smoothing
  payload_features AS (
    SELECT
      *,
      -- load_weight_smoothed using 5-point rolling average
      AVG(load_weight) OVER (
        PARTITION BY device_id
        ORDER BY timestamp 
        ROWS BETWEEN 2 PRECEDING AND 2 FOLLOWING
      ) AS load_weight_smoothed
    FROM kinematic_features
  ),
  
  -- Step 4: Add payload rate of change
  payload_with_rate AS (
    SELECT
      *,
      -- load_weight_rate_of_change on smoothed values
      AVG(load_weight) OVER (
        PARTITION BY device_id
        ORDER BY timestamp 
        ROWS BETWEEN 2 PRECEDING AND 2 FOLLOWING
      ) - LAG(AVG(load_weight) OVER (
        PARTITION BY device_id
        ORDER BY timestamp 
        ROWS BETWEEN 2 PRECEDING AND 2 FOLLOWING
      ), 1) OVER (
        PARTITION BY device_id
        ORDER BY timestamp
      ) AS load_weight_rate_of_change
    FROM payload_features
  ),
  
  -- Step 5: Sensor health flags using device-level aggregation
  sensor_health_features AS (
    SELECT
      *,
      -- has_reliable_payload flag based on load_weight variance
      (STDDEV(load_weight) OVER (PARTITION BY device_id) > 1000) AS has_reliable_payload
    FROM payload_with_rate
  ),
  
  -- Step 6: PRNDL one-hot encoding
  prndl_features AS (
    SELECT
      *,
      (prndl = 'park') AS prndl_park,
      (prndl = 'reverse') AS prndl_reverse, 
      (prndl = 'neutral') AS prndl_neutral,
      (prndl = 'drive') AS prndl_drive,
      (prndl = 'unknown') AS prndl_unknown
    FROM sensor_health_features
  ),
  
  -- Step 7: Duration features - identify stationary state changes
  duration_prep AS (
    SELECT
      *,
      -- Create unique IDs for consecutive stationary blocks
      SUM(CASE
        WHEN is_stationary != LAG(is_stationary, 1, is_stationary) OVER (
          PARTITION BY device_id ORDER BY timestamp
        ) THEN 1
        ELSE 0
      END) OVER (
        PARTITION BY device_id ORDER BY timestamp
      ) AS stationary_block_id,
      
      -- Calculate time delta between consecutive records (in seconds)
      COALESCE(
        EXTRACT(EPOCH FROM (
          timestamp - LAG(timestamp, 1) OVER (
            PARTITION BY device_id ORDER BY timestamp
          )
        )), 0
      ) AS time_delta
    FROM prndl_features
  ),
  
  -- Step 8: Calculate cumulative time within stationary blocks
  final_features AS (
    SELECT
      *,
      -- Cumulative time within each stationary block
      SUM(time_delta) OVER (
        PARTITION BY device_id, stationary_block_id
        ORDER BY timestamp
      ) AS time_in_block
    FROM duration_prep
  )

-- Final selection with all original and engineered features
SELECT
  -- === ORIGINAL COLUMNS ===
  timestamp,
  ingested_at,
  raw_event_hash_id,  -- Primary key for fast joins
  device_id,
  device_date,
  system_engaged,
  parking_brake_applied,
  current_position,
  current_speed,
  load_weight,
  state,
  software_state,
  prndl,
  extras,
  
  -- === ENGINEERED FEATURES ===
  
  -- Geospatial Features
  location_type,
  
  -- Kinematic Features
  is_stationary,
  altitude,
  altitude_rate_of_change,
  speed_rolling_avg_5s,
  
  -- System State Features (PRNDL one-hot)
  prndl_park,
  prndl_reverse,
  prndl_neutral,
  prndl_drive,
  prndl_unknown,
  
  -- Payload Features
  load_weight_smoothed,
  load_weight_rate_of_change,
  
  -- Sensor Health Features
  has_reliable_payload,
  
  -- Duration Features
  CASE
    WHEN is_stationary THEN time_in_block
    ELSE 0
  END AS time_in_stationary_state,
  
  -- Interaction Features (composite)
  (is_stationary AND prndl = 'neutral' AND parking_brake_applied) AS is_ready_for_load,
  (location_type = 'Haul Road / Other' AND NOT is_stationary) AS is_hauling,
  (is_stationary AND location_type LIKE '%%Loading%%') AS is_loading_position,
  (is_stationary AND location_type LIKE '%%Dump%%') AS is_dumping_position,
  (load_weight_smoothed > 50000) AS is_heavy_load

FROM final_features
ORDER BY device_id, timestamp;

-- === PERFORMANCE OPTIMIZATIONS ===

-- Create primary key on raw_event_hash_id for fastest possible joins
ALTER TABLE "03_primary_feature_table" ADD PRIMARY KEY (raw_event_hash_id);

-- Create optimized indexes for common query patterns
CREATE INDEX CONCURRENTLY idx_03_primary_device_timestamp 
ON "03_primary_feature_table" (device_id, timestamp);

CREATE INDEX CONCURRENTLY idx_03_primary_stationary 
ON "03_primary_feature_table" (is_stationary) 
WHERE is_stationary = true;

CREATE INDEX CONCURRENTLY idx_03_primary_location 
ON "03_primary_feature_table" (location_type);

CREATE INDEX CONCURRENTLY idx_03_primary_heavy_load
ON "03_primary_feature_table" (is_heavy_load)
WHERE is_heavy_load = true;

-- Update table statistics for optimal query planning
ANALYZE "03_primary_feature_table";

-- Log completion
SELECT 
    COUNT(*) as total_records,
    COUNT(DISTINCT device_id) as unique_devices,
    MIN(timestamp) as earliest_record,
    MAX(timestamp) as latest_record
FROM "03_primary_feature_table";]
(Background on this error at: https://sqlalche.me/e/20/f405)
2025-09-03 05:37:28,365 - INFO - Connected to database with high-performance configuration
2025-09-03 05:37:28,365 - INFO - ======================================================================
2025-09-03 05:37:28,365 - INFO - STARTING HIGH-PERFORMANCE DATABASE-CENTRIC FEATURE ENGINEERING
2025-09-03 05:37:28,365 - INFO - ======================================================================
2025-09-03 05:37:28,365 - INFO - 🚀 Leveraging 100 CPUs and 300GB RAM through PostgreSQL parallel processing
2025-09-03 05:37:28,365 - INFO - 🔗 Using raw_event_hash_id for optimal join performance
2025-09-03 05:37:28,365 - INFO - ✓ Loaded SQL script: generate_features.sql
2025-09-03 05:37:28,365 - INFO - Executing comprehensive feature engineering query...
2025-09-03 05:37:28,365 - INFO - This leverages all available CPU cores and memory for parallel processing
2025-09-03 05:37:28,377 - INFO - ✓ Database session optimized for high-performance processing
2025-09-03 05:37:28,379 - ERROR - SQL execution failed: (psycopg2.errors.WindowingError) window function calls cannot be nested
LINE 107:         WHEN is_stationary != LAG(is_stationary, 1, is_stati...
                                        ^

[SQL: -- High-Performance Feature Engineering Pipeline
-- AHS Analytics Engine - DataMine V2
-- This SQL script generates all features directly in the database using parallel processing
-- Optimized with raw_event_hash_id joins for maximum performance

-- Enable parallel execution and optimize memory
SET max_parallel_workers_per_gather = 8;
SET work_mem = '4GB';
SET effective_cache_size = '200GB';

-- Drop the target table to ensure a fresh build
DROP TABLE IF EXISTS "03_primary_feature_table";

-- Create comprehensive feature table in a single parallel operation
CREATE TABLE "03_primary_feature_table" AS
WITH
  -- Step 1: Base data with geospatial features
  base_features AS (
    SELECT
      t.*,
      -- Extract altitude directly from PostGIS geometry
      ST_Z(t.current_position::geometry) AS altitude,
      -- Simple geospatial classification (can be enhanced with actual operational_zones table)
      CASE
        WHEN ST_Z(t.current_position::geometry) < 200 THEN 'Loading Zone A'
        WHEN ST_Z(t.current_position::geometry) > 250 THEN 'Dump Zone B' 
        ELSE 'Haul Road / Other'
      END AS location_type
    FROM "02_raw_telemetry_transformed" t
    WHERE current_position IS NOT NULL
  ),
  
  -- Step 2: Kinematic features using optimized window functions
  kinematic_features AS (
    SELECT
      *,
      -- A. is_stationary
      (current_speed < 0.5) AS is_stationary,
      
      -- B. altitude_rate_of_change using LAG window function
      altitude - LAG(altitude, 1) OVER (
        PARTITION BY device_id
        ORDER BY timestamp
      ) AS altitude_rate_of_change,
      
      -- C. speed_rolling_avg_5s using optimized window
      AVG(current_speed) OVER (
        PARTITION BY device_id
        ORDER BY timestamp 
        ROWS BETWEEN 2 PRECEDING AND 2 FOLLOWING
      ) AS speed_rolling_avg_5s
    FROM base_features
  ),
  
  -- Step 3: Payload features with smoothing
  payload_features AS (
    SELECT
      *,
      -- load_weight_smoothed using 5-point rolling average
      AVG(load_weight) OVER (
        PARTITION BY device_id
        ORDER BY timestamp 
        ROWS BETWEEN 2 PRECEDING AND 2 FOLLOWING
      ) AS load_weight_smoothed
    FROM kinematic_features
  ),
  
  -- Step 4: Add payload rate of change (fix nested window functions)
  payload_with_rate AS (
    SELECT
      *,
      -- load_weight_rate_of_change using LAG on the smoothed column
      load_weight_smoothed - LAG(load_weight_smoothed, 1) OVER (
        PARTITION BY device_id
        ORDER BY timestamp
      ) AS load_weight_rate_of_change
    FROM payload_features
  ),
  
  -- Step 5: Sensor health flags using device-level aggregation
  sensor_health_features AS (
    SELECT
      *,
      -- has_reliable_payload flag based on load_weight variance
      (STDDEV(load_weight) OVER (PARTITION BY device_id) > 1000) AS has_reliable_payload
    FROM payload_with_rate
  ),
  
  -- Step 6: PRNDL one-hot encoding
  prndl_features AS (
    SELECT
      *,
      (prndl = 'park') AS prndl_park,
      (prndl = 'reverse') AS prndl_reverse, 
      (prndl = 'neutral') AS prndl_neutral,
      (prndl = 'drive') AS prndl_drive,
      (prndl = 'unknown') AS prndl_unknown
    FROM sensor_health_features
  ),
  
  -- Step 7: Duration features - identify stationary state changes
  duration_prep AS (
    SELECT
      *,
      -- Create unique IDs for consecutive stationary blocks
      SUM(CASE
        WHEN is_stationary != LAG(is_stationary, 1, is_stationary) OVER (
          PARTITION BY device_id ORDER BY timestamp
        ) THEN 1
        ELSE 0
      END) OVER (
        PARTITION BY device_id ORDER BY timestamp
      ) AS stationary_block_id,
      
      -- Calculate time delta between consecutive records (in seconds)
      COALESCE(
        EXTRACT(EPOCH FROM (
          timestamp - LAG(timestamp, 1) OVER (
            PARTITION BY device_id ORDER BY timestamp
          )
        )), 0
      ) AS time_delta
    FROM prndl_features
  ),
  
  -- Step 8: Calculate cumulative time within stationary blocks
  final_features AS (
    SELECT
      *,
      -- Cumulative time within each stationary block
      SUM(time_delta) OVER (
        PARTITION BY device_id, stationary_block_id
        ORDER BY timestamp
      ) AS time_in_block
    FROM duration_prep
  )

-- Final selection with all original and engineered features
SELECT
  -- === ORIGINAL COLUMNS ===
  timestamp,
  ingested_at,
  raw_event_hash_id,  -- Primary key for fast joins
  device_id,
  device_date,
  system_engaged,
  parking_brake_applied,
  current_position,
  current_speed,
  load_weight,
  state,
  software_state,
  prndl,
  extras,
  
  -- === ENGINEERED FEATURES ===
  
  -- Geospatial Features
  location_type,
  
  -- Kinematic Features
  is_stationary,
  altitude,
  altitude_rate_of_change,
  speed_rolling_avg_5s,
  
  -- System State Features (PRNDL one-hot)
  prndl_park,
  prndl_reverse,
  prndl_neutral,
  prndl_drive,
  prndl_unknown,
  
  -- Payload Features
  load_weight_smoothed,
  load_weight_rate_of_change,
  
  -- Sensor Health Features
  has_reliable_payload,
  
  -- Duration Features
  CASE
    WHEN is_stationary THEN time_in_block
    ELSE 0
  END AS time_in_stationary_state,
  
  -- Interaction Features (composite)
  (is_stationary AND prndl = 'neutral' AND parking_brake_applied) AS is_ready_for_load,
  (location_type = 'Haul Road / Other' AND NOT is_stationary) AS is_hauling,
  (is_stationary AND location_type LIKE '%%Loading%%') AS is_loading_position,
  (is_stationary AND location_type LIKE '%%Dump%%') AS is_dumping_position,
  (load_weight_smoothed > 50000) AS is_heavy_load

FROM final_features
ORDER BY device_id, timestamp;

-- === PERFORMANCE OPTIMIZATIONS ===

-- Create primary key on raw_event_hash_id for fastest possible joins
ALTER TABLE "03_primary_feature_table" ADD PRIMARY KEY (raw_event_hash_id);

-- Create optimized indexes for common query patterns
CREATE INDEX CONCURRENTLY idx_03_primary_device_timestamp 
ON "03_primary_feature_table" (device_id, timestamp);

CREATE INDEX CONCURRENTLY idx_03_primary_stationary 
ON "03_primary_feature_table" (is_stationary) 
WHERE is_stationary = true;

CREATE INDEX CONCURRENTLY idx_03_primary_location 
ON "03_primary_feature_table" (location_type);

CREATE INDEX CONCURRENTLY idx_03_primary_heavy_load
ON "03_primary_feature_table" (is_heavy_load)
WHERE is_heavy_load = true;

-- Update table statistics for optimal query planning
ANALYZE "03_primary_feature_table";

-- Log completion
SELECT 
    COUNT(*) as total_records,
    COUNT(DISTINCT device_id) as unique_devices,
    MIN(timestamp) as earliest_record,
    MAX(timestamp) as latest_record
FROM "03_primary_feature_table";]
(Background on this error at: https://sqlalche.me/e/20/f405)
2025-09-03 05:37:28,379 - ERROR - High-performance pipeline failed: (psycopg2.errors.WindowingError) window function calls cannot be nested
LINE 107:         WHEN is_stationary != LAG(is_stationary, 1, is_stati...
                                        ^

[SQL: -- High-Performance Feature Engineering Pipeline
-- AHS Analytics Engine - DataMine V2
-- This SQL script generates all features directly in the database using parallel processing
-- Optimized with raw_event_hash_id joins for maximum performance

-- Enable parallel execution and optimize memory
SET max_parallel_workers_per_gather = 8;
SET work_mem = '4GB';
SET effective_cache_size = '200GB';

-- Drop the target table to ensure a fresh build
DROP TABLE IF EXISTS "03_primary_feature_table";

-- Create comprehensive feature table in a single parallel operation
CREATE TABLE "03_primary_feature_table" AS
WITH
  -- Step 1: Base data with geospatial features
  base_features AS (
    SELECT
      t.*,
      -- Extract altitude directly from PostGIS geometry
      ST_Z(t.current_position::geometry) AS altitude,
      -- Simple geospatial classification (can be enhanced with actual operational_zones table)
      CASE
        WHEN ST_Z(t.current_position::geometry) < 200 THEN 'Loading Zone A'
        WHEN ST_Z(t.current_position::geometry) > 250 THEN 'Dump Zone B' 
        ELSE 'Haul Road / Other'
      END AS location_type
    FROM "02_raw_telemetry_transformed" t
    WHERE current_position IS NOT NULL
  ),
  
  -- Step 2: Kinematic features using optimized window functions
  kinematic_features AS (
    SELECT
      *,
      -- A. is_stationary
      (current_speed < 0.5) AS is_stationary,
      
      -- B. altitude_rate_of_change using LAG window function
      altitude - LAG(altitude, 1) OVER (
        PARTITION BY device_id
        ORDER BY timestamp
      ) AS altitude_rate_of_change,
      
      -- C. speed_rolling_avg_5s using optimized window
      AVG(current_speed) OVER (
        PARTITION BY device_id
        ORDER BY timestamp 
        ROWS BETWEEN 2 PRECEDING AND 2 FOLLOWING
      ) AS speed_rolling_avg_5s
    FROM base_features
  ),
  
  -- Step 3: Payload features with smoothing
  payload_features AS (
    SELECT
      *,
      -- load_weight_smoothed using 5-point rolling average
      AVG(load_weight) OVER (
        PARTITION BY device_id
        ORDER BY timestamp 
        ROWS BETWEEN 2 PRECEDING AND 2 FOLLOWING
      ) AS load_weight_smoothed
    FROM kinematic_features
  ),
  
  -- Step 4: Add payload rate of change (fix nested window functions)
  payload_with_rate AS (
    SELECT
      *,
      -- load_weight_rate_of_change using LAG on the smoothed column
      load_weight_smoothed - LAG(load_weight_smoothed, 1) OVER (
        PARTITION BY device_id
        ORDER BY timestamp
      ) AS load_weight_rate_of_change
    FROM payload_features
  ),
  
  -- Step 5: Sensor health flags using device-level aggregation
  sensor_health_features AS (
    SELECT
      *,
      -- has_reliable_payload flag based on load_weight variance
      (STDDEV(load_weight) OVER (PARTITION BY device_id) > 1000) AS has_reliable_payload
    FROM payload_with_rate
  ),
  
  -- Step 6: PRNDL one-hot encoding
  prndl_features AS (
    SELECT
      *,
      (prndl = 'park') AS prndl_park,
      (prndl = 'reverse') AS prndl_reverse, 
      (prndl = 'neutral') AS prndl_neutral,
      (prndl = 'drive') AS prndl_drive,
      (prndl = 'unknown') AS prndl_unknown
    FROM sensor_health_features
  ),
  
  -- Step 7: Duration features - identify stationary state changes
  duration_prep AS (
    SELECT
      *,
      -- Create unique IDs for consecutive stationary blocks
      SUM(CASE
        WHEN is_stationary != LAG(is_stationary, 1, is_stationary) OVER (
          PARTITION BY device_id ORDER BY timestamp
        ) THEN 1
        ELSE 0
      END) OVER (
        PARTITION BY device_id ORDER BY timestamp
      ) AS stationary_block_id,
      
      -- Calculate time delta between consecutive records (in seconds)
      COALESCE(
        EXTRACT(EPOCH FROM (
          timestamp - LAG(timestamp, 1) OVER (
            PARTITION BY device_id ORDER BY timestamp
          )
        )), 0
      ) AS time_delta
    FROM prndl_features
  ),
  
  -- Step 8: Calculate cumulative time within stationary blocks
  final_features AS (
    SELECT
      *,
      -- Cumulative time within each stationary block
      SUM(time_delta) OVER (
        PARTITION BY device_id, stationary_block_id
        ORDER BY timestamp
      ) AS time_in_block
    FROM duration_prep
  )

-- Final selection with all original and engineered features
SELECT
  -- === ORIGINAL COLUMNS ===
  timestamp,
  ingested_at,
  raw_event_hash_id,  -- Primary key for fast joins
  device_id,
  device_date,
  system_engaged,
  parking_brake_applied,
  current_position,
  current_speed,
  load_weight,
  state,
  software_state,
  prndl,
  extras,
  
  -- === ENGINEERED FEATURES ===
  
  -- Geospatial Features
  location_type,
  
  -- Kinematic Features
  is_stationary,
  altitude,
  altitude_rate_of_change,
  speed_rolling_avg_5s,
  
  -- System State Features (PRNDL one-hot)
  prndl_park,
  prndl_reverse,
  prndl_neutral,
  prndl_drive,
  prndl_unknown,
  
  -- Payload Features
  load_weight_smoothed,
  load_weight_rate_of_change,
  
  -- Sensor Health Features
  has_reliable_payload,
  
  -- Duration Features
  CASE
    WHEN is_stationary THEN time_in_block
    ELSE 0
  END AS time_in_stationary_state,
  
  -- Interaction Features (composite)
  (is_stationary AND prndl = 'neutral' AND parking_brake_applied) AS is_ready_for_load,
  (location_type = 'Haul Road / Other' AND NOT is_stationary) AS is_hauling,
  (is_stationary AND location_type LIKE '%%Loading%%') AS is_loading_position,
  (is_stationary AND location_type LIKE '%%Dump%%') AS is_dumping_position,
  (load_weight_smoothed > 50000) AS is_heavy_load

FROM final_features
ORDER BY device_id, timestamp;

-- === PERFORMANCE OPTIMIZATIONS ===

-- Create primary key on raw_event_hash_id for fastest possible joins
ALTER TABLE "03_primary_feature_table" ADD PRIMARY KEY (raw_event_hash_id);

-- Create optimized indexes for common query patterns
CREATE INDEX CONCURRENTLY idx_03_primary_device_timestamp 
ON "03_primary_feature_table" (device_id, timestamp);

CREATE INDEX CONCURRENTLY idx_03_primary_stationary 
ON "03_primary_feature_table" (is_stationary) 
WHERE is_stationary = true;

CREATE INDEX CONCURRENTLY idx_03_primary_location 
ON "03_primary_feature_table" (location_type);

CREATE INDEX CONCURRENTLY idx_03_primary_heavy_load
ON "03_primary_feature_table" (is_heavy_load)
WHERE is_heavy_load = true;

-- Update table statistics for optimal query planning
ANALYZE "03_primary_feature_table";

-- Log completion
SELECT 
    COUNT(*) as total_records,
    COUNT(DISTINCT device_id) as unique_devices,
    MIN(timestamp) as earliest_record,
    MAX(timestamp) as latest_record
FROM "03_primary_feature_table";]
(Background on this error at: https://sqlalche.me/e/20/f405)
2025-09-03 05:37:28,379 - ERROR - Check the logs above for specific error details
2025-09-03 05:37:28,379 - ERROR - ❌ Pipeline execution failed: (psycopg2.errors.WindowingError) window function calls cannot be nested
LINE 107:         WHEN is_stationary != LAG(is_stationary, 1, is_stati...
                                        ^

[SQL: -- High-Performance Feature Engineering Pipeline
-- AHS Analytics Engine - DataMine V2
-- This SQL script generates all features directly in the database using parallel processing
-- Optimized with raw_event_hash_id joins for maximum performance

-- Enable parallel execution and optimize memory
SET max_parallel_workers_per_gather = 8;
SET work_mem = '4GB';
SET effective_cache_size = '200GB';

-- Drop the target table to ensure a fresh build
DROP TABLE IF EXISTS "03_primary_feature_table";

-- Create comprehensive feature table in a single parallel operation
CREATE TABLE "03_primary_feature_table" AS
WITH
  -- Step 1: Base data with geospatial features
  base_features AS (
    SELECT
      t.*,
      -- Extract altitude directly from PostGIS geometry
      ST_Z(t.current_position::geometry) AS altitude,
      -- Simple geospatial classification (can be enhanced with actual operational_zones table)
      CASE
        WHEN ST_Z(t.current_position::geometry) < 200 THEN 'Loading Zone A'
        WHEN ST_Z(t.current_position::geometry) > 250 THEN 'Dump Zone B' 
        ELSE 'Haul Road / Other'
      END AS location_type
    FROM "02_raw_telemetry_transformed" t
    WHERE current_position IS NOT NULL
  ),
  
  -- Step 2: Kinematic features using optimized window functions
  kinematic_features AS (
    SELECT
      *,
      -- A. is_stationary
      (current_speed < 0.5) AS is_stationary,
      
      -- B. altitude_rate_of_change using LAG window function
      altitude - LAG(altitude, 1) OVER (
        PARTITION BY device_id
        ORDER BY timestamp
      ) AS altitude_rate_of_change,
      
      -- C. speed_rolling_avg_5s using optimized window
      AVG(current_speed) OVER (
        PARTITION BY device_id
        ORDER BY timestamp 
        ROWS BETWEEN 2 PRECEDING AND 2 FOLLOWING
      ) AS speed_rolling_avg_5s
    FROM base_features
  ),
  
  -- Step 3: Payload features with smoothing
  payload_features AS (
    SELECT
      *,
      -- load_weight_smoothed using 5-point rolling average
      AVG(load_weight) OVER (
        PARTITION BY device_id
        ORDER BY timestamp 
        ROWS BETWEEN 2 PRECEDING AND 2 FOLLOWING
      ) AS load_weight_smoothed
    FROM kinematic_features
  ),
  
  -- Step 4: Add payload rate of change (fix nested window functions)
  payload_with_rate AS (
    SELECT
      *,
      -- load_weight_rate_of_change using LAG on the smoothed column
      load_weight_smoothed - LAG(load_weight_smoothed, 1) OVER (
        PARTITION BY device_id
        ORDER BY timestamp
      ) AS load_weight_rate_of_change
    FROM payload_features
  ),
  
  -- Step 5: Sensor health flags using device-level aggregation
  sensor_health_features AS (
    SELECT
      *,
      -- has_reliable_payload flag based on load_weight variance
      (STDDEV(load_weight) OVER (PARTITION BY device_id) > 1000) AS has_reliable_payload
    FROM payload_with_rate
  ),
  
  -- Step 6: PRNDL one-hot encoding
  prndl_features AS (
    SELECT
      *,
      (prndl = 'park') AS prndl_park,
      (prndl = 'reverse') AS prndl_reverse, 
      (prndl = 'neutral') AS prndl_neutral,
      (prndl = 'drive') AS prndl_drive,
      (prndl = 'unknown') AS prndl_unknown
    FROM sensor_health_features
  ),
  
  -- Step 7: Duration features - identify stationary state changes
  duration_prep AS (
    SELECT
      *,
      -- Create unique IDs for consecutive stationary blocks
      SUM(CASE
        WHEN is_stationary != LAG(is_stationary, 1, is_stationary) OVER (
          PARTITION BY device_id ORDER BY timestamp
        ) THEN 1
        ELSE 0
      END) OVER (
        PARTITION BY device_id ORDER BY timestamp
      ) AS stationary_block_id,
      
      -- Calculate time delta between consecutive records (in seconds)
      COALESCE(
        EXTRACT(EPOCH FROM (
          timestamp - LAG(timestamp, 1) OVER (
            PARTITION BY device_id ORDER BY timestamp
          )
        )), 0
      ) AS time_delta
    FROM prndl_features
  ),
  
  -- Step 8: Calculate cumulative time within stationary blocks
  final_features AS (
    SELECT
      *,
      -- Cumulative time within each stationary block
      SUM(time_delta) OVER (
        PARTITION BY device_id, stationary_block_id
        ORDER BY timestamp
      ) AS time_in_block
    FROM duration_prep
  )

-- Final selection with all original and engineered features
SELECT
  -- === ORIGINAL COLUMNS ===
  timestamp,
  ingested_at,
  raw_event_hash_id,  -- Primary key for fast joins
  device_id,
  device_date,
  system_engaged,
  parking_brake_applied,
  current_position,
  current_speed,
  load_weight,
  state,
  software_state,
  prndl,
  extras,
  
  -- === ENGINEERED FEATURES ===
  
  -- Geospatial Features
  location_type,
  
  -- Kinematic Features
  is_stationary,
  altitude,
  altitude_rate_of_change,
  speed_rolling_avg_5s,
  
  -- System State Features (PRNDL one-hot)
  prndl_park,
  prndl_reverse,
  prndl_neutral,
  prndl_drive,
  prndl_unknown,
  
  -- Payload Features
  load_weight_smoothed,
  load_weight_rate_of_change,
  
  -- Sensor Health Features
  has_reliable_payload,
  
  -- Duration Features
  CASE
    WHEN is_stationary THEN time_in_block
    ELSE 0
  END AS time_in_stationary_state,
  
  -- Interaction Features (composite)
  (is_stationary AND prndl = 'neutral' AND parking_brake_applied) AS is_ready_for_load,
  (location_type = 'Haul Road / Other' AND NOT is_stationary) AS is_hauling,
  (is_stationary AND location_type LIKE '%%Loading%%') AS is_loading_position,
  (is_stationary AND location_type LIKE '%%Dump%%') AS is_dumping_position,
  (load_weight_smoothed > 50000) AS is_heavy_load

FROM final_features
ORDER BY device_id, timestamp;

-- === PERFORMANCE OPTIMIZATIONS ===

-- Create primary key on raw_event_hash_id for fastest possible joins
ALTER TABLE "03_primary_feature_table" ADD PRIMARY KEY (raw_event_hash_id);

-- Create optimized indexes for common query patterns
CREATE INDEX CONCURRENTLY idx_03_primary_device_timestamp 
ON "03_primary_feature_table" (device_id, timestamp);

CREATE INDEX CONCURRENTLY idx_03_primary_stationary 
ON "03_primary_feature_table" (is_stationary) 
WHERE is_stationary = true;

CREATE INDEX CONCURRENTLY idx_03_primary_location 
ON "03_primary_feature_table" (location_type);

CREATE INDEX CONCURRENTLY idx_03_primary_heavy_load
ON "03_primary_feature_table" (is_heavy_load)
WHERE is_heavy_load = true;

-- Update table statistics for optimal query planning
ANALYZE "03_primary_feature_table";

-- Log completion
SELECT 
    COUNT(*) as total_records,
    COUNT(DISTINCT device_id) as unique_devices,
    MIN(timestamp) as earliest_record,
    MAX(timestamp) as latest_record
FROM "03_primary_feature_table";]
(Background on this error at: https://sqlalche.me/e/20/f405)
2025-09-03 05:37:49,057 - INFO - Connected to database with high-performance configuration
2025-09-03 05:37:49,058 - INFO - ======================================================================
2025-09-03 05:37:49,058 - INFO - STARTING HIGH-PERFORMANCE DATABASE-CENTRIC FEATURE ENGINEERING
2025-09-03 05:37:49,058 - INFO - ======================================================================
2025-09-03 05:37:49,058 - INFO - 🚀 Leveraging 100 CPUs and 300GB RAM through PostgreSQL parallel processing
2025-09-03 05:37:49,058 - INFO - 🔗 Using raw_event_hash_id for optimal join performance
2025-09-03 05:37:49,058 - INFO - ✓ Loaded SQL script: generate_features.sql
2025-09-03 05:37:49,058 - INFO - Executing comprehensive feature engineering query...
2025-09-03 05:37:49,058 - INFO - This leverages all available CPU cores and memory for parallel processing
2025-09-03 05:37:49,069 - INFO - ✓ Database session optimized for high-performance processing
2025-09-03 05:46:37,373 - INFO - Connected to database with high-performance configuration
2025-09-03 05:46:37,373 - INFO - ======================================================================
2025-09-03 05:46:37,373 - INFO - STARTING HIGH-PERFORMANCE DATABASE-CENTRIC FEATURE ENGINEERING
2025-09-03 05:46:37,373 - INFO - ======================================================================
2025-09-03 05:46:37,373 - INFO - 🚀 Leveraging 100 CPUs and 300GB RAM through PostgreSQL parallel processing
2025-09-03 05:46:37,373 - INFO - 🔗 Using raw_event_hash_id for optimal join performance
2025-09-03 05:46:37,373 - INFO - ✓ Loaded SQL script: generate_features_optimized.sql
2025-09-03 05:46:37,373 - INFO - Executing comprehensive feature engineering query...
2025-09-03 05:46:37,373 - INFO - This leverages all available CPU cores and memory for parallel processing
2025-09-03 05:46:37,385 - INFO - ✓ Database session optimized for high-performance processing
2025-09-03 05:49:58,020 - INFO - Connected to database with high-performance configuration
2025-09-03 05:49:58,020 - INFO - ======================================================================
2025-09-03 05:49:58,020 - INFO - STARTING HIGH-PERFORMANCE DATABASE-CENTRIC FEATURE ENGINEERING
2025-09-03 05:49:58,020 - INFO - ======================================================================
2025-09-03 05:49:58,020 - INFO - 🚀 Leveraging 100 CPUs and 300GB RAM through PostgreSQL parallel processing
2025-09-03 05:49:58,020 - INFO - 🔗 Using raw_event_hash_id for optimal join performance
2025-09-03 05:49:58,020 - INFO - ✓ Loaded SQL script: generate_features_hybrid.sql
2025-09-03 05:49:58,020 - INFO - Executing comprehensive feature engineering query...
2025-09-03 05:49:58,020 - INFO - This leverages all available CPU cores and memory for parallel processing
2025-09-03 05:49:58,034 - INFO - ✓ Database session optimized for high-performance processing
2025-09-03 05:50:47,879 - ERROR - SQL execution failed: (psycopg2.errors.WindowingError) window functions are not allowed in UPDATE
LINE 54:     altitude - LAG(altitude, 1) OVER (PARTITION BY device_id...
                        ^

[SQL: -- Hybrid Materialization Feature Engineering Pipeline
-- AHS Analytics Engine - DataMine V2
-- This approach uses temporary tables to maximize parallel execution while handling window functions efficiently

-- Enable maximum parallelism for parallel-safe operations
SET max_parallel_workers_per_gather = 32;
SET work_mem = '8GB';
SET effective_cache_size = '250GB';
SET parallel_tuple_cost = 0.01;
SET parallel_setup_cost = 10.0;

-- ============================================================================
-- STEP 1: Create Base Table with Maximum Parallelism
-- This step processes ~8.8M records using all available CPU cores
-- ============================================================================

SELECT 'STEP 1: Creating base features table with parallel processing...' AS status;

DROP TABLE IF EXISTS temp_base_features;
CREATE TEMP TABLE temp_base_features AS
SELECT
    t.*,
    -- Extract altitude directly from PostGIS geometry (parallel-safe)
    ST_Z(t.current_position::geometry) AS altitude,
    -- Geospatial classification (parallel-safe)
    CASE
        WHEN ST_Z(t.current_position::geometry) < 200 THEN 'Loading Zone A'
        WHEN ST_Z(t.current_position::geometry) > 250 THEN 'Dump Zone B'
        ELSE 'Haul Road / Other'
    END AS location_type,
    -- Basic boolean features (parallel-safe)
    (current_speed < 0.5) AS is_stationary
FROM "02_raw_telemetry_transformed" t
WHERE t.current_position IS NOT NULL;

-- Create critical indexes on temp table for efficient window function processing
CREATE INDEX ON temp_base_features (device_id, timestamp);
CREATE INDEX ON temp_base_features (raw_event_hash_id);

SELECT 'STEP 1 Complete: Base table created with ' || COUNT(*) || ' records' AS status
FROM temp_base_features;

-- ============================================================================
-- STEP 2: Add Window Function Features Sequentially (Single-threaded but Fast)
-- These operations are memory-bound and very efficient on the indexed temp table
-- ============================================================================

SELECT 'STEP 2: Adding kinematic features...' AS status;

-- A. Altitude rate of change
ALTER TABLE temp_base_features ADD COLUMN altitude_rate_of_change DOUBLE PRECISION;
UPDATE temp_base_features 
SET altitude_rate_of_change = (
    altitude - LAG(altitude, 1) OVER (PARTITION BY device_id ORDER BY timestamp)
);

-- B. Speed rolling average (5-point window)
ALTER TABLE temp_base_features ADD COLUMN speed_rolling_avg_5s DOUBLE PRECISION;
UPDATE temp_base_features 
SET speed_rolling_avg_5s = (
    AVG(current_speed) OVER (
        PARTITION BY device_id 
        ORDER BY timestamp 
        ROWS BETWEEN 2 PRECEDING AND 2 FOLLOWING
    )
);

SELECT 'STEP 2A Complete: Kinematic features added' AS status;

-- ============================================================================
-- STEP 3: Add Payload Features
-- ============================================================================

SELECT 'STEP 3: Adding payload features...' AS status;

-- C. Load weight smoothed (5-point rolling average)
ALTER TABLE temp_base_features ADD COLUMN load_weight_smoothed DOUBLE PRECISION;
UPDATE temp_base_features 
SET load_weight_smoothed = (
    AVG(load_weight) OVER (
        PARTITION BY device_id 
        ORDER BY timestamp 
        ROWS BETWEEN 2 PRECEDING AND 2 FOLLOWING
    )
);

-- D. Load weight rate of change
ALTER TABLE temp_base_features ADD COLUMN load_weight_rate_of_change DOUBLE PRECISION;
UPDATE temp_base_features 
SET load_weight_rate_of_change = (
    load_weight_smoothed - LAG(load_weight_smoothed, 1) OVER (
        PARTITION BY device_id ORDER BY timestamp
    )
);

SELECT 'STEP 3 Complete: Payload features added' AS status;

-- ============================================================================
-- STEP 4: Add Sensor Health Features
-- ============================================================================

SELECT 'STEP 4: Adding sensor health features...' AS status;

-- E. Sensor reliability flag (device-level standard deviation)
ALTER TABLE temp_base_features ADD COLUMN has_reliable_payload BOOLEAN;
UPDATE temp_base_features 
SET has_reliable_payload = (
    STDDEV(load_weight) OVER (PARTITION BY device_id) > 1000
);

SELECT 'STEP 4 Complete: Sensor health features added' AS status;

-- ============================================================================
-- STEP 5: Add Duration Features (Most Complex Window Functions)
-- ============================================================================

SELECT 'STEP 5: Adding duration features...' AS status;

-- F. Identify stationary state changes
ALTER TABLE temp_base_features ADD COLUMN prev_stationary BOOLEAN;
UPDATE temp_base_features 
SET prev_stationary = LAG(is_stationary, 1, is_stationary) OVER (
    PARTITION BY device_id ORDER BY timestamp
);

-- G. Calculate time deltas
ALTER TABLE temp_base_features ADD COLUMN time_delta DOUBLE PRECISION;
UPDATE temp_base_features 
SET time_delta = COALESCE(
    EXTRACT(EPOCH FROM (
        timestamp - LAG(timestamp, 1) OVER (
            PARTITION BY device_id ORDER BY timestamp
        )
    )), 0
);

-- H. Create stationary block IDs
ALTER TABLE temp_base_features ADD COLUMN stationary_block_id INTEGER;
UPDATE temp_base_features 
SET stationary_block_id = SUM(
    CASE WHEN is_stationary != prev_stationary THEN 1 ELSE 0 END
) OVER (PARTITION BY device_id ORDER BY timestamp);

-- I. Calculate time in stationary state
ALTER TABLE temp_base_features ADD COLUMN time_in_stationary_state DOUBLE PRECISION;
UPDATE temp_base_features 
SET time_in_stationary_state = CASE
    WHEN is_stationary THEN
        SUM(time_delta) OVER (
            PARTITION BY device_id, stationary_block_id
            ORDER BY timestamp
        )
    ELSE 0
END;

SELECT 'STEP 5 Complete: Duration features added' AS status;

-- ============================================================================
-- STEP 6: Create Final Table with All Features (Parallel-Safe Final Step)
-- ============================================================================

SELECT 'STEP 6: Creating final feature table...' AS status;

DROP TABLE IF EXISTS "03_primary_feature_table";
CREATE TABLE "03_primary_feature_table" AS
SELECT
    -- === ORIGINAL COLUMNS ===
    timestamp,
    ingested_at,
    raw_event_hash_id,
    device_id,
    device_date,
    system_engaged,
    parking_brake_applied,
    current_position,
    current_speed,
    load_weight,
    state,
    software_state,
    prndl,
    extras,
    
    -- === ENGINEERED FEATURES ===
    
    -- Geospatial Features
    location_type,
    
    -- Kinematic Features
    is_stationary,
    altitude,
    altitude_rate_of_change,
    speed_rolling_avg_5s,
    
    -- System State Features (PRNDL one-hot encoding)
    (prndl = 'park') AS prndl_park,
    (prndl = 'reverse') AS prndl_reverse, 
    (prndl = 'neutral') AS prndl_neutral,
    (prndl = 'drive') AS prndl_drive,
    (prndl = 'unknown') AS prndl_unknown,
    
    -- Payload Features
    load_weight_smoothed,
    load_weight_rate_of_change,
    
    -- Sensor Health Features
    has_reliable_payload,
    
    -- Duration Features
    time_in_stationary_state,
    
    -- Interaction Features (composite features computed on the fly)
    (is_stationary AND prndl = 'neutral' AND parking_brake_applied) AS is_ready_for_load,
    (location_type = 'Haul Road / Other' AND NOT is_stationary) AS is_hauling,
    (is_stationary AND location_type LIKE '%%Loading%%') AS is_loading_position,
    (is_stationary AND location_type LIKE '%%Dump%%') AS is_dumping_position,
    (load_weight_smoothed > 50000) AS is_heavy_load
    
FROM temp_base_features
ORDER BY device_id, timestamp;

-- Clean up temporary table
DROP TABLE temp_base_features;

SELECT 'STEP 6 Complete: Final table created' AS status;

-- ============================================================================
-- STEP 7: Performance Optimizations
-- ============================================================================

SELECT 'STEP 7: Adding performance optimizations...' AS status;

-- Create primary key on raw_event_hash_id for fastest possible joins
ALTER TABLE "03_primary_feature_table" ADD PRIMARY KEY (raw_event_hash_id);

-- Create optimized indexes for common query patterns
CREATE INDEX CONCURRENTLY idx_03_primary_device_timestamp 
ON "03_primary_feature_table" (device_id, timestamp);

CREATE INDEX CONCURRENTLY idx_03_primary_stationary 
ON "03_primary_feature_table" (is_stationary) 
WHERE is_stationary = true;

CREATE INDEX CONCURRENTLY idx_03_primary_location 
ON "03_primary_feature_table" (location_type);

CREATE INDEX CONCURRENTLY idx_03_primary_heavy_load
ON "03_primary_feature_table" (is_heavy_load)
WHERE is_heavy_load = true;

-- Update table statistics for optimal query planning
ANALYZE "03_primary_feature_table";

SELECT 'STEP 7 Complete: All optimizations applied' AS status;

-- ============================================================================
-- FINAL SUMMARY: Success Report
-- ============================================================================

SELECT 
    '🎉 HYBRID MATERIALIZATION FEATURE ENGINEERING COMPLETE!' AS status,
    COUNT(*) as total_records,
    COUNT(DISTINCT device_id) as unique_devices,
    MIN(timestamp) as earliest_record,
    MAX(timestamp) as latest_record,
    ROUND(AVG(CASE WHEN is_stationary THEN 1 ELSE 0 END) * 100, 1) as pct_stationary,
    ROUND(AVG(load_weight_smoothed), 1) as avg_load_weight,
    COUNT(CASE WHEN is_heavy_load THEN 1 END) as heavy_load_records,
    COUNT(CASE WHEN is_ready_for_load THEN 1 END) as ready_for_load_records
FROM "03_primary_feature_table";]
(Background on this error at: https://sqlalche.me/e/20/f405)
2025-09-03 05:50:47,879 - ERROR - High-performance pipeline failed: (psycopg2.errors.WindowingError) window functions are not allowed in UPDATE
LINE 54:     altitude - LAG(altitude, 1) OVER (PARTITION BY device_id...
                        ^

[SQL: -- Hybrid Materialization Feature Engineering Pipeline
-- AHS Analytics Engine - DataMine V2
-- This approach uses temporary tables to maximize parallel execution while handling window functions efficiently

-- Enable maximum parallelism for parallel-safe operations
SET max_parallel_workers_per_gather = 32;
SET work_mem = '8GB';
SET effective_cache_size = '250GB';
SET parallel_tuple_cost = 0.01;
SET parallel_setup_cost = 10.0;

-- ============================================================================
-- STEP 1: Create Base Table with Maximum Parallelism
-- This step processes ~8.8M records using all available CPU cores
-- ============================================================================

SELECT 'STEP 1: Creating base features table with parallel processing...' AS status;

DROP TABLE IF EXISTS temp_base_features;
CREATE TEMP TABLE temp_base_features AS
SELECT
    t.*,
    -- Extract altitude directly from PostGIS geometry (parallel-safe)
    ST_Z(t.current_position::geometry) AS altitude,
    -- Geospatial classification (parallel-safe)
    CASE
        WHEN ST_Z(t.current_position::geometry) < 200 THEN 'Loading Zone A'
        WHEN ST_Z(t.current_position::geometry) > 250 THEN 'Dump Zone B'
        ELSE 'Haul Road / Other'
    END AS location_type,
    -- Basic boolean features (parallel-safe)
    (current_speed < 0.5) AS is_stationary
FROM "02_raw_telemetry_transformed" t
WHERE t.current_position IS NOT NULL;

-- Create critical indexes on temp table for efficient window function processing
CREATE INDEX ON temp_base_features (device_id, timestamp);
CREATE INDEX ON temp_base_features (raw_event_hash_id);

SELECT 'STEP 1 Complete: Base table created with ' || COUNT(*) || ' records' AS status
FROM temp_base_features;

-- ============================================================================
-- STEP 2: Add Window Function Features Sequentially (Single-threaded but Fast)
-- These operations are memory-bound and very efficient on the indexed temp table
-- ============================================================================

SELECT 'STEP 2: Adding kinematic features...' AS status;

-- A. Altitude rate of change
ALTER TABLE temp_base_features ADD COLUMN altitude_rate_of_change DOUBLE PRECISION;
UPDATE temp_base_features 
SET altitude_rate_of_change = (
    altitude - LAG(altitude, 1) OVER (PARTITION BY device_id ORDER BY timestamp)
);

-- B. Speed rolling average (5-point window)
ALTER TABLE temp_base_features ADD COLUMN speed_rolling_avg_5s DOUBLE PRECISION;
UPDATE temp_base_features 
SET speed_rolling_avg_5s = (
    AVG(current_speed) OVER (
        PARTITION BY device_id 
        ORDER BY timestamp 
        ROWS BETWEEN 2 PRECEDING AND 2 FOLLOWING
    )
);

SELECT 'STEP 2A Complete: Kinematic features added' AS status;

-- ============================================================================
-- STEP 3: Add Payload Features
-- ============================================================================

SELECT 'STEP 3: Adding payload features...' AS status;

-- C. Load weight smoothed (5-point rolling average)
ALTER TABLE temp_base_features ADD COLUMN load_weight_smoothed DOUBLE PRECISION;
UPDATE temp_base_features 
SET load_weight_smoothed = (
    AVG(load_weight) OVER (
        PARTITION BY device_id 
        ORDER BY timestamp 
        ROWS BETWEEN 2 PRECEDING AND 2 FOLLOWING
    )
);

-- D. Load weight rate of change
ALTER TABLE temp_base_features ADD COLUMN load_weight_rate_of_change DOUBLE PRECISION;
UPDATE temp_base_features 
SET load_weight_rate_of_change = (
    load_weight_smoothed - LAG(load_weight_smoothed, 1) OVER (
        PARTITION BY device_id ORDER BY timestamp
    )
);

SELECT 'STEP 3 Complete: Payload features added' AS status;

-- ============================================================================
-- STEP 4: Add Sensor Health Features
-- ============================================================================

SELECT 'STEP 4: Adding sensor health features...' AS status;

-- E. Sensor reliability flag (device-level standard deviation)
ALTER TABLE temp_base_features ADD COLUMN has_reliable_payload BOOLEAN;
UPDATE temp_base_features 
SET has_reliable_payload = (
    STDDEV(load_weight) OVER (PARTITION BY device_id) > 1000
);

SELECT 'STEP 4 Complete: Sensor health features added' AS status;

-- ============================================================================
-- STEP 5: Add Duration Features (Most Complex Window Functions)
-- ============================================================================

SELECT 'STEP 5: Adding duration features...' AS status;

-- F. Identify stationary state changes
ALTER TABLE temp_base_features ADD COLUMN prev_stationary BOOLEAN;
UPDATE temp_base_features 
SET prev_stationary = LAG(is_stationary, 1, is_stationary) OVER (
    PARTITION BY device_id ORDER BY timestamp
);

-- G. Calculate time deltas
ALTER TABLE temp_base_features ADD COLUMN time_delta DOUBLE PRECISION;
UPDATE temp_base_features 
SET time_delta = COALESCE(
    EXTRACT(EPOCH FROM (
        timestamp - LAG(timestamp, 1) OVER (
            PARTITION BY device_id ORDER BY timestamp
        )
    )), 0
);

-- H. Create stationary block IDs
ALTER TABLE temp_base_features ADD COLUMN stationary_block_id INTEGER;
UPDATE temp_base_features 
SET stationary_block_id = SUM(
    CASE WHEN is_stationary != prev_stationary THEN 1 ELSE 0 END
) OVER (PARTITION BY device_id ORDER BY timestamp);

-- I. Calculate time in stationary state
ALTER TABLE temp_base_features ADD COLUMN time_in_stationary_state DOUBLE PRECISION;
UPDATE temp_base_features 
SET time_in_stationary_state = CASE
    WHEN is_stationary THEN
        SUM(time_delta) OVER (
            PARTITION BY device_id, stationary_block_id
            ORDER BY timestamp
        )
    ELSE 0
END;

SELECT 'STEP 5 Complete: Duration features added' AS status;

-- ============================================================================
-- STEP 6: Create Final Table with All Features (Parallel-Safe Final Step)
-- ============================================================================

SELECT 'STEP 6: Creating final feature table...' AS status;

DROP TABLE IF EXISTS "03_primary_feature_table";
CREATE TABLE "03_primary_feature_table" AS
SELECT
    -- === ORIGINAL COLUMNS ===
    timestamp,
    ingested_at,
    raw_event_hash_id,
    device_id,
    device_date,
    system_engaged,
    parking_brake_applied,
    current_position,
    current_speed,
    load_weight,
    state,
    software_state,
    prndl,
    extras,
    
    -- === ENGINEERED FEATURES ===
    
    -- Geospatial Features
    location_type,
    
    -- Kinematic Features
    is_stationary,
    altitude,
    altitude_rate_of_change,
    speed_rolling_avg_5s,
    
    -- System State Features (PRNDL one-hot encoding)
    (prndl = 'park') AS prndl_park,
    (prndl = 'reverse') AS prndl_reverse, 
    (prndl = 'neutral') AS prndl_neutral,
    (prndl = 'drive') AS prndl_drive,
    (prndl = 'unknown') AS prndl_unknown,
    
    -- Payload Features
    load_weight_smoothed,
    load_weight_rate_of_change,
    
    -- Sensor Health Features
    has_reliable_payload,
    
    -- Duration Features
    time_in_stationary_state,
    
    -- Interaction Features (composite features computed on the fly)
    (is_stationary AND prndl = 'neutral' AND parking_brake_applied) AS is_ready_for_load,
    (location_type = 'Haul Road / Other' AND NOT is_stationary) AS is_hauling,
    (is_stationary AND location_type LIKE '%%Loading%%') AS is_loading_position,
    (is_stationary AND location_type LIKE '%%Dump%%') AS is_dumping_position,
    (load_weight_smoothed > 50000) AS is_heavy_load
    
FROM temp_base_features
ORDER BY device_id, timestamp;

-- Clean up temporary table
DROP TABLE temp_base_features;

SELECT 'STEP 6 Complete: Final table created' AS status;

-- ============================================================================
-- STEP 7: Performance Optimizations
-- ============================================================================

SELECT 'STEP 7: Adding performance optimizations...' AS status;

-- Create primary key on raw_event_hash_id for fastest possible joins
ALTER TABLE "03_primary_feature_table" ADD PRIMARY KEY (raw_event_hash_id);

-- Create optimized indexes for common query patterns
CREATE INDEX CONCURRENTLY idx_03_primary_device_timestamp 
ON "03_primary_feature_table" (device_id, timestamp);

CREATE INDEX CONCURRENTLY idx_03_primary_stationary 
ON "03_primary_feature_table" (is_stationary) 
WHERE is_stationary = true;

CREATE INDEX CONCURRENTLY idx_03_primary_location 
ON "03_primary_feature_table" (location_type);

CREATE INDEX CONCURRENTLY idx_03_primary_heavy_load
ON "03_primary_feature_table" (is_heavy_load)
WHERE is_heavy_load = true;

-- Update table statistics for optimal query planning
ANALYZE "03_primary_feature_table";

SELECT 'STEP 7 Complete: All optimizations applied' AS status;

-- ============================================================================
-- FINAL SUMMARY: Success Report
-- ============================================================================

SELECT 
    '🎉 HYBRID MATERIALIZATION FEATURE ENGINEERING COMPLETE!' AS status,
    COUNT(*) as total_records,
    COUNT(DISTINCT device_id) as unique_devices,
    MIN(timestamp) as earliest_record,
    MAX(timestamp) as latest_record,
    ROUND(AVG(CASE WHEN is_stationary THEN 1 ELSE 0 END) * 100, 1) as pct_stationary,
    ROUND(AVG(load_weight_smoothed), 1) as avg_load_weight,
    COUNT(CASE WHEN is_heavy_load THEN 1 END) as heavy_load_records,
    COUNT(CASE WHEN is_ready_for_load THEN 1 END) as ready_for_load_records
FROM "03_primary_feature_table";]
(Background on this error at: https://sqlalche.me/e/20/f405)
2025-09-03 05:50:47,879 - ERROR - Check the logs above for specific error details
2025-09-03 05:50:47,879 - ERROR - ❌ Pipeline execution failed: (psycopg2.errors.WindowingError) window functions are not allowed in UPDATE
LINE 54:     altitude - LAG(altitude, 1) OVER (PARTITION BY device_id...
                        ^

[SQL: -- Hybrid Materialization Feature Engineering Pipeline
-- AHS Analytics Engine - DataMine V2
-- This approach uses temporary tables to maximize parallel execution while handling window functions efficiently

-- Enable maximum parallelism for parallel-safe operations
SET max_parallel_workers_per_gather = 32;
SET work_mem = '8GB';
SET effective_cache_size = '250GB';
SET parallel_tuple_cost = 0.01;
SET parallel_setup_cost = 10.0;

-- ============================================================================
-- STEP 1: Create Base Table with Maximum Parallelism
-- This step processes ~8.8M records using all available CPU cores
-- ============================================================================

SELECT 'STEP 1: Creating base features table with parallel processing...' AS status;

DROP TABLE IF EXISTS temp_base_features;
CREATE TEMP TABLE temp_base_features AS
SELECT
    t.*,
    -- Extract altitude directly from PostGIS geometry (parallel-safe)
    ST_Z(t.current_position::geometry) AS altitude,
    -- Geospatial classification (parallel-safe)
    CASE
        WHEN ST_Z(t.current_position::geometry) < 200 THEN 'Loading Zone A'
        WHEN ST_Z(t.current_position::geometry) > 250 THEN 'Dump Zone B'
        ELSE 'Haul Road / Other'
    END AS location_type,
    -- Basic boolean features (parallel-safe)
    (current_speed < 0.5) AS is_stationary
FROM "02_raw_telemetry_transformed" t
WHERE t.current_position IS NOT NULL;

-- Create critical indexes on temp table for efficient window function processing
CREATE INDEX ON temp_base_features (device_id, timestamp);
CREATE INDEX ON temp_base_features (raw_event_hash_id);

SELECT 'STEP 1 Complete: Base table created with ' || COUNT(*) || ' records' AS status
FROM temp_base_features;

-- ============================================================================
-- STEP 2: Add Window Function Features Sequentially (Single-threaded but Fast)
-- These operations are memory-bound and very efficient on the indexed temp table
-- ============================================================================

SELECT 'STEP 2: Adding kinematic features...' AS status;

-- A. Altitude rate of change
ALTER TABLE temp_base_features ADD COLUMN altitude_rate_of_change DOUBLE PRECISION;
UPDATE temp_base_features 
SET altitude_rate_of_change = (
    altitude - LAG(altitude, 1) OVER (PARTITION BY device_id ORDER BY timestamp)
);

-- B. Speed rolling average (5-point window)
ALTER TABLE temp_base_features ADD COLUMN speed_rolling_avg_5s DOUBLE PRECISION;
UPDATE temp_base_features 
SET speed_rolling_avg_5s = (
    AVG(current_speed) OVER (
        PARTITION BY device_id 
        ORDER BY timestamp 
        ROWS BETWEEN 2 PRECEDING AND 2 FOLLOWING
    )
);

SELECT 'STEP 2A Complete: Kinematic features added' AS status;

-- ============================================================================
-- STEP 3: Add Payload Features
-- ============================================================================

SELECT 'STEP 3: Adding payload features...' AS status;

-- C. Load weight smoothed (5-point rolling average)
ALTER TABLE temp_base_features ADD COLUMN load_weight_smoothed DOUBLE PRECISION;
UPDATE temp_base_features 
SET load_weight_smoothed = (
    AVG(load_weight) OVER (
        PARTITION BY device_id 
        ORDER BY timestamp 
        ROWS BETWEEN 2 PRECEDING AND 2 FOLLOWING
    )
);

-- D. Load weight rate of change
ALTER TABLE temp_base_features ADD COLUMN load_weight_rate_of_change DOUBLE PRECISION;
UPDATE temp_base_features 
SET load_weight_rate_of_change = (
    load_weight_smoothed - LAG(load_weight_smoothed, 1) OVER (
        PARTITION BY device_id ORDER BY timestamp
    )
);

SELECT 'STEP 3 Complete: Payload features added' AS status;

-- ============================================================================
-- STEP 4: Add Sensor Health Features
-- ============================================================================

SELECT 'STEP 4: Adding sensor health features...' AS status;

-- E. Sensor reliability flag (device-level standard deviation)
ALTER TABLE temp_base_features ADD COLUMN has_reliable_payload BOOLEAN;
UPDATE temp_base_features 
SET has_reliable_payload = (
    STDDEV(load_weight) OVER (PARTITION BY device_id) > 1000
);

SELECT 'STEP 4 Complete: Sensor health features added' AS status;

-- ============================================================================
-- STEP 5: Add Duration Features (Most Complex Window Functions)
-- ============================================================================

SELECT 'STEP 5: Adding duration features...' AS status;

-- F. Identify stationary state changes
ALTER TABLE temp_base_features ADD COLUMN prev_stationary BOOLEAN;
UPDATE temp_base_features 
SET prev_stationary = LAG(is_stationary, 1, is_stationary) OVER (
    PARTITION BY device_id ORDER BY timestamp
);

-- G. Calculate time deltas
ALTER TABLE temp_base_features ADD COLUMN time_delta DOUBLE PRECISION;
UPDATE temp_base_features 
SET time_delta = COALESCE(
    EXTRACT(EPOCH FROM (
        timestamp - LAG(timestamp, 1) OVER (
            PARTITION BY device_id ORDER BY timestamp
        )
    )), 0
);

-- H. Create stationary block IDs
ALTER TABLE temp_base_features ADD COLUMN stationary_block_id INTEGER;
UPDATE temp_base_features 
SET stationary_block_id = SUM(
    CASE WHEN is_stationary != prev_stationary THEN 1 ELSE 0 END
) OVER (PARTITION BY device_id ORDER BY timestamp);

-- I. Calculate time in stationary state
ALTER TABLE temp_base_features ADD COLUMN time_in_stationary_state DOUBLE PRECISION;
UPDATE temp_base_features 
SET time_in_stationary_state = CASE
    WHEN is_stationary THEN
        SUM(time_delta) OVER (
            PARTITION BY device_id, stationary_block_id
            ORDER BY timestamp
        )
    ELSE 0
END;

SELECT 'STEP 5 Complete: Duration features added' AS status;

-- ============================================================================
-- STEP 6: Create Final Table with All Features (Parallel-Safe Final Step)
-- ============================================================================

SELECT 'STEP 6: Creating final feature table...' AS status;

DROP TABLE IF EXISTS "03_primary_feature_table";
CREATE TABLE "03_primary_feature_table" AS
SELECT
    -- === ORIGINAL COLUMNS ===
    timestamp,
    ingested_at,
    raw_event_hash_id,
    device_id,
    device_date,
    system_engaged,
    parking_brake_applied,
    current_position,
    current_speed,
    load_weight,
    state,
    software_state,
    prndl,
    extras,
    
    -- === ENGINEERED FEATURES ===
    
    -- Geospatial Features
    location_type,
    
    -- Kinematic Features
    is_stationary,
    altitude,
    altitude_rate_of_change,
    speed_rolling_avg_5s,
    
    -- System State Features (PRNDL one-hot encoding)
    (prndl = 'park') AS prndl_park,
    (prndl = 'reverse') AS prndl_reverse, 
    (prndl = 'neutral') AS prndl_neutral,
    (prndl = 'drive') AS prndl_drive,
    (prndl = 'unknown') AS prndl_unknown,
    
    -- Payload Features
    load_weight_smoothed,
    load_weight_rate_of_change,
    
    -- Sensor Health Features
    has_reliable_payload,
    
    -- Duration Features
    time_in_stationary_state,
    
    -- Interaction Features (composite features computed on the fly)
    (is_stationary AND prndl = 'neutral' AND parking_brake_applied) AS is_ready_for_load,
    (location_type = 'Haul Road / Other' AND NOT is_stationary) AS is_hauling,
    (is_stationary AND location_type LIKE '%%Loading%%') AS is_loading_position,
    (is_stationary AND location_type LIKE '%%Dump%%') AS is_dumping_position,
    (load_weight_smoothed > 50000) AS is_heavy_load
    
FROM temp_base_features
ORDER BY device_id, timestamp;

-- Clean up temporary table
DROP TABLE temp_base_features;

SELECT 'STEP 6 Complete: Final table created' AS status;

-- ============================================================================
-- STEP 7: Performance Optimizations
-- ============================================================================

SELECT 'STEP 7: Adding performance optimizations...' AS status;

-- Create primary key on raw_event_hash_id for fastest possible joins
ALTER TABLE "03_primary_feature_table" ADD PRIMARY KEY (raw_event_hash_id);

-- Create optimized indexes for common query patterns
CREATE INDEX CONCURRENTLY idx_03_primary_device_timestamp 
ON "03_primary_feature_table" (device_id, timestamp);

CREATE INDEX CONCURRENTLY idx_03_primary_stationary 
ON "03_primary_feature_table" (is_stationary) 
WHERE is_stationary = true;

CREATE INDEX CONCURRENTLY idx_03_primary_location 
ON "03_primary_feature_table" (location_type);

CREATE INDEX CONCURRENTLY idx_03_primary_heavy_load
ON "03_primary_feature_table" (is_heavy_load)
WHERE is_heavy_load = true;

-- Update table statistics for optimal query planning
ANALYZE "03_primary_feature_table";

SELECT 'STEP 7 Complete: All optimizations applied' AS status;

-- ============================================================================
-- FINAL SUMMARY: Success Report
-- ============================================================================

SELECT 
    '🎉 HYBRID MATERIALIZATION FEATURE ENGINEERING COMPLETE!' AS status,
    COUNT(*) as total_records,
    COUNT(DISTINCT device_id) as unique_devices,
    MIN(timestamp) as earliest_record,
    MAX(timestamp) as latest_record,
    ROUND(AVG(CASE WHEN is_stationary THEN 1 ELSE 0 END) * 100, 1) as pct_stationary,
    ROUND(AVG(load_weight_smoothed), 1) as avg_load_weight,
    COUNT(CASE WHEN is_heavy_load THEN 1 END) as heavy_load_records,
    COUNT(CASE WHEN is_ready_for_load THEN 1 END) as ready_for_load_records
FROM "03_primary_feature_table";]
(Background on this error at: https://sqlalche.me/e/20/f405)
2025-09-03 05:56:26,064 - INFO - Connected to database with high-performance configuration
2025-09-03 05:56:26,064 - INFO - ======================================================================
2025-09-03 05:56:26,064 - INFO - STARTING HIGH-PERFORMANCE DATABASE-CENTRIC FEATURE ENGINEERING
2025-09-03 05:56:26,064 - INFO - ======================================================================
2025-09-03 05:56:26,064 - INFO - 🚀 Leveraging 100 CPUs and 300GB RAM through PostgreSQL parallel processing
2025-09-03 05:56:26,064 - INFO - 🔗 Using raw_event_hash_id for optimal join performance
2025-09-03 05:56:26,064 - INFO - ✓ Loaded SQL script: generate_features_optimized_final.sql
2025-09-03 05:56:26,064 - INFO - Executing comprehensive feature engineering query...
2025-09-03 05:56:26,064 - INFO - This leverages all available CPU cores and memory for parallel processing
2025-09-03 05:56:26,076 - INFO - ✓ Database session optimized for high-performance processing
2025-09-03 05:56:50,076 - ERROR - SQL execution failed: (psycopg2.errors.WindowingError) window function calls cannot be nested
LINE 61:         WHEN is_stationary != LAG(is_stationary, 1, is_stati...
                                       ^

[SQL: -- Ultra-Optimized Two-Step Materialization Feature Engineering Pipeline
-- AHS Analytics Engine - DataMine V2
-- Minimizes disk I/O by using only TWO table operations: one parallel read+write, one sequential read+write
-- Maximum efficiency through single-pass window function calculations

-- Enable maximum parallelism and memory optimization
SET max_parallel_workers_per_gather = 32;
SET work_mem = '8GB';
SET effective_cache_size = '250GB';
SET parallel_tuple_cost = 0.01;
SET parallel_setup_cost = 10.0;

SELECT '🚀 ULTRA-OPTIMIZED TWO-STEP FEATURE ENGINEERING STARTING' AS status;

-- ============================================================================
-- STEP 1: PARALLEL MATERIALIZATION - Base Features with Maximum CPU Usage
-- This step leverages all 100 CPUs for parallel-safe operations
-- ============================================================================

SELECT 'STEP 1: Creating base features with MAXIMUM PARALLELISM...' AS status;

DROP TABLE IF EXISTS temp_base_features;
CREATE TEMP TABLE temp_base_features AS
SELECT
    t.*,
    -- Extract altitude directly from PostGIS geometry (parallel-safe)
    ST_Z(t.current_position::geometry) AS altitude,
    -- Basic stationary classification (parallel-safe)
    (t.current_speed < 0.5) AS is_stationary,
    -- Geospatial zone classification (parallel-safe)
    CASE
        WHEN ST_Z(t.current_position::geometry) < 200 THEN 'Loading Zone A'
        WHEN ST_Z(t.current_position::geometry) > 250 THEN 'Dump Zone B'
        ELSE 'Haul Road / Other'
    END AS location_type
FROM "02_raw_telemetry_transformed" t
WHERE t.current_position IS NOT NULL;

-- Critical index for efficient sequential processing in Step 2
CREATE INDEX ON temp_base_features (device_id, timestamp);

SELECT 'STEP 1 Complete: ' || COUNT(*) || ' records created with PARALLEL processing' AS status 
FROM temp_base_features;

-- ============================================================================
-- STEP 2: EFFICIENT SEQUENTIAL - All Window Functions in Single Operation
-- Reads temp table ONCE, calculates ALL window features in memory, writes final table ONCE
-- ============================================================================

SELECT 'STEP 2: Creating final table with ALL window features in single operation...' AS status;

DROP TABLE IF EXISTS "03_primary_feature_table";
CREATE TABLE "03_primary_feature_table" AS
WITH
  -- Pre-calculate duration features for stationary state analysis
  duration_prep AS (
    SELECT
      *,
      -- Identify consecutive stationary state changes
      SUM(CASE 
        WHEN is_stationary != LAG(is_stationary, 1, is_stationary) OVER w THEN 1 
        ELSE 0 
      END) OVER w AS stationary_block_id,
      
      -- Calculate time deltas between consecutive records
      COALESCE(
        EXTRACT(EPOCH FROM (
          timestamp - LAG(timestamp, 1) OVER w
        )), 0
      ) AS time_delta
    FROM temp_base_features
    WINDOW w AS (PARTITION BY device_id ORDER BY timestamp)
  )

SELECT
  -- === ORIGINAL COLUMNS ===
  timestamp,
  ingested_at,
  raw_event_hash_id,  -- Primary key for fastest joins
  device_id,
  device_date,
  system_engaged,
  parking_brake_applied,
  current_position,
  current_speed,
  load_weight,
  state,
  software_state,
  prndl,
  extras,

  -- === BASE FEATURES (from parallel step) ===
  location_type,
  is_stationary,
  altitude,

  -- === ALL WINDOW-BASED FEATURES (calculated in single pass) ===
  
  -- Kinematic Features
  (altitude - LAG(altitude, 1) OVER w) AS altitude_rate_of_change,
  AVG(current_speed) OVER (w ROWS BETWEEN 2 PRECEDING AND 2 FOLLOWING) AS speed_rolling_avg_5s,
  
  -- Payload Features (smoothed values and rates of change)
  AVG(load_weight) OVER (w ROWS BETWEEN 2 PRECEDING AND 2 FOLLOWING) AS load_weight_smoothed,
  (
    AVG(load_weight) OVER (w ROWS BETWEEN 2 PRECEDING AND 2 FOLLOWING) - 
    LAG(AVG(load_weight) OVER (w ROWS BETWEEN 2 PRECEDING AND 2 FOLLOWING), 1) OVER w
  ) AS load_weight_rate_of_change,
  
  -- Sensor Health Features
  (STDDEV(load_weight) OVER (PARTITION BY device_id) > 1000) AS has_reliable_payload,
  
  -- Duration Features (most complex - cumulative time in stationary blocks)
  CASE
    WHEN is_stationary THEN
      SUM(time_delta) OVER (PARTITION BY device_id, stationary_block_id ORDER BY timestamp)
    ELSE 0
  END AS time_in_stationary_state,
  
  -- === SYSTEM STATE FEATURES (PRNDL one-hot encoding) ===
  (prndl = 'park') AS prndl_park,
  (prndl = 'reverse') AS prndl_reverse, 
  (prndl = 'neutral') AS prndl_neutral,
  (prndl = 'drive') AS prndl_drive,
  (prndl = 'unknown') AS prndl_unknown,
  
  -- === INTERACTION FEATURES (composite features) ===
  (is_stationary AND prndl = 'neutral' AND parking_brake_applied) AS is_ready_for_load,
  (location_type = 'Haul Road / Other' AND NOT is_stationary) AS is_hauling,
  (is_stationary AND location_type LIKE '%%Loading%%') AS is_loading_position,
  (is_stationary AND location_type LIKE '%%Dump%%') AS is_dumping_position,
  (AVG(load_weight) OVER (w ROWS BETWEEN 2 PRECEDING AND 2 FOLLOWING) > 50000) AS is_heavy_load

FROM duration_prep
WINDOW w AS (PARTITION BY device_id ORDER BY timestamp)
ORDER BY device_id, timestamp;

-- Clean up temporary table immediately
DROP TABLE temp_base_features;

SELECT 'STEP 2 Complete: Final table created with ALL features in single operation' AS status;

-- ============================================================================
-- STEP 3: PERFORMANCE OPTIMIZATIONS
-- ============================================================================

SELECT 'STEP 3: Applying performance optimizations...' AS status;

-- Create primary key on raw_event_hash_id for fastest possible joins with other tables
ALTER TABLE "03_primary_feature_table" ADD PRIMARY KEY (raw_event_hash_id);

-- Create optimized indexes for common query patterns
CREATE INDEX CONCURRENTLY idx_03_primary_device_timestamp 
ON "03_primary_feature_table" (device_id, timestamp);

CREATE INDEX CONCURRENTLY idx_03_primary_stationary 
ON "03_primary_feature_table" (is_stationary) 
WHERE is_stationary = true;

CREATE INDEX CONCURRENTLY idx_03_primary_location 
ON "03_primary_feature_table" (location_type);

CREATE INDEX CONCURRENTLY idx_03_primary_heavy_load
ON "03_primary_feature_table" (is_heavy_load)
WHERE is_heavy_load = true;

CREATE INDEX CONCURRENTLY idx_03_primary_ready_for_load
ON "03_primary_feature_table" (is_ready_for_load)
WHERE is_ready_for_load = true;

-- Update table statistics for optimal query planning
ANALYZE "03_primary_feature_table";

SELECT 'STEP 3 Complete: All performance optimizations applied' AS status;

-- ============================================================================
-- FINAL SUCCESS REPORT WITH COMPREHENSIVE METRICS
-- ============================================================================

SELECT 
    '🎉 ULTRA-OPTIMIZED FEATURE ENGINEERING COMPLETE! 🎉' AS status,
    
    -- Basic metrics
    COUNT(*) as total_records,
    COUNT(DISTINCT device_id) as unique_devices,
    MIN(timestamp) as earliest_record,
    MAX(timestamp) as latest_record,
    
    -- Feature distribution metrics
    ROUND(AVG(CASE WHEN is_stationary THEN 1 ELSE 0 END) * 100, 1) || '%%' as pct_stationary,
    ROUND(AVG(load_weight_smoothed), 1) as avg_load_weight_smoothed,
    ROUND(AVG(speed_rolling_avg_5s), 2) as avg_speed_smoothed,
    
    -- Operational metrics
    COUNT(CASE WHEN is_heavy_load THEN 1 END) as heavy_load_records,
    COUNT(CASE WHEN is_ready_for_load THEN 1 END) as ready_for_load_records,
    COUNT(CASE WHEN is_hauling THEN 1 END) as hauling_records,
    COUNT(CASE WHEN is_loading_position THEN 1 END) as loading_position_records,
    COUNT(CASE WHEN is_dumping_position THEN 1 END) as dumping_position_records,
    
    -- Data quality metrics
    COUNT(CASE WHEN has_reliable_payload THEN 1 END) as reliable_payload_records,
    ROUND(MAX(time_in_stationary_state), 1) as max_stationary_duration_seconds
    
FROM "03_primary_feature_table";

SELECT 
    '✅ SUCCESS: Two-step materialization completed!' AS optimization_summary,
    'Step 1: Parallel base features (~8.8M records processed with 100 CPUs)' AS step1_summary,
    'Step 2: Single-pass window functions (all features calculated efficiently)' AS step2_summary,
    'Result: Comprehensive feature table ready for ML training!' AS final_result;]
(Background on this error at: https://sqlalche.me/e/20/f405)
2025-09-03 05:56:50,077 - ERROR - High-performance pipeline failed: (psycopg2.errors.WindowingError) window function calls cannot be nested
LINE 61:         WHEN is_stationary != LAG(is_stationary, 1, is_stati...
                                       ^

[SQL: -- Ultra-Optimized Two-Step Materialization Feature Engineering Pipeline
-- AHS Analytics Engine - DataMine V2
-- Minimizes disk I/O by using only TWO table operations: one parallel read+write, one sequential read+write
-- Maximum efficiency through single-pass window function calculations

-- Enable maximum parallelism and memory optimization
SET max_parallel_workers_per_gather = 32;
SET work_mem = '8GB';
SET effective_cache_size = '250GB';
SET parallel_tuple_cost = 0.01;
SET parallel_setup_cost = 10.0;

SELECT '🚀 ULTRA-OPTIMIZED TWO-STEP FEATURE ENGINEERING STARTING' AS status;

-- ============================================================================
-- STEP 1: PARALLEL MATERIALIZATION - Base Features with Maximum CPU Usage
-- This step leverages all 100 CPUs for parallel-safe operations
-- ============================================================================

SELECT 'STEP 1: Creating base features with MAXIMUM PARALLELISM...' AS status;

DROP TABLE IF EXISTS temp_base_features;
CREATE TEMP TABLE temp_base_features AS
SELECT
    t.*,
    -- Extract altitude directly from PostGIS geometry (parallel-safe)
    ST_Z(t.current_position::geometry) AS altitude,
    -- Basic stationary classification (parallel-safe)
    (t.current_speed < 0.5) AS is_stationary,
    -- Geospatial zone classification (parallel-safe)
    CASE
        WHEN ST_Z(t.current_position::geometry) < 200 THEN 'Loading Zone A'
        WHEN ST_Z(t.current_position::geometry) > 250 THEN 'Dump Zone B'
        ELSE 'Haul Road / Other'
    END AS location_type
FROM "02_raw_telemetry_transformed" t
WHERE t.current_position IS NOT NULL;

-- Critical index for efficient sequential processing in Step 2
CREATE INDEX ON temp_base_features (device_id, timestamp);

SELECT 'STEP 1 Complete: ' || COUNT(*) || ' records created with PARALLEL processing' AS status 
FROM temp_base_features;

-- ============================================================================
-- STEP 2: EFFICIENT SEQUENTIAL - All Window Functions in Single Operation
-- Reads temp table ONCE, calculates ALL window features in memory, writes final table ONCE
-- ============================================================================

SELECT 'STEP 2: Creating final table with ALL window features in single operation...' AS status;

DROP TABLE IF EXISTS "03_primary_feature_table";
CREATE TABLE "03_primary_feature_table" AS
WITH
  -- Pre-calculate duration features for stationary state analysis
  duration_prep AS (
    SELECT
      *,
      -- Identify consecutive stationary state changes
      SUM(CASE 
        WHEN is_stationary != LAG(is_stationary, 1, is_stationary) OVER w THEN 1 
        ELSE 0 
      END) OVER w AS stationary_block_id,
      
      -- Calculate time deltas between consecutive records
      COALESCE(
        EXTRACT(EPOCH FROM (
          timestamp - LAG(timestamp, 1) OVER w
        )), 0
      ) AS time_delta
    FROM temp_base_features
    WINDOW w AS (PARTITION BY device_id ORDER BY timestamp)
  )

SELECT
  -- === ORIGINAL COLUMNS ===
  timestamp,
  ingested_at,
  raw_event_hash_id,  -- Primary key for fastest joins
  device_id,
  device_date,
  system_engaged,
  parking_brake_applied,
  current_position,
  current_speed,
  load_weight,
  state,
  software_state,
  prndl,
  extras,

  -- === BASE FEATURES (from parallel step) ===
  location_type,
  is_stationary,
  altitude,

  -- === ALL WINDOW-BASED FEATURES (calculated in single pass) ===
  
  -- Kinematic Features
  (altitude - LAG(altitude, 1) OVER w) AS altitude_rate_of_change,
  AVG(current_speed) OVER (w ROWS BETWEEN 2 PRECEDING AND 2 FOLLOWING) AS speed_rolling_avg_5s,
  
  -- Payload Features (smoothed values and rates of change)
  AVG(load_weight) OVER (w ROWS BETWEEN 2 PRECEDING AND 2 FOLLOWING) AS load_weight_smoothed,
  (
    AVG(load_weight) OVER (w ROWS BETWEEN 2 PRECEDING AND 2 FOLLOWING) - 
    LAG(AVG(load_weight) OVER (w ROWS BETWEEN 2 PRECEDING AND 2 FOLLOWING), 1) OVER w
  ) AS load_weight_rate_of_change,
  
  -- Sensor Health Features
  (STDDEV(load_weight) OVER (PARTITION BY device_id) > 1000) AS has_reliable_payload,
  
  -- Duration Features (most complex - cumulative time in stationary blocks)
  CASE
    WHEN is_stationary THEN
      SUM(time_delta) OVER (PARTITION BY device_id, stationary_block_id ORDER BY timestamp)
    ELSE 0
  END AS time_in_stationary_state,
  
  -- === SYSTEM STATE FEATURES (PRNDL one-hot encoding) ===
  (prndl = 'park') AS prndl_park,
  (prndl = 'reverse') AS prndl_reverse, 
  (prndl = 'neutral') AS prndl_neutral,
  (prndl = 'drive') AS prndl_drive,
  (prndl = 'unknown') AS prndl_unknown,
  
  -- === INTERACTION FEATURES (composite features) ===
  (is_stationary AND prndl = 'neutral' AND parking_brake_applied) AS is_ready_for_load,
  (location_type = 'Haul Road / Other' AND NOT is_stationary) AS is_hauling,
  (is_stationary AND location_type LIKE '%%Loading%%') AS is_loading_position,
  (is_stationary AND location_type LIKE '%%Dump%%') AS is_dumping_position,
  (AVG(load_weight) OVER (w ROWS BETWEEN 2 PRECEDING AND 2 FOLLOWING) > 50000) AS is_heavy_load

FROM duration_prep
WINDOW w AS (PARTITION BY device_id ORDER BY timestamp)
ORDER BY device_id, timestamp;

-- Clean up temporary table immediately
DROP TABLE temp_base_features;

SELECT 'STEP 2 Complete: Final table created with ALL features in single operation' AS status;

-- ============================================================================
-- STEP 3: PERFORMANCE OPTIMIZATIONS
-- ============================================================================

SELECT 'STEP 3: Applying performance optimizations...' AS status;

-- Create primary key on raw_event_hash_id for fastest possible joins with other tables
ALTER TABLE "03_primary_feature_table" ADD PRIMARY KEY (raw_event_hash_id);

-- Create optimized indexes for common query patterns
CREATE INDEX CONCURRENTLY idx_03_primary_device_timestamp 
ON "03_primary_feature_table" (device_id, timestamp);

CREATE INDEX CONCURRENTLY idx_03_primary_stationary 
ON "03_primary_feature_table" (is_stationary) 
WHERE is_stationary = true;

CREATE INDEX CONCURRENTLY idx_03_primary_location 
ON "03_primary_feature_table" (location_type);

CREATE INDEX CONCURRENTLY idx_03_primary_heavy_load
ON "03_primary_feature_table" (is_heavy_load)
WHERE is_heavy_load = true;

CREATE INDEX CONCURRENTLY idx_03_primary_ready_for_load
ON "03_primary_feature_table" (is_ready_for_load)
WHERE is_ready_for_load = true;

-- Update table statistics for optimal query planning
ANALYZE "03_primary_feature_table";

SELECT 'STEP 3 Complete: All performance optimizations applied' AS status;

-- ============================================================================
-- FINAL SUCCESS REPORT WITH COMPREHENSIVE METRICS
-- ============================================================================

SELECT 
    '🎉 ULTRA-OPTIMIZED FEATURE ENGINEERING COMPLETE! 🎉' AS status,
    
    -- Basic metrics
    COUNT(*) as total_records,
    COUNT(DISTINCT device_id) as unique_devices,
    MIN(timestamp) as earliest_record,
    MAX(timestamp) as latest_record,
    
    -- Feature distribution metrics
    ROUND(AVG(CASE WHEN is_stationary THEN 1 ELSE 0 END) * 100, 1) || '%%' as pct_stationary,
    ROUND(AVG(load_weight_smoothed), 1) as avg_load_weight_smoothed,
    ROUND(AVG(speed_rolling_avg_5s), 2) as avg_speed_smoothed,
    
    -- Operational metrics
    COUNT(CASE WHEN is_heavy_load THEN 1 END) as heavy_load_records,
    COUNT(CASE WHEN is_ready_for_load THEN 1 END) as ready_for_load_records,
    COUNT(CASE WHEN is_hauling THEN 1 END) as hauling_records,
    COUNT(CASE WHEN is_loading_position THEN 1 END) as loading_position_records,
    COUNT(CASE WHEN is_dumping_position THEN 1 END) as dumping_position_records,
    
    -- Data quality metrics
    COUNT(CASE WHEN has_reliable_payload THEN 1 END) as reliable_payload_records,
    ROUND(MAX(time_in_stationary_state), 1) as max_stationary_duration_seconds
    
FROM "03_primary_feature_table";

SELECT 
    '✅ SUCCESS: Two-step materialization completed!' AS optimization_summary,
    'Step 1: Parallel base features (~8.8M records processed with 100 CPUs)' AS step1_summary,
    'Step 2: Single-pass window functions (all features calculated efficiently)' AS step2_summary,
    'Result: Comprehensive feature table ready for ML training!' AS final_result;]
(Background on this error at: https://sqlalche.me/e/20/f405)
2025-09-03 05:56:50,077 - ERROR - Check the logs above for specific error details
2025-09-03 05:56:50,077 - ERROR - ❌ Pipeline execution failed: (psycopg2.errors.WindowingError) window function calls cannot be nested
LINE 61:         WHEN is_stationary != LAG(is_stationary, 1, is_stati...
                                       ^

[SQL: -- Ultra-Optimized Two-Step Materialization Feature Engineering Pipeline
-- AHS Analytics Engine - DataMine V2
-- Minimizes disk I/O by using only TWO table operations: one parallel read+write, one sequential read+write
-- Maximum efficiency through single-pass window function calculations

-- Enable maximum parallelism and memory optimization
SET max_parallel_workers_per_gather = 32;
SET work_mem = '8GB';
SET effective_cache_size = '250GB';
SET parallel_tuple_cost = 0.01;
SET parallel_setup_cost = 10.0;

SELECT '🚀 ULTRA-OPTIMIZED TWO-STEP FEATURE ENGINEERING STARTING' AS status;

-- ============================================================================
-- STEP 1: PARALLEL MATERIALIZATION - Base Features with Maximum CPU Usage
-- This step leverages all 100 CPUs for parallel-safe operations
-- ============================================================================

SELECT 'STEP 1: Creating base features with MAXIMUM PARALLELISM...' AS status;

DROP TABLE IF EXISTS temp_base_features;
CREATE TEMP TABLE temp_base_features AS
SELECT
    t.*,
    -- Extract altitude directly from PostGIS geometry (parallel-safe)
    ST_Z(t.current_position::geometry) AS altitude,
    -- Basic stationary classification (parallel-safe)
    (t.current_speed < 0.5) AS is_stationary,
    -- Geospatial zone classification (parallel-safe)
    CASE
        WHEN ST_Z(t.current_position::geometry) < 200 THEN 'Loading Zone A'
        WHEN ST_Z(t.current_position::geometry) > 250 THEN 'Dump Zone B'
        ELSE 'Haul Road / Other'
    END AS location_type
FROM "02_raw_telemetry_transformed" t
WHERE t.current_position IS NOT NULL;

-- Critical index for efficient sequential processing in Step 2
CREATE INDEX ON temp_base_features (device_id, timestamp);

SELECT 'STEP 1 Complete: ' || COUNT(*) || ' records created with PARALLEL processing' AS status 
FROM temp_base_features;

-- ============================================================================
-- STEP 2: EFFICIENT SEQUENTIAL - All Window Functions in Single Operation
-- Reads temp table ONCE, calculates ALL window features in memory, writes final table ONCE
-- ============================================================================

SELECT 'STEP 2: Creating final table with ALL window features in single operation...' AS status;

DROP TABLE IF EXISTS "03_primary_feature_table";
CREATE TABLE "03_primary_feature_table" AS
WITH
  -- Pre-calculate duration features for stationary state analysis
  duration_prep AS (
    SELECT
      *,
      -- Identify consecutive stationary state changes
      SUM(CASE 
        WHEN is_stationary != LAG(is_stationary, 1, is_stationary) OVER w THEN 1 
        ELSE 0 
      END) OVER w AS stationary_block_id,
      
      -- Calculate time deltas between consecutive records
      COALESCE(
        EXTRACT(EPOCH FROM (
          timestamp - LAG(timestamp, 1) OVER w
        )), 0
      ) AS time_delta
    FROM temp_base_features
    WINDOW w AS (PARTITION BY device_id ORDER BY timestamp)
  )

SELECT
  -- === ORIGINAL COLUMNS ===
  timestamp,
  ingested_at,
  raw_event_hash_id,  -- Primary key for fastest joins
  device_id,
  device_date,
  system_engaged,
  parking_brake_applied,
  current_position,
  current_speed,
  load_weight,
  state,
  software_state,
  prndl,
  extras,

  -- === BASE FEATURES (from parallel step) ===
  location_type,
  is_stationary,
  altitude,

  -- === ALL WINDOW-BASED FEATURES (calculated in single pass) ===
  
  -- Kinematic Features
  (altitude - LAG(altitude, 1) OVER w) AS altitude_rate_of_change,
  AVG(current_speed) OVER (w ROWS BETWEEN 2 PRECEDING AND 2 FOLLOWING) AS speed_rolling_avg_5s,
  
  -- Payload Features (smoothed values and rates of change)
  AVG(load_weight) OVER (w ROWS BETWEEN 2 PRECEDING AND 2 FOLLOWING) AS load_weight_smoothed,
  (
    AVG(load_weight) OVER (w ROWS BETWEEN 2 PRECEDING AND 2 FOLLOWING) - 
    LAG(AVG(load_weight) OVER (w ROWS BETWEEN 2 PRECEDING AND 2 FOLLOWING), 1) OVER w
  ) AS load_weight_rate_of_change,
  
  -- Sensor Health Features
  (STDDEV(load_weight) OVER (PARTITION BY device_id) > 1000) AS has_reliable_payload,
  
  -- Duration Features (most complex - cumulative time in stationary blocks)
  CASE
    WHEN is_stationary THEN
      SUM(time_delta) OVER (PARTITION BY device_id, stationary_block_id ORDER BY timestamp)
    ELSE 0
  END AS time_in_stationary_state,
  
  -- === SYSTEM STATE FEATURES (PRNDL one-hot encoding) ===
  (prndl = 'park') AS prndl_park,
  (prndl = 'reverse') AS prndl_reverse, 
  (prndl = 'neutral') AS prndl_neutral,
  (prndl = 'drive') AS prndl_drive,
  (prndl = 'unknown') AS prndl_unknown,
  
  -- === INTERACTION FEATURES (composite features) ===
  (is_stationary AND prndl = 'neutral' AND parking_brake_applied) AS is_ready_for_load,
  (location_type = 'Haul Road / Other' AND NOT is_stationary) AS is_hauling,
  (is_stationary AND location_type LIKE '%%Loading%%') AS is_loading_position,
  (is_stationary AND location_type LIKE '%%Dump%%') AS is_dumping_position,
  (AVG(load_weight) OVER (w ROWS BETWEEN 2 PRECEDING AND 2 FOLLOWING) > 50000) AS is_heavy_load

FROM duration_prep
WINDOW w AS (PARTITION BY device_id ORDER BY timestamp)
ORDER BY device_id, timestamp;

-- Clean up temporary table immediately
DROP TABLE temp_base_features;

SELECT 'STEP 2 Complete: Final table created with ALL features in single operation' AS status;

-- ============================================================================
-- STEP 3: PERFORMANCE OPTIMIZATIONS
-- ============================================================================

SELECT 'STEP 3: Applying performance optimizations...' AS status;

-- Create primary key on raw_event_hash_id for fastest possible joins with other tables
ALTER TABLE "03_primary_feature_table" ADD PRIMARY KEY (raw_event_hash_id);

-- Create optimized indexes for common query patterns
CREATE INDEX CONCURRENTLY idx_03_primary_device_timestamp 
ON "03_primary_feature_table" (device_id, timestamp);

CREATE INDEX CONCURRENTLY idx_03_primary_stationary 
ON "03_primary_feature_table" (is_stationary) 
WHERE is_stationary = true;

CREATE INDEX CONCURRENTLY idx_03_primary_location 
ON "03_primary_feature_table" (location_type);

CREATE INDEX CONCURRENTLY idx_03_primary_heavy_load
ON "03_primary_feature_table" (is_heavy_load)
WHERE is_heavy_load = true;

CREATE INDEX CONCURRENTLY idx_03_primary_ready_for_load
ON "03_primary_feature_table" (is_ready_for_load)
WHERE is_ready_for_load = true;

-- Update table statistics for optimal query planning
ANALYZE "03_primary_feature_table";

SELECT 'STEP 3 Complete: All performance optimizations applied' AS status;

-- ============================================================================
-- FINAL SUCCESS REPORT WITH COMPREHENSIVE METRICS
-- ============================================================================

SELECT 
    '🎉 ULTRA-OPTIMIZED FEATURE ENGINEERING COMPLETE! 🎉' AS status,
    
    -- Basic metrics
    COUNT(*) as total_records,
    COUNT(DISTINCT device_id) as unique_devices,
    MIN(timestamp) as earliest_record,
    MAX(timestamp) as latest_record,
    
    -- Feature distribution metrics
    ROUND(AVG(CASE WHEN is_stationary THEN 1 ELSE 0 END) * 100, 1) || '%%' as pct_stationary,
    ROUND(AVG(load_weight_smoothed), 1) as avg_load_weight_smoothed,
    ROUND(AVG(speed_rolling_avg_5s), 2) as avg_speed_smoothed,
    
    -- Operational metrics
    COUNT(CASE WHEN is_heavy_load THEN 1 END) as heavy_load_records,
    COUNT(CASE WHEN is_ready_for_load THEN 1 END) as ready_for_load_records,
    COUNT(CASE WHEN is_hauling THEN 1 END) as hauling_records,
    COUNT(CASE WHEN is_loading_position THEN 1 END) as loading_position_records,
    COUNT(CASE WHEN is_dumping_position THEN 1 END) as dumping_position_records,
    
    -- Data quality metrics
    COUNT(CASE WHEN has_reliable_payload THEN 1 END) as reliable_payload_records,
    ROUND(MAX(time_in_stationary_state), 1) as max_stationary_duration_seconds
    
FROM "03_primary_feature_table";

SELECT 
    '✅ SUCCESS: Two-step materialization completed!' AS optimization_summary,
    'Step 1: Parallel base features (~8.8M records processed with 100 CPUs)' AS step1_summary,
    'Step 2: Single-pass window functions (all features calculated efficiently)' AS step2_summary,
    'Result: Comprehensive feature table ready for ML training!' AS final_result;]
(Background on this error at: https://sqlalche.me/e/20/f405)
2025-09-03 05:57:59,882 - INFO - Connected to database with high-performance configuration
2025-09-03 05:57:59,882 - INFO - ======================================================================
2025-09-03 05:57:59,882 - INFO - STARTING HIGH-PERFORMANCE DATABASE-CENTRIC FEATURE ENGINEERING
2025-09-03 05:57:59,882 - INFO - ======================================================================
2025-09-03 05:57:59,882 - INFO - 🚀 Leveraging 100 CPUs and 300GB RAM through PostgreSQL parallel processing
2025-09-03 05:57:59,882 - INFO - 🔗 Using raw_event_hash_id for optimal join performance
2025-09-03 05:57:59,882 - INFO - ✓ Loaded SQL script: generate_features_optimized_final.sql
2025-09-03 05:57:59,882 - INFO - Executing comprehensive feature engineering query...
2025-09-03 05:57:59,882 - INFO - This leverages all available CPU cores and memory for parallel processing
2025-09-03 05:57:59,894 - INFO - ✓ Database session optimized for high-performance processing
2025-09-03 05:58:23,807 - ERROR - SQL execution failed: (psycopg2.errors.WindowingError) window function calls cannot be nested
LINE 61:         WHEN is_stationary != LAG(is_stationary, 1, is_stati...
                                       ^

[SQL: -- Ultra-Optimized Two-Step Materialization Feature Engineering Pipeline
-- AHS Analytics Engine - DataMine V2
-- Minimizes disk I/O by using only TWO table operations: one parallel read+write, one sequential read+write
-- Maximum efficiency through single-pass window function calculations

-- Enable maximum parallelism and memory optimization
SET max_parallel_workers_per_gather = 32;
SET work_mem = '8GB';
SET effective_cache_size = '250GB';
SET parallel_tuple_cost = 0.01;
SET parallel_setup_cost = 10.0;

SELECT '🚀 ULTRA-OPTIMIZED TWO-STEP FEATURE ENGINEERING STARTING' AS status;

-- ============================================================================
-- STEP 1: PARALLEL MATERIALIZATION - Base Features with Maximum CPU Usage
-- This step leverages all 100 CPUs for parallel-safe operations
-- ============================================================================

SELECT 'STEP 1: Creating base features with MAXIMUM PARALLELISM...' AS status;

DROP TABLE IF EXISTS temp_base_features;
CREATE TEMP TABLE temp_base_features AS
SELECT
    t.*,
    -- Extract altitude directly from PostGIS geometry (parallel-safe)
    ST_Z(t.current_position::geometry) AS altitude,
    -- Basic stationary classification (parallel-safe)
    (t.current_speed < 0.5) AS is_stationary,
    -- Geospatial zone classification (parallel-safe)
    CASE
        WHEN ST_Z(t.current_position::geometry) < 200 THEN 'Loading Zone A'
        WHEN ST_Z(t.current_position::geometry) > 250 THEN 'Dump Zone B'
        ELSE 'Haul Road / Other'
    END AS location_type
FROM "02_raw_telemetry_transformed" t
WHERE t.current_position IS NOT NULL;

-- Critical index for efficient sequential processing in Step 2
CREATE INDEX ON temp_base_features (device_id, timestamp);

SELECT 'STEP 1 Complete: ' || COUNT(*) || ' records created with PARALLEL processing' AS status 
FROM temp_base_features;

-- ============================================================================
-- STEP 2: EFFICIENT SEQUENTIAL - All Window Functions in Single Operation
-- Reads temp table ONCE, calculates ALL window features in memory, writes final table ONCE
-- ============================================================================

SELECT 'STEP 2: Creating final table with ALL window features in single operation...' AS status;

DROP TABLE IF EXISTS "03_primary_feature_table";
CREATE TABLE "03_primary_feature_table" AS
WITH
  -- Pre-calculate duration features and smoothed load weight
  duration_prep AS (
    SELECT
      *,
      -- Identify consecutive stationary state changes
      SUM(CASE 
        WHEN is_stationary != LAG(is_stationary, 1, is_stationary) OVER w THEN 1 
        ELSE 0 
      END) OVER w AS stationary_block_id,
      
      -- Calculate time deltas between consecutive records
      COALESCE(
        EXTRACT(EPOCH FROM (
          timestamp - LAG(timestamp, 1) OVER w
        )), 0
      ) AS time_delta,
      
      -- Pre-calculate smoothed load weight to avoid nested window functions
      AVG(load_weight) OVER (w ROWS BETWEEN 2 PRECEDING AND 2 FOLLOWING) AS smoothed_load_weight
    FROM temp_base_features
    WINDOW w AS (PARTITION BY device_id ORDER BY timestamp)
  )

SELECT
  -- === ORIGINAL COLUMNS ===
  timestamp,
  ingested_at,
  raw_event_hash_id,  -- Primary key for fastest joins
  device_id,
  device_date,
  system_engaged,
  parking_brake_applied,
  current_position,
  current_speed,
  load_weight,
  state,
  software_state,
  prndl,
  extras,

  -- === BASE FEATURES (from parallel step) ===
  location_type,
  is_stationary,
  altitude,

  -- === ALL WINDOW-BASED FEATURES (calculated in single pass) ===
  
  -- Kinematic Features
  (altitude - LAG(altitude, 1) OVER w) AS altitude_rate_of_change,
  AVG(current_speed) OVER (w ROWS BETWEEN 2 PRECEDING AND 2 FOLLOWING) AS speed_rolling_avg_5s,
  
  -- Payload Features (using pre-calculated smoothed values)
  smoothed_load_weight AS load_weight_smoothed,
  (smoothed_load_weight - LAG(smoothed_load_weight, 1) OVER w) AS load_weight_rate_of_change,
  
  -- Sensor Health Features
  (STDDEV(load_weight) OVER (PARTITION BY device_id) > 1000) AS has_reliable_payload,
  
  -- Duration Features (most complex - cumulative time in stationary blocks)
  CASE
    WHEN is_stationary THEN
      SUM(time_delta) OVER (PARTITION BY device_id, stationary_block_id ORDER BY timestamp)
    ELSE 0
  END AS time_in_stationary_state,
  
  -- === SYSTEM STATE FEATURES (PRNDL one-hot encoding) ===
  (prndl = 'park') AS prndl_park,
  (prndl = 'reverse') AS prndl_reverse, 
  (prndl = 'neutral') AS prndl_neutral,
  (prndl = 'drive') AS prndl_drive,
  (prndl = 'unknown') AS prndl_unknown,
  
  -- === INTERACTION FEATURES (composite features) ===
  (is_stationary AND prndl = 'neutral' AND parking_brake_applied) AS is_ready_for_load,
  (location_type = 'Haul Road / Other' AND NOT is_stationary) AS is_hauling,
  (is_stationary AND location_type LIKE '%%Loading%%') AS is_loading_position,
  (is_stationary AND location_type LIKE '%%Dump%%') AS is_dumping_position,
  (smoothed_load_weight > 50000) AS is_heavy_load

FROM duration_prep
WINDOW w AS (PARTITION BY device_id ORDER BY timestamp)
ORDER BY device_id, timestamp;

-- Clean up temporary table immediately
DROP TABLE temp_base_features;

SELECT 'STEP 2 Complete: Final table created with ALL features in single operation' AS status;

-- ============================================================================
-- STEP 3: PERFORMANCE OPTIMIZATIONS
-- ============================================================================

SELECT 'STEP 3: Applying performance optimizations...' AS status;

-- Create primary key on raw_event_hash_id for fastest possible joins with other tables
ALTER TABLE "03_primary_feature_table" ADD PRIMARY KEY (raw_event_hash_id);

-- Create optimized indexes for common query patterns
CREATE INDEX CONCURRENTLY idx_03_primary_device_timestamp 
ON "03_primary_feature_table" (device_id, timestamp);

CREATE INDEX CONCURRENTLY idx_03_primary_stationary 
ON "03_primary_feature_table" (is_stationary) 
WHERE is_stationary = true;

CREATE INDEX CONCURRENTLY idx_03_primary_location 
ON "03_primary_feature_table" (location_type);

CREATE INDEX CONCURRENTLY idx_03_primary_heavy_load
ON "03_primary_feature_table" (is_heavy_load)
WHERE is_heavy_load = true;

CREATE INDEX CONCURRENTLY idx_03_primary_ready_for_load
ON "03_primary_feature_table" (is_ready_for_load)
WHERE is_ready_for_load = true;

-- Update table statistics for optimal query planning
ANALYZE "03_primary_feature_table";

SELECT 'STEP 3 Complete: All performance optimizations applied' AS status;

-- ============================================================================
-- FINAL SUCCESS REPORT WITH COMPREHENSIVE METRICS
-- ============================================================================

SELECT 
    '🎉 ULTRA-OPTIMIZED FEATURE ENGINEERING COMPLETE! 🎉' AS status,
    
    -- Basic metrics
    COUNT(*) as total_records,
    COUNT(DISTINCT device_id) as unique_devices,
    MIN(timestamp) as earliest_record,
    MAX(timestamp) as latest_record,
    
    -- Feature distribution metrics
    ROUND(AVG(CASE WHEN is_stationary THEN 1 ELSE 0 END) * 100, 1) || '%%' as pct_stationary,
    ROUND(AVG(load_weight_smoothed), 1) as avg_load_weight_smoothed,
    ROUND(AVG(speed_rolling_avg_5s), 2) as avg_speed_smoothed,
    
    -- Operational metrics
    COUNT(CASE WHEN is_heavy_load THEN 1 END) as heavy_load_records,
    COUNT(CASE WHEN is_ready_for_load THEN 1 END) as ready_for_load_records,
    COUNT(CASE WHEN is_hauling THEN 1 END) as hauling_records,
    COUNT(CASE WHEN is_loading_position THEN 1 END) as loading_position_records,
    COUNT(CASE WHEN is_dumping_position THEN 1 END) as dumping_position_records,
    
    -- Data quality metrics
    COUNT(CASE WHEN has_reliable_payload THEN 1 END) as reliable_payload_records,
    ROUND(MAX(time_in_stationary_state), 1) as max_stationary_duration_seconds
    
FROM "03_primary_feature_table";

SELECT 
    '✅ SUCCESS: Two-step materialization completed!' AS optimization_summary,
    'Step 1: Parallel base features (~8.8M records processed with 100 CPUs)' AS step1_summary,
    'Step 2: Single-pass window functions (all features calculated efficiently)' AS step2_summary,
    'Result: Comprehensive feature table ready for ML training!' AS final_result;]
(Background on this error at: https://sqlalche.me/e/20/f405)
2025-09-03 05:58:23,807 - ERROR - High-performance pipeline failed: (psycopg2.errors.WindowingError) window function calls cannot be nested
LINE 61:         WHEN is_stationary != LAG(is_stationary, 1, is_stati...
                                       ^

[SQL: -- Ultra-Optimized Two-Step Materialization Feature Engineering Pipeline
-- AHS Analytics Engine - DataMine V2
-- Minimizes disk I/O by using only TWO table operations: one parallel read+write, one sequential read+write
-- Maximum efficiency through single-pass window function calculations

-- Enable maximum parallelism and memory optimization
SET max_parallel_workers_per_gather = 32;
SET work_mem = '8GB';
SET effective_cache_size = '250GB';
SET parallel_tuple_cost = 0.01;
SET parallel_setup_cost = 10.0;

SELECT '🚀 ULTRA-OPTIMIZED TWO-STEP FEATURE ENGINEERING STARTING' AS status;

-- ============================================================================
-- STEP 1: PARALLEL MATERIALIZATION - Base Features with Maximum CPU Usage
-- This step leverages all 100 CPUs for parallel-safe operations
-- ============================================================================

SELECT 'STEP 1: Creating base features with MAXIMUM PARALLELISM...' AS status;

DROP TABLE IF EXISTS temp_base_features;
CREATE TEMP TABLE temp_base_features AS
SELECT
    t.*,
    -- Extract altitude directly from PostGIS geometry (parallel-safe)
    ST_Z(t.current_position::geometry) AS altitude,
    -- Basic stationary classification (parallel-safe)
    (t.current_speed < 0.5) AS is_stationary,
    -- Geospatial zone classification (parallel-safe)
    CASE
        WHEN ST_Z(t.current_position::geometry) < 200 THEN 'Loading Zone A'
        WHEN ST_Z(t.current_position::geometry) > 250 THEN 'Dump Zone B'
        ELSE 'Haul Road / Other'
    END AS location_type
FROM "02_raw_telemetry_transformed" t
WHERE t.current_position IS NOT NULL;

-- Critical index for efficient sequential processing in Step 2
CREATE INDEX ON temp_base_features (device_id, timestamp);

SELECT 'STEP 1 Complete: ' || COUNT(*) || ' records created with PARALLEL processing' AS status 
FROM temp_base_features;

-- ============================================================================
-- STEP 2: EFFICIENT SEQUENTIAL - All Window Functions in Single Operation
-- Reads temp table ONCE, calculates ALL window features in memory, writes final table ONCE
-- ============================================================================

SELECT 'STEP 2: Creating final table with ALL window features in single operation...' AS status;

DROP TABLE IF EXISTS "03_primary_feature_table";
CREATE TABLE "03_primary_feature_table" AS
WITH
  -- Pre-calculate duration features and smoothed load weight
  duration_prep AS (
    SELECT
      *,
      -- Identify consecutive stationary state changes
      SUM(CASE 
        WHEN is_stationary != LAG(is_stationary, 1, is_stationary) OVER w THEN 1 
        ELSE 0 
      END) OVER w AS stationary_block_id,
      
      -- Calculate time deltas between consecutive records
      COALESCE(
        EXTRACT(EPOCH FROM (
          timestamp - LAG(timestamp, 1) OVER w
        )), 0
      ) AS time_delta,
      
      -- Pre-calculate smoothed load weight to avoid nested window functions
      AVG(load_weight) OVER (w ROWS BETWEEN 2 PRECEDING AND 2 FOLLOWING) AS smoothed_load_weight
    FROM temp_base_features
    WINDOW w AS (PARTITION BY device_id ORDER BY timestamp)
  )

SELECT
  -- === ORIGINAL COLUMNS ===
  timestamp,
  ingested_at,
  raw_event_hash_id,  -- Primary key for fastest joins
  device_id,
  device_date,
  system_engaged,
  parking_brake_applied,
  current_position,
  current_speed,
  load_weight,
  state,
  software_state,
  prndl,
  extras,

  -- === BASE FEATURES (from parallel step) ===
  location_type,
  is_stationary,
  altitude,

  -- === ALL WINDOW-BASED FEATURES (calculated in single pass) ===
  
  -- Kinematic Features
  (altitude - LAG(altitude, 1) OVER w) AS altitude_rate_of_change,
  AVG(current_speed) OVER (w ROWS BETWEEN 2 PRECEDING AND 2 FOLLOWING) AS speed_rolling_avg_5s,
  
  -- Payload Features (using pre-calculated smoothed values)
  smoothed_load_weight AS load_weight_smoothed,
  (smoothed_load_weight - LAG(smoothed_load_weight, 1) OVER w) AS load_weight_rate_of_change,
  
  -- Sensor Health Features
  (STDDEV(load_weight) OVER (PARTITION BY device_id) > 1000) AS has_reliable_payload,
  
  -- Duration Features (most complex - cumulative time in stationary blocks)
  CASE
    WHEN is_stationary THEN
      SUM(time_delta) OVER (PARTITION BY device_id, stationary_block_id ORDER BY timestamp)
    ELSE 0
  END AS time_in_stationary_state,
  
  -- === SYSTEM STATE FEATURES (PRNDL one-hot encoding) ===
  (prndl = 'park') AS prndl_park,
  (prndl = 'reverse') AS prndl_reverse, 
  (prndl = 'neutral') AS prndl_neutral,
  (prndl = 'drive') AS prndl_drive,
  (prndl = 'unknown') AS prndl_unknown,
  
  -- === INTERACTION FEATURES (composite features) ===
  (is_stationary AND prndl = 'neutral' AND parking_brake_applied) AS is_ready_for_load,
  (location_type = 'Haul Road / Other' AND NOT is_stationary) AS is_hauling,
  (is_stationary AND location_type LIKE '%%Loading%%') AS is_loading_position,
  (is_stationary AND location_type LIKE '%%Dump%%') AS is_dumping_position,
  (smoothed_load_weight > 50000) AS is_heavy_load

FROM duration_prep
WINDOW w AS (PARTITION BY device_id ORDER BY timestamp)
ORDER BY device_id, timestamp;

-- Clean up temporary table immediately
DROP TABLE temp_base_features;

SELECT 'STEP 2 Complete: Final table created with ALL features in single operation' AS status;

-- ============================================================================
-- STEP 3: PERFORMANCE OPTIMIZATIONS
-- ============================================================================

SELECT 'STEP 3: Applying performance optimizations...' AS status;

-- Create primary key on raw_event_hash_id for fastest possible joins with other tables
ALTER TABLE "03_primary_feature_table" ADD PRIMARY KEY (raw_event_hash_id);

-- Create optimized indexes for common query patterns
CREATE INDEX CONCURRENTLY idx_03_primary_device_timestamp 
ON "03_primary_feature_table" (device_id, timestamp);

CREATE INDEX CONCURRENTLY idx_03_primary_stationary 
ON "03_primary_feature_table" (is_stationary) 
WHERE is_stationary = true;

CREATE INDEX CONCURRENTLY idx_03_primary_location 
ON "03_primary_feature_table" (location_type);

CREATE INDEX CONCURRENTLY idx_03_primary_heavy_load
ON "03_primary_feature_table" (is_heavy_load)
WHERE is_heavy_load = true;

CREATE INDEX CONCURRENTLY idx_03_primary_ready_for_load
ON "03_primary_feature_table" (is_ready_for_load)
WHERE is_ready_for_load = true;

-- Update table statistics for optimal query planning
ANALYZE "03_primary_feature_table";

SELECT 'STEP 3 Complete: All performance optimizations applied' AS status;

-- ============================================================================
-- FINAL SUCCESS REPORT WITH COMPREHENSIVE METRICS
-- ============================================================================

SELECT 
    '🎉 ULTRA-OPTIMIZED FEATURE ENGINEERING COMPLETE! 🎉' AS status,
    
    -- Basic metrics
    COUNT(*) as total_records,
    COUNT(DISTINCT device_id) as unique_devices,
    MIN(timestamp) as earliest_record,
    MAX(timestamp) as latest_record,
    
    -- Feature distribution metrics
    ROUND(AVG(CASE WHEN is_stationary THEN 1 ELSE 0 END) * 100, 1) || '%%' as pct_stationary,
    ROUND(AVG(load_weight_smoothed), 1) as avg_load_weight_smoothed,
    ROUND(AVG(speed_rolling_avg_5s), 2) as avg_speed_smoothed,
    
    -- Operational metrics
    COUNT(CASE WHEN is_heavy_load THEN 1 END) as heavy_load_records,
    COUNT(CASE WHEN is_ready_for_load THEN 1 END) as ready_for_load_records,
    COUNT(CASE WHEN is_hauling THEN 1 END) as hauling_records,
    COUNT(CASE WHEN is_loading_position THEN 1 END) as loading_position_records,
    COUNT(CASE WHEN is_dumping_position THEN 1 END) as dumping_position_records,
    
    -- Data quality metrics
    COUNT(CASE WHEN has_reliable_payload THEN 1 END) as reliable_payload_records,
    ROUND(MAX(time_in_stationary_state), 1) as max_stationary_duration_seconds
    
FROM "03_primary_feature_table";

SELECT 
    '✅ SUCCESS: Two-step materialization completed!' AS optimization_summary,
    'Step 1: Parallel base features (~8.8M records processed with 100 CPUs)' AS step1_summary,
    'Step 2: Single-pass window functions (all features calculated efficiently)' AS step2_summary,
    'Result: Comprehensive feature table ready for ML training!' AS final_result;]
(Background on this error at: https://sqlalche.me/e/20/f405)
2025-09-03 05:58:23,807 - ERROR - Check the logs above for specific error details
2025-09-03 05:58:23,807 - ERROR - ❌ Pipeline execution failed: (psycopg2.errors.WindowingError) window function calls cannot be nested
LINE 61:         WHEN is_stationary != LAG(is_stationary, 1, is_stati...
                                       ^

[SQL: -- Ultra-Optimized Two-Step Materialization Feature Engineering Pipeline
-- AHS Analytics Engine - DataMine V2
-- Minimizes disk I/O by using only TWO table operations: one parallel read+write, one sequential read+write
-- Maximum efficiency through single-pass window function calculations

-- Enable maximum parallelism and memory optimization
SET max_parallel_workers_per_gather = 32;
SET work_mem = '8GB';
SET effective_cache_size = '250GB';
SET parallel_tuple_cost = 0.01;
SET parallel_setup_cost = 10.0;

SELECT '🚀 ULTRA-OPTIMIZED TWO-STEP FEATURE ENGINEERING STARTING' AS status;

-- ============================================================================
-- STEP 1: PARALLEL MATERIALIZATION - Base Features with Maximum CPU Usage
-- This step leverages all 100 CPUs for parallel-safe operations
-- ============================================================================

SELECT 'STEP 1: Creating base features with MAXIMUM PARALLELISM...' AS status;

DROP TABLE IF EXISTS temp_base_features;
CREATE TEMP TABLE temp_base_features AS
SELECT
    t.*,
    -- Extract altitude directly from PostGIS geometry (parallel-safe)
    ST_Z(t.current_position::geometry) AS altitude,
    -- Basic stationary classification (parallel-safe)
    (t.current_speed < 0.5) AS is_stationary,
    -- Geospatial zone classification (parallel-safe)
    CASE
        WHEN ST_Z(t.current_position::geometry) < 200 THEN 'Loading Zone A'
        WHEN ST_Z(t.current_position::geometry) > 250 THEN 'Dump Zone B'
        ELSE 'Haul Road / Other'
    END AS location_type
FROM "02_raw_telemetry_transformed" t
WHERE t.current_position IS NOT NULL;

-- Critical index for efficient sequential processing in Step 2
CREATE INDEX ON temp_base_features (device_id, timestamp);

SELECT 'STEP 1 Complete: ' || COUNT(*) || ' records created with PARALLEL processing' AS status 
FROM temp_base_features;

-- ============================================================================
-- STEP 2: EFFICIENT SEQUENTIAL - All Window Functions in Single Operation
-- Reads temp table ONCE, calculates ALL window features in memory, writes final table ONCE
-- ============================================================================

SELECT 'STEP 2: Creating final table with ALL window features in single operation...' AS status;

DROP TABLE IF EXISTS "03_primary_feature_table";
CREATE TABLE "03_primary_feature_table" AS
WITH
  -- Pre-calculate duration features and smoothed load weight
  duration_prep AS (
    SELECT
      *,
      -- Identify consecutive stationary state changes
      SUM(CASE 
        WHEN is_stationary != LAG(is_stationary, 1, is_stationary) OVER w THEN 1 
        ELSE 0 
      END) OVER w AS stationary_block_id,
      
      -- Calculate time deltas between consecutive records
      COALESCE(
        EXTRACT(EPOCH FROM (
          timestamp - LAG(timestamp, 1) OVER w
        )), 0
      ) AS time_delta,
      
      -- Pre-calculate smoothed load weight to avoid nested window functions
      AVG(load_weight) OVER (w ROWS BETWEEN 2 PRECEDING AND 2 FOLLOWING) AS smoothed_load_weight
    FROM temp_base_features
    WINDOW w AS (PARTITION BY device_id ORDER BY timestamp)
  )

SELECT
  -- === ORIGINAL COLUMNS ===
  timestamp,
  ingested_at,
  raw_event_hash_id,  -- Primary key for fastest joins
  device_id,
  device_date,
  system_engaged,
  parking_brake_applied,
  current_position,
  current_speed,
  load_weight,
  state,
  software_state,
  prndl,
  extras,

  -- === BASE FEATURES (from parallel step) ===
  location_type,
  is_stationary,
  altitude,

  -- === ALL WINDOW-BASED FEATURES (calculated in single pass) ===
  
  -- Kinematic Features
  (altitude - LAG(altitude, 1) OVER w) AS altitude_rate_of_change,
  AVG(current_speed) OVER (w ROWS BETWEEN 2 PRECEDING AND 2 FOLLOWING) AS speed_rolling_avg_5s,
  
  -- Payload Features (using pre-calculated smoothed values)
  smoothed_load_weight AS load_weight_smoothed,
  (smoothed_load_weight - LAG(smoothed_load_weight, 1) OVER w) AS load_weight_rate_of_change,
  
  -- Sensor Health Features
  (STDDEV(load_weight) OVER (PARTITION BY device_id) > 1000) AS has_reliable_payload,
  
  -- Duration Features (most complex - cumulative time in stationary blocks)
  CASE
    WHEN is_stationary THEN
      SUM(time_delta) OVER (PARTITION BY device_id, stationary_block_id ORDER BY timestamp)
    ELSE 0
  END AS time_in_stationary_state,
  
  -- === SYSTEM STATE FEATURES (PRNDL one-hot encoding) ===
  (prndl = 'park') AS prndl_park,
  (prndl = 'reverse') AS prndl_reverse, 
  (prndl = 'neutral') AS prndl_neutral,
  (prndl = 'drive') AS prndl_drive,
  (prndl = 'unknown') AS prndl_unknown,
  
  -- === INTERACTION FEATURES (composite features) ===
  (is_stationary AND prndl = 'neutral' AND parking_brake_applied) AS is_ready_for_load,
  (location_type = 'Haul Road / Other' AND NOT is_stationary) AS is_hauling,
  (is_stationary AND location_type LIKE '%%Loading%%') AS is_loading_position,
  (is_stationary AND location_type LIKE '%%Dump%%') AS is_dumping_position,
  (smoothed_load_weight > 50000) AS is_heavy_load

FROM duration_prep
WINDOW w AS (PARTITION BY device_id ORDER BY timestamp)
ORDER BY device_id, timestamp;

-- Clean up temporary table immediately
DROP TABLE temp_base_features;

SELECT 'STEP 2 Complete: Final table created with ALL features in single operation' AS status;

-- ============================================================================
-- STEP 3: PERFORMANCE OPTIMIZATIONS
-- ============================================================================

SELECT 'STEP 3: Applying performance optimizations...' AS status;

-- Create primary key on raw_event_hash_id for fastest possible joins with other tables
ALTER TABLE "03_primary_feature_table" ADD PRIMARY KEY (raw_event_hash_id);

-- Create optimized indexes for common query patterns
CREATE INDEX CONCURRENTLY idx_03_primary_device_timestamp 
ON "03_primary_feature_table" (device_id, timestamp);

CREATE INDEX CONCURRENTLY idx_03_primary_stationary 
ON "03_primary_feature_table" (is_stationary) 
WHERE is_stationary = true;

CREATE INDEX CONCURRENTLY idx_03_primary_location 
ON "03_primary_feature_table" (location_type);

CREATE INDEX CONCURRENTLY idx_03_primary_heavy_load
ON "03_primary_feature_table" (is_heavy_load)
WHERE is_heavy_load = true;

CREATE INDEX CONCURRENTLY idx_03_primary_ready_for_load
ON "03_primary_feature_table" (is_ready_for_load)
WHERE is_ready_for_load = true;

-- Update table statistics for optimal query planning
ANALYZE "03_primary_feature_table";

SELECT 'STEP 3 Complete: All performance optimizations applied' AS status;

-- ============================================================================
-- FINAL SUCCESS REPORT WITH COMPREHENSIVE METRICS
-- ============================================================================

SELECT 
    '🎉 ULTRA-OPTIMIZED FEATURE ENGINEERING COMPLETE! 🎉' AS status,
    
    -- Basic metrics
    COUNT(*) as total_records,
    COUNT(DISTINCT device_id) as unique_devices,
    MIN(timestamp) as earliest_record,
    MAX(timestamp) as latest_record,
    
    -- Feature distribution metrics
    ROUND(AVG(CASE WHEN is_stationary THEN 1 ELSE 0 END) * 100, 1) || '%%' as pct_stationary,
    ROUND(AVG(load_weight_smoothed), 1) as avg_load_weight_smoothed,
    ROUND(AVG(speed_rolling_avg_5s), 2) as avg_speed_smoothed,
    
    -- Operational metrics
    COUNT(CASE WHEN is_heavy_load THEN 1 END) as heavy_load_records,
    COUNT(CASE WHEN is_ready_for_load THEN 1 END) as ready_for_load_records,
    COUNT(CASE WHEN is_hauling THEN 1 END) as hauling_records,
    COUNT(CASE WHEN is_loading_position THEN 1 END) as loading_position_records,
    COUNT(CASE WHEN is_dumping_position THEN 1 END) as dumping_position_records,
    
    -- Data quality metrics
    COUNT(CASE WHEN has_reliable_payload THEN 1 END) as reliable_payload_records,
    ROUND(MAX(time_in_stationary_state), 1) as max_stationary_duration_seconds
    
FROM "03_primary_feature_table";

SELECT 
    '✅ SUCCESS: Two-step materialization completed!' AS optimization_summary,
    'Step 1: Parallel base features (~8.8M records processed with 100 CPUs)' AS step1_summary,
    'Step 2: Single-pass window functions (all features calculated efficiently)' AS step2_summary,
    'Result: Comprehensive feature table ready for ML training!' AS final_result;]
(Background on this error at: https://sqlalche.me/e/20/f405)
2025-09-03 05:58:40,964 - INFO - Connected to database with high-performance configuration
2025-09-03 05:58:40,964 - INFO - ======================================================================
2025-09-03 05:58:40,964 - INFO - STARTING HIGH-PERFORMANCE DATABASE-CENTRIC FEATURE ENGINEERING
2025-09-03 05:58:40,964 - INFO - ======================================================================
2025-09-03 05:58:40,964 - INFO - 🚀 Leveraging 100 CPUs and 300GB RAM through PostgreSQL parallel processing
2025-09-03 05:58:40,964 - INFO - 🔗 Using raw_event_hash_id for optimal join performance
2025-09-03 05:58:40,964 - INFO - ✓ Loaded SQL script: generate_features_optimized_final.sql
2025-09-03 05:58:40,965 - INFO - Executing comprehensive feature engineering query...
2025-09-03 05:58:40,965 - INFO - This leverages all available CPU cores and memory for parallel processing
2025-09-03 05:58:40,977 - INFO - ✓ Database session optimized for high-performance processing
2025-09-03 06:08:15,383 - INFO - Connected to database with high-performance configuration
2025-09-03 06:08:15,383 - INFO - ======================================================================
2025-09-03 06:08:15,383 - INFO - STARTING HIGH-PERFORMANCE DATABASE-CENTRIC FEATURE ENGINEERING
2025-09-03 06:08:15,383 - INFO - ======================================================================
2025-09-03 06:08:15,383 - INFO - 🚀 Leveraging 100 CPUs and 300GB RAM through PostgreSQL parallel processing
2025-09-03 06:08:15,383 - INFO - 🔗 Using raw_event_hash_id for optimal join performance
2025-09-03 06:08:15,384 - INFO - ✓ Loaded SQL script: generate_features_optimized_final.sql
2025-09-03 06:08:15,384 - INFO - Executing comprehensive feature engineering query...
2025-09-03 06:08:15,384 - INFO - This leverages all available CPU cores and memory for parallel processing
2025-09-03 06:08:15,396 - INFO - ✓ Database session optimized for high-performance processing
2025-09-03 06:10:51,613 - INFO - Connected to database with high-performance configuration
2025-09-03 06:10:51,613 - INFO - ======================================================================
2025-09-03 06:10:51,613 - INFO - STARTING HIGH-PERFORMANCE DATABASE-CENTRIC FEATURE ENGINEERING
2025-09-03 06:10:51,613 - INFO - ======================================================================
2025-09-03 06:10:51,613 - INFO - 🚀 Leveraging 100 CPUs and 300GB RAM through PostgreSQL parallel processing
2025-09-03 06:10:51,613 - INFO - 🔗 Using raw_event_hash_id for optimal join performance
2025-09-03 06:10:51,613 - INFO - ✓ Loaded SQL script: generate_features_parallel_chunks.sql
2025-09-03 06:10:51,614 - INFO - Executing comprehensive feature engineering query...
2025-09-03 06:10:51,614 - INFO - This leverages all available CPU cores and memory for parallel processing
2025-09-03 06:10:51,625 - INFO - ✓ Database session optimized for high-performance processing
2025-09-03 06:17:39,003 - INFO - Connected to database with high-performance configuration
2025-09-03 06:17:39,003 - INFO - Initialized with 32 parallel workers.
2025-09-03 06:17:39,003 - INFO - ======================================================================
2025-09-03 06:17:39,003 - INFO - STARTING MASSIVELY PARALLEL FEATURE ENGINEERING
2025-09-03 06:17:39,003 - INFO - ======================================================================
2025-09-03 06:17:39,003 - INFO - Fetching all device-date chunks to be processed...
2025-09-03 06:17:39,454 - INFO - ✓ Divided 96 total chunks into 32 batches for parallel processing.
2025-09-03 06:17:39,454 - INFO - 🚀 Launching parallel workers...
2025-09-03 06:17:39,455 - INFO - [Worker-127239456814784] Started, assigned 3 chunks.
2025-09-03 06:17:39,456 - INFO - [Worker-127239448422080] Started, assigned 3 chunks.
2025-09-03 06:17:39,457 - INFO - [Worker-127239440029376] Started, assigned 3 chunks.
2025-09-03 06:17:39,457 - INFO - [Worker-127239431636672] Started, assigned 3 chunks.
2025-09-03 06:17:39,459 - INFO - [Worker-127239423243968] Started, assigned 3 chunks.
2025-09-03 06:17:39,459 - INFO - [Worker-127239077230272] Started, assigned 3 chunks.
2025-09-03 06:17:39,461 - INFO - [Worker-127239068837568] Started, assigned 3 chunks.
2025-09-03 06:17:39,461 - INFO - [Worker-127239060444864] Started, assigned 3 chunks.
2025-09-03 06:17:39,463 - INFO - [Worker-127239052052160] Started, assigned 3 chunks.
2025-09-03 06:17:39,463 - INFO - [Worker-127239043659456] Started, assigned 3 chunks.
2025-09-03 06:17:39,464 - INFO - [Worker-127239035266752] Started, assigned 3 chunks.
2025-09-03 06:17:39,464 - INFO - [Worker-127239026874048] Started, assigned 3 chunks.
2025-09-03 06:17:39,466 - INFO - [Worker-127238540359360] Started, assigned 3 chunks.
2025-09-03 06:17:39,467 - INFO - [Worker-127238531966656] Started, assigned 3 chunks.
2025-09-03 06:17:39,468 - INFO - [Worker-127238523573952] Started, assigned 3 chunks.
2025-09-03 06:17:39,469 - INFO - [Worker-127238515181248] Started, assigned 3 chunks.
2025-09-03 06:17:39,470 - INFO - [Worker-127238506788544] Started, assigned 3 chunks.
2025-09-03 06:17:39,473 - INFO - [Worker-127238498395840] Started, assigned 3 chunks.
2025-09-03 06:17:39,475 - ERROR - ❌ [Worker-127239431636672] Failed during SQL execution: (psycopg2.errors.SyntaxError) syntax error at or near "%"
LINE 17: WHERE t.device_date = ANY(%(chunk_list)s);
                                   ^

[SQL: -- This is a template script executed by each parallel Python worker.
-- It processes an assigned subset of device_date chunks.

-- Step A: Create a temporary table with base features FOR THIS CHUNK ONLY.
-- This uses the two-step materialization pattern correctly to avoid nested window function errors.
CREATE TEMP TABLE temp_base AS
SELECT
    t.*,
    ST_Z(t.current_position::geometry) AS altitude,
    (t.current_speed < 0.5) AS is_stationary,
    CASE
        WHEN ST_Z(t.current_position::geometry) < 200 THEN 'Loading Zone A'
        WHEN ST_Z(t.current_position::geometry) > 250 THEN 'Dump Zone B'
        ELSE 'Haul Road / Other'
    END AS location_type
FROM "02_raw_telemetry_transformed" t
WHERE t.device_date = ANY(%%(chunk_list)s);

-- Index the small temp table for the next step.
CREATE INDEX ON temp_base (device_id, device_date, timestamp);

-- Step B: Create the final results table FOR THIS CHUNK ONLY.
-- This table is uniquely named for each worker to avoid conflicts.
CREATE TEMP TABLE temp_features_127239431636672 AS
WITH
  basic_windows AS (
    SELECT
      *,
      LAG(is_stationary, 1, is_stationary) OVER w AS prev_stationary,
      COALESCE(EXTRACT(EPOCH FROM (timestamp - LAG(timestamp, 1) OVER w)), 0) AS time_delta,
      AVG(load_weight) OVER (w ROWS BETWEEN 2 PRECEDING AND 2 FOLLOWING) AS smoothed_load_weight
    FROM temp_base
    WINDOW w AS (PARTITION BY device_id, device_date ORDER BY timestamp)
  ),
  duration_prep AS (
    SELECT
      *,
      SUM(CASE WHEN is_stationary != prev_stationary THEN 1 ELSE 0 END) OVER w AS stationary_block_id
    FROM basic_windows
    WINDOW w AS (PARTITION BY device_id, device_date ORDER BY timestamp)
  )
SELECT
  -- Select all final columns and calculate the remaining window functions
  timestamp, ingested_at, raw_event_hash_id, device_id, device_date, system_engaged,
  parking_brake_applied, current_position, current_speed, load_weight, state,
  software_state, prndl, extras, location_type, is_stationary, altitude,
  (altitude - LAG(altitude, 1) OVER w) AS altitude_rate_of_change,
  AVG(current_speed) OVER (w ROWS BETWEEN 2 PRECEDING AND 2 FOLLOWING) AS speed_rolling_avg_5s,
  smoothed_load_weight AS load_weight_smoothed,
  (smoothed_load_weight - LAG(smoothed_load_weight, 1) OVER w) AS load_weight_rate_of_change,
  (STDDEV(load_weight) OVER (PARTITION BY device_id) > 1000) AS has_reliable_payload,
  CASE
    WHEN is_stationary THEN SUM(time_delta) OVER (PARTITION BY device_id, device_date, stationary_block_id ORDER BY timestamp)
    ELSE 0
  END AS time_in_stationary_state
FROM duration_prep
WINDOW w AS (PARTITION BY device_id, device_date ORDER BY timestamp);]
(Background on this error at: https://sqlalche.me/e/20/f405)
2025-09-03 06:17:39,476 - INFO - [Worker-127238490003136] Started, assigned 3 chunks.
2025-09-03 06:17:39,479 - ERROR - ❌ [Worker-127239448422080] Failed during SQL execution: (psycopg2.errors.SyntaxError) syntax error at or near "%"
LINE 17: WHERE t.device_date = ANY(%(chunk_list)s);
                                   ^

[SQL: -- This is a template script executed by each parallel Python worker.
-- It processes an assigned subset of device_date chunks.

-- Step A: Create a temporary table with base features FOR THIS CHUNK ONLY.
-- This uses the two-step materialization pattern correctly to avoid nested window function errors.
CREATE TEMP TABLE temp_base AS
SELECT
    t.*,
    ST_Z(t.current_position::geometry) AS altitude,
    (t.current_speed < 0.5) AS is_stationary,
    CASE
        WHEN ST_Z(t.current_position::geometry) < 200 THEN 'Loading Zone A'
        WHEN ST_Z(t.current_position::geometry) > 250 THEN 'Dump Zone B'
        ELSE 'Haul Road / Other'
    END AS location_type
FROM "02_raw_telemetry_transformed" t
WHERE t.device_date = ANY(%%(chunk_list)s);

-- Index the small temp table for the next step.
CREATE INDEX ON temp_base (device_id, device_date, timestamp);

-- Step B: Create the final results table FOR THIS CHUNK ONLY.
-- This table is uniquely named for each worker to avoid conflicts.
CREATE TEMP TABLE temp_features_127239448422080 AS
WITH
  basic_windows AS (
    SELECT
      *,
      LAG(is_stationary, 1, is_stationary) OVER w AS prev_stationary,
      COALESCE(EXTRACT(EPOCH FROM (timestamp - LAG(timestamp, 1) OVER w)), 0) AS time_delta,
      AVG(load_weight) OVER (w ROWS BETWEEN 2 PRECEDING AND 2 FOLLOWING) AS smoothed_load_weight
    FROM temp_base
    WINDOW w AS (PARTITION BY device_id, device_date ORDER BY timestamp)
  ),
  duration_prep AS (
    SELECT
      *,
      SUM(CASE WHEN is_stationary != prev_stationary THEN 1 ELSE 0 END) OVER w AS stationary_block_id
    FROM basic_windows
    WINDOW w AS (PARTITION BY device_id, device_date ORDER BY timestamp)
  )
SELECT
  -- Select all final columns and calculate the remaining window functions
  timestamp, ingested_at, raw_event_hash_id, device_id, device_date, system_engaged,
  parking_brake_applied, current_position, current_speed, load_weight, state,
  software_state, prndl, extras, location_type, is_stationary, altitude,
  (altitude - LAG(altitude, 1) OVER w) AS altitude_rate_of_change,
  AVG(current_speed) OVER (w ROWS BETWEEN 2 PRECEDING AND 2 FOLLOWING) AS speed_rolling_avg_5s,
  smoothed_load_weight AS load_weight_smoothed,
  (smoothed_load_weight - LAG(smoothed_load_weight, 1) OVER w) AS load_weight_rate_of_change,
  (STDDEV(load_weight) OVER (PARTITION BY device_id) > 1000) AS has_reliable_payload,
  CASE
    WHEN is_stationary THEN SUM(time_delta) OVER (PARTITION BY device_id, device_date, stationary_block_id ORDER BY timestamp)
    ELSE 0
  END AS time_in_stationary_state
FROM duration_prep
WINDOW w AS (PARTITION BY device_id, device_date ORDER BY timestamp);]
(Background on this error at: https://sqlalche.me/e/20/f405)
2025-09-03 06:17:39,479 - INFO - [Worker-127238003488448] Started, assigned 3 chunks.
2025-09-03 06:17:39,480 - ERROR - ❌ [Worker-127239423243968] Failed during SQL execution: (psycopg2.errors.SyntaxError) syntax error at or near "%"
LINE 17: WHERE t.device_date = ANY(%(chunk_list)s);
                                   ^

[SQL: -- This is a template script executed by each parallel Python worker.
-- It processes an assigned subset of device_date chunks.

-- Step A: Create a temporary table with base features FOR THIS CHUNK ONLY.
-- This uses the two-step materialization pattern correctly to avoid nested window function errors.
CREATE TEMP TABLE temp_base AS
SELECT
    t.*,
    ST_Z(t.current_position::geometry) AS altitude,
    (t.current_speed < 0.5) AS is_stationary,
    CASE
        WHEN ST_Z(t.current_position::geometry) < 200 THEN 'Loading Zone A'
        WHEN ST_Z(t.current_position::geometry) > 250 THEN 'Dump Zone B'
        ELSE 'Haul Road / Other'
    END AS location_type
FROM "02_raw_telemetry_transformed" t
WHERE t.device_date = ANY(%%(chunk_list)s);

-- Index the small temp table for the next step.
CREATE INDEX ON temp_base (device_id, device_date, timestamp);

-- Step B: Create the final results table FOR THIS CHUNK ONLY.
-- This table is uniquely named for each worker to avoid conflicts.
CREATE TEMP TABLE temp_features_127239423243968 AS
WITH
  basic_windows AS (
    SELECT
      *,
      LAG(is_stationary, 1, is_stationary) OVER w AS prev_stationary,
      COALESCE(EXTRACT(EPOCH FROM (timestamp - LAG(timestamp, 1) OVER w)), 0) AS time_delta,
      AVG(load_weight) OVER (w ROWS BETWEEN 2 PRECEDING AND 2 FOLLOWING) AS smoothed_load_weight
    FROM temp_base
    WINDOW w AS (PARTITION BY device_id, device_date ORDER BY timestamp)
  ),
  duration_prep AS (
    SELECT
      *,
      SUM(CASE WHEN is_stationary != prev_stationary THEN 1 ELSE 0 END) OVER w AS stationary_block_id
    FROM basic_windows
    WINDOW w AS (PARTITION BY device_id, device_date ORDER BY timestamp)
  )
SELECT
  -- Select all final columns and calculate the remaining window functions
  timestamp, ingested_at, raw_event_hash_id, device_id, device_date, system_engaged,
  parking_brake_applied, current_position, current_speed, load_weight, state,
  software_state, prndl, extras, location_type, is_stationary, altitude,
  (altitude - LAG(altitude, 1) OVER w) AS altitude_rate_of_change,
  AVG(current_speed) OVER (w ROWS BETWEEN 2 PRECEDING AND 2 FOLLOWING) AS speed_rolling_avg_5s,
  smoothed_load_weight AS load_weight_smoothed,
  (smoothed_load_weight - LAG(smoothed_load_weight, 1) OVER w) AS load_weight_rate_of_change,
  (STDDEV(load_weight) OVER (PARTITION BY device_id) > 1000) AS has_reliable_payload,
  CASE
    WHEN is_stationary THEN SUM(time_delta) OVER (PARTITION BY device_id, device_date, stationary_block_id ORDER BY timestamp)
    ELSE 0
  END AS time_in_stationary_state
FROM duration_prep
WINDOW w AS (PARTITION BY device_id, device_date ORDER BY timestamp);]
(Background on this error at: https://sqlalche.me/e/20/f405)
2025-09-03 06:17:39,480 - INFO - [Worker-127239431636672] Started, assigned 3 chunks.
2025-09-03 06:17:39,481 - ERROR - ❌ [Worker-127239077230272] Failed during SQL execution: (psycopg2.errors.SyntaxError) syntax error at or near "%"
LINE 17: WHERE t.device_date = ANY(%(chunk_list)s);
                                   ^

[SQL: -- This is a template script executed by each parallel Python worker.
-- It processes an assigned subset of device_date chunks.

-- Step A: Create a temporary table with base features FOR THIS CHUNK ONLY.
-- This uses the two-step materialization pattern correctly to avoid nested window function errors.
CREATE TEMP TABLE temp_base AS
SELECT
    t.*,
    ST_Z(t.current_position::geometry) AS altitude,
    (t.current_speed < 0.5) AS is_stationary,
    CASE
        WHEN ST_Z(t.current_position::geometry) < 200 THEN 'Loading Zone A'
        WHEN ST_Z(t.current_position::geometry) > 250 THEN 'Dump Zone B'
        ELSE 'Haul Road / Other'
    END AS location_type
FROM "02_raw_telemetry_transformed" t
WHERE t.device_date = ANY(%%(chunk_list)s);

-- Index the small temp table for the next step.
CREATE INDEX ON temp_base (device_id, device_date, timestamp);

-- Step B: Create the final results table FOR THIS CHUNK ONLY.
-- This table is uniquely named for each worker to avoid conflicts.
CREATE TEMP TABLE temp_features_127239077230272 AS
WITH
  basic_windows AS (
    SELECT
      *,
      LAG(is_stationary, 1, is_stationary) OVER w AS prev_stationary,
      COALESCE(EXTRACT(EPOCH FROM (timestamp - LAG(timestamp, 1) OVER w)), 0) AS time_delta,
      AVG(load_weight) OVER (w ROWS BETWEEN 2 PRECEDING AND 2 FOLLOWING) AS smoothed_load_weight
    FROM temp_base
    WINDOW w AS (PARTITION BY device_id, device_date ORDER BY timestamp)
  ),
  duration_prep AS (
    SELECT
      *,
      SUM(CASE WHEN is_stationary != prev_stationary THEN 1 ELSE 0 END) OVER w AS stationary_block_id
    FROM basic_windows
    WINDOW w AS (PARTITION BY device_id, device_date ORDER BY timestamp)
  )
SELECT
  -- Select all final columns and calculate the remaining window functions
  timestamp, ingested_at, raw_event_hash_id, device_id, device_date, system_engaged,
  parking_brake_applied, current_position, current_speed, load_weight, state,
  software_state, prndl, extras, location_type, is_stationary, altitude,
  (altitude - LAG(altitude, 1) OVER w) AS altitude_rate_of_change,
  AVG(current_speed) OVER (w ROWS BETWEEN 2 PRECEDING AND 2 FOLLOWING) AS speed_rolling_avg_5s,
  smoothed_load_weight AS load_weight_smoothed,
  (smoothed_load_weight - LAG(smoothed_load_weight, 1) OVER w) AS load_weight_rate_of_change,
  (STDDEV(load_weight) OVER (PARTITION BY device_id) > 1000) AS has_reliable_payload,
  CASE
    WHEN is_stationary THEN SUM(time_delta) OVER (PARTITION BY device_id, device_date, stationary_block_id ORDER BY timestamp)
    ELSE 0
  END AS time_in_stationary_state
FROM duration_prep
WINDOW w AS (PARTITION BY device_id, device_date ORDER BY timestamp);]
(Background on this error at: https://sqlalche.me/e/20/f405)
2025-09-03 06:17:39,481 - ERROR - ❌ [Worker-127239052052160] Failed during SQL execution: (psycopg2.errors.SyntaxError) syntax error at or near "%"
LINE 17: WHERE t.device_date = ANY(%(chunk_list)s);
                                   ^

[SQL: -- This is a template script executed by each parallel Python worker.
-- It processes an assigned subset of device_date chunks.

-- Step A: Create a temporary table with base features FOR THIS CHUNK ONLY.
-- This uses the two-step materialization pattern correctly to avoid nested window function errors.
CREATE TEMP TABLE temp_base AS
SELECT
    t.*,
    ST_Z(t.current_position::geometry) AS altitude,
    (t.current_speed < 0.5) AS is_stationary,
    CASE
        WHEN ST_Z(t.current_position::geometry) < 200 THEN 'Loading Zone A'
        WHEN ST_Z(t.current_position::geometry) > 250 THEN 'Dump Zone B'
        ELSE 'Haul Road / Other'
    END AS location_type
FROM "02_raw_telemetry_transformed" t
WHERE t.device_date = ANY(%%(chunk_list)s);

-- Index the small temp table for the next step.
CREATE INDEX ON temp_base (device_id, device_date, timestamp);

-- Step B: Create the final results table FOR THIS CHUNK ONLY.
-- This table is uniquely named for each worker to avoid conflicts.
CREATE TEMP TABLE temp_features_127239052052160 AS
WITH
  basic_windows AS (
    SELECT
      *,
      LAG(is_stationary, 1, is_stationary) OVER w AS prev_stationary,
      COALESCE(EXTRACT(EPOCH FROM (timestamp - LAG(timestamp, 1) OVER w)), 0) AS time_delta,
      AVG(load_weight) OVER (w ROWS BETWEEN 2 PRECEDING AND 2 FOLLOWING) AS smoothed_load_weight
    FROM temp_base
    WINDOW w AS (PARTITION BY device_id, device_date ORDER BY timestamp)
  ),
  duration_prep AS (
    SELECT
      *,
      SUM(CASE WHEN is_stationary != prev_stationary THEN 1 ELSE 0 END) OVER w AS stationary_block_id
    FROM basic_windows
    WINDOW w AS (PARTITION BY device_id, device_date ORDER BY timestamp)
  )
SELECT
  -- Select all final columns and calculate the remaining window functions
  timestamp, ingested_at, raw_event_hash_id, device_id, device_date, system_engaged,
  parking_brake_applied, current_position, current_speed, load_weight, state,
  software_state, prndl, extras, location_type, is_stationary, altitude,
  (altitude - LAG(altitude, 1) OVER w) AS altitude_rate_of_change,
  AVG(current_speed) OVER (w ROWS BETWEEN 2 PRECEDING AND 2 FOLLOWING) AS speed_rolling_avg_5s,
  smoothed_load_weight AS load_weight_smoothed,
  (smoothed_load_weight - LAG(smoothed_load_weight, 1) OVER w) AS load_weight_rate_of_change,
  (STDDEV(load_weight) OVER (PARTITION BY device_id) > 1000) AS has_reliable_payload,
  CASE
    WHEN is_stationary THEN SUM(time_delta) OVER (PARTITION BY device_id, device_date, stationary_block_id ORDER BY timestamp)
    ELSE 0
  END AS time_in_stationary_state
FROM duration_prep
WINDOW w AS (PARTITION BY device_id, device_date ORDER BY timestamp);]
(Background on this error at: https://sqlalche.me/e/20/f405)
2025-09-03 06:17:39,482 - ERROR - ❌ [Worker-127239440029376] Failed during SQL execution: (psycopg2.errors.SyntaxError) syntax error at or near "%"
LINE 17: WHERE t.device_date = ANY(%(chunk_list)s);
                                   ^

[SQL: -- This is a template script executed by each parallel Python worker.
-- It processes an assigned subset of device_date chunks.

-- Step A: Create a temporary table with base features FOR THIS CHUNK ONLY.
-- This uses the two-step materialization pattern correctly to avoid nested window function errors.
CREATE TEMP TABLE temp_base AS
SELECT
    t.*,
    ST_Z(t.current_position::geometry) AS altitude,
    (t.current_speed < 0.5) AS is_stationary,
    CASE
        WHEN ST_Z(t.current_position::geometry) < 200 THEN 'Loading Zone A'
        WHEN ST_Z(t.current_position::geometry) > 250 THEN 'Dump Zone B'
        ELSE 'Haul Road / Other'
    END AS location_type
FROM "02_raw_telemetry_transformed" t
WHERE t.device_date = ANY(%%(chunk_list)s);

-- Index the small temp table for the next step.
CREATE INDEX ON temp_base (device_id, device_date, timestamp);

-- Step B: Create the final results table FOR THIS CHUNK ONLY.
-- This table is uniquely named for each worker to avoid conflicts.
CREATE TEMP TABLE temp_features_127239440029376 AS
WITH
  basic_windows AS (
    SELECT
      *,
      LAG(is_stationary, 1, is_stationary) OVER w AS prev_stationary,
      COALESCE(EXTRACT(EPOCH FROM (timestamp - LAG(timestamp, 1) OVER w)), 0) AS time_delta,
      AVG(load_weight) OVER (w ROWS BETWEEN 2 PRECEDING AND 2 FOLLOWING) AS smoothed_load_weight
    FROM temp_base
    WINDOW w AS (PARTITION BY device_id, device_date ORDER BY timestamp)
  ),
  duration_prep AS (
    SELECT
      *,
      SUM(CASE WHEN is_stationary != prev_stationary THEN 1 ELSE 0 END) OVER w AS stationary_block_id
    FROM basic_windows
    WINDOW w AS (PARTITION BY device_id, device_date ORDER BY timestamp)
  )
SELECT
  -- Select all final columns and calculate the remaining window functions
  timestamp, ingested_at, raw_event_hash_id, device_id, device_date, system_engaged,
  parking_brake_applied, current_position, current_speed, load_weight, state,
  software_state, prndl, extras, location_type, is_stationary, altitude,
  (altitude - LAG(altitude, 1) OVER w) AS altitude_rate_of_change,
  AVG(current_speed) OVER (w ROWS BETWEEN 2 PRECEDING AND 2 FOLLOWING) AS speed_rolling_avg_5s,
  smoothed_load_weight AS load_weight_smoothed,
  (smoothed_load_weight - LAG(smoothed_load_weight, 1) OVER w) AS load_weight_rate_of_change,
  (STDDEV(load_weight) OVER (PARTITION BY device_id) > 1000) AS has_reliable_payload,
  CASE
    WHEN is_stationary THEN SUM(time_delta) OVER (PARTITION BY device_id, device_date, stationary_block_id ORDER BY timestamp)
    ELSE 0
  END AS time_in_stationary_state
FROM duration_prep
WINDOW w AS (PARTITION BY device_id, device_date ORDER BY timestamp);]
(Background on this error at: https://sqlalche.me/e/20/f405)
2025-09-03 06:17:39,482 - ERROR - ❌ [Worker-127239456814784] Failed during SQL execution: (psycopg2.errors.SyntaxError) syntax error at or near "%"
LINE 17: WHERE t.device_date = ANY(%(chunk_list)s);
                                   ^

[SQL: -- This is a template script executed by each parallel Python worker.
-- It processes an assigned subset of device_date chunks.

-- Step A: Create a temporary table with base features FOR THIS CHUNK ONLY.
-- This uses the two-step materialization pattern correctly to avoid nested window function errors.
CREATE TEMP TABLE temp_base AS
SELECT
    t.*,
    ST_Z(t.current_position::geometry) AS altitude,
    (t.current_speed < 0.5) AS is_stationary,
    CASE
        WHEN ST_Z(t.current_position::geometry) < 200 THEN 'Loading Zone A'
        WHEN ST_Z(t.current_position::geometry) > 250 THEN 'Dump Zone B'
        ELSE 'Haul Road / Other'
    END AS location_type
FROM "02_raw_telemetry_transformed" t
WHERE t.device_date = ANY(%%(chunk_list)s);

-- Index the small temp table for the next step.
CREATE INDEX ON temp_base (device_id, device_date, timestamp);

-- Step B: Create the final results table FOR THIS CHUNK ONLY.
-- This table is uniquely named for each worker to avoid conflicts.
CREATE TEMP TABLE temp_features_127239456814784 AS
WITH
  basic_windows AS (
    SELECT
      *,
      LAG(is_stationary, 1, is_stationary) OVER w AS prev_stationary,
      COALESCE(EXTRACT(EPOCH FROM (timestamp - LAG(timestamp, 1) OVER w)), 0) AS time_delta,
      AVG(load_weight) OVER (w ROWS BETWEEN 2 PRECEDING AND 2 FOLLOWING) AS smoothed_load_weight
    FROM temp_base
    WINDOW w AS (PARTITION BY device_id, device_date ORDER BY timestamp)
  ),
  duration_prep AS (
    SELECT
      *,
      SUM(CASE WHEN is_stationary != prev_stationary THEN 1 ELSE 0 END) OVER w AS stationary_block_id
    FROM basic_windows
    WINDOW w AS (PARTITION BY device_id, device_date ORDER BY timestamp)
  )
SELECT
  -- Select all final columns and calculate the remaining window functions
  timestamp, ingested_at, raw_event_hash_id, device_id, device_date, system_engaged,
  parking_brake_applied, current_position, current_speed, load_weight, state,
  software_state, prndl, extras, location_type, is_stationary, altitude,
  (altitude - LAG(altitude, 1) OVER w) AS altitude_rate_of_change,
  AVG(current_speed) OVER (w ROWS BETWEEN 2 PRECEDING AND 2 FOLLOWING) AS speed_rolling_avg_5s,
  smoothed_load_weight AS load_weight_smoothed,
  (smoothed_load_weight - LAG(smoothed_load_weight, 1) OVER w) AS load_weight_rate_of_change,
  (STDDEV(load_weight) OVER (PARTITION BY device_id) > 1000) AS has_reliable_payload,
  CASE
    WHEN is_stationary THEN SUM(time_delta) OVER (PARTITION BY device_id, device_date, stationary_block_id ORDER BY timestamp)
    ELSE 0
  END AS time_in_stationary_state
FROM duration_prep
WINDOW w AS (PARTITION BY device_id, device_date ORDER BY timestamp);]
(Background on this error at: https://sqlalche.me/e/20/f405)
2025-09-03 06:17:39,486 - ERROR - ❌ [Worker-127239068837568] Failed during SQL execution: (psycopg2.errors.SyntaxError) syntax error at or near "%"
LINE 17: WHERE t.device_date = ANY(%(chunk_list)s);
                                   ^

[SQL: -- This is a template script executed by each parallel Python worker.
-- It processes an assigned subset of device_date chunks.

-- Step A: Create a temporary table with base features FOR THIS CHUNK ONLY.
-- This uses the two-step materialization pattern correctly to avoid nested window function errors.
CREATE TEMP TABLE temp_base AS
SELECT
    t.*,
    ST_Z(t.current_position::geometry) AS altitude,
    (t.current_speed < 0.5) AS is_stationary,
    CASE
        WHEN ST_Z(t.current_position::geometry) < 200 THEN 'Loading Zone A'
        WHEN ST_Z(t.current_position::geometry) > 250 THEN 'Dump Zone B'
        ELSE 'Haul Road / Other'
    END AS location_type
FROM "02_raw_telemetry_transformed" t
WHERE t.device_date = ANY(%%(chunk_list)s);

-- Index the small temp table for the next step.
CREATE INDEX ON temp_base (device_id, device_date, timestamp);

-- Step B: Create the final results table FOR THIS CHUNK ONLY.
-- This table is uniquely named for each worker to avoid conflicts.
CREATE TEMP TABLE temp_features_127239068837568 AS
WITH
  basic_windows AS (
    SELECT
      *,
      LAG(is_stationary, 1, is_stationary) OVER w AS prev_stationary,
      COALESCE(EXTRACT(EPOCH FROM (timestamp - LAG(timestamp, 1) OVER w)), 0) AS time_delta,
      AVG(load_weight) OVER (w ROWS BETWEEN 2 PRECEDING AND 2 FOLLOWING) AS smoothed_load_weight
    FROM temp_base
    WINDOW w AS (PARTITION BY device_id, device_date ORDER BY timestamp)
  ),
  duration_prep AS (
    SELECT
      *,
      SUM(CASE WHEN is_stationary != prev_stationary THEN 1 ELSE 0 END) OVER w AS stationary_block_id
    FROM basic_windows
    WINDOW w AS (PARTITION BY device_id, device_date ORDER BY timestamp)
  )
SELECT
  -- Select all final columns and calculate the remaining window functions
  timestamp, ingested_at, raw_event_hash_id, device_id, device_date, system_engaged,
  parking_brake_applied, current_position, current_speed, load_weight, state,
  software_state, prndl, extras, location_type, is_stationary, altitude,
  (altitude - LAG(altitude, 1) OVER w) AS altitude_rate_of_change,
  AVG(current_speed) OVER (w ROWS BETWEEN 2 PRECEDING AND 2 FOLLOWING) AS speed_rolling_avg_5s,
  smoothed_load_weight AS load_weight_smoothed,
  (smoothed_load_weight - LAG(smoothed_load_weight, 1) OVER w) AS load_weight_rate_of_change,
  (STDDEV(load_weight) OVER (PARTITION BY device_id) > 1000) AS has_reliable_payload,
  CASE
    WHEN is_stationary THEN SUM(time_delta) OVER (PARTITION BY device_id, device_date, stationary_block_id ORDER BY timestamp)
    ELSE 0
  END AS time_in_stationary_state
FROM duration_prep
WINDOW w AS (PARTITION BY device_id, device_date ORDER BY timestamp);]
(Background on this error at: https://sqlalche.me/e/20/f405)
2025-09-03 06:17:39,489 - ERROR - ❌ [Worker-127239060444864] Failed during SQL execution: (psycopg2.errors.SyntaxError) syntax error at or near "%"
LINE 17: WHERE t.device_date = ANY(%(chunk_list)s);
                                   ^

[SQL: -- This is a template script executed by each parallel Python worker.
-- It processes an assigned subset of device_date chunks.

-- Step A: Create a temporary table with base features FOR THIS CHUNK ONLY.
-- This uses the two-step materialization pattern correctly to avoid nested window function errors.
CREATE TEMP TABLE temp_base AS
SELECT
    t.*,
    ST_Z(t.current_position::geometry) AS altitude,
    (t.current_speed < 0.5) AS is_stationary,
    CASE
        WHEN ST_Z(t.current_position::geometry) < 200 THEN 'Loading Zone A'
        WHEN ST_Z(t.current_position::geometry) > 250 THEN 'Dump Zone B'
        ELSE 'Haul Road / Other'
    END AS location_type
FROM "02_raw_telemetry_transformed" t
WHERE t.device_date = ANY(%%(chunk_list)s);

-- Index the small temp table for the next step.
CREATE INDEX ON temp_base (device_id, device_date, timestamp);

-- Step B: Create the final results table FOR THIS CHUNK ONLY.
-- This table is uniquely named for each worker to avoid conflicts.
CREATE TEMP TABLE temp_features_127239060444864 AS
WITH
  basic_windows AS (
    SELECT
      *,
      LAG(is_stationary, 1, is_stationary) OVER w AS prev_stationary,
      COALESCE(EXTRACT(EPOCH FROM (timestamp - LAG(timestamp, 1) OVER w)), 0) AS time_delta,
      AVG(load_weight) OVER (w ROWS BETWEEN 2 PRECEDING AND 2 FOLLOWING) AS smoothed_load_weight
    FROM temp_base
    WINDOW w AS (PARTITION BY device_id, device_date ORDER BY timestamp)
  ),
  duration_prep AS (
    SELECT
      *,
      SUM(CASE WHEN is_stationary != prev_stationary THEN 1 ELSE 0 END) OVER w AS stationary_block_id
    FROM basic_windows
    WINDOW w AS (PARTITION BY device_id, device_date ORDER BY timestamp)
  )
SELECT
  -- Select all final columns and calculate the remaining window functions
  timestamp, ingested_at, raw_event_hash_id, device_id, device_date, system_engaged,
  parking_brake_applied, current_position, current_speed, load_weight, state,
  software_state, prndl, extras, location_type, is_stationary, altitude,
  (altitude - LAG(altitude, 1) OVER w) AS altitude_rate_of_change,
  AVG(current_speed) OVER (w ROWS BETWEEN 2 PRECEDING AND 2 FOLLOWING) AS speed_rolling_avg_5s,
  smoothed_load_weight AS load_weight_smoothed,
  (smoothed_load_weight - LAG(smoothed_load_weight, 1) OVER w) AS load_weight_rate_of_change,
  (STDDEV(load_weight) OVER (PARTITION BY device_id) > 1000) AS has_reliable_payload,
  CASE
    WHEN is_stationary THEN SUM(time_delta) OVER (PARTITION BY device_id, device_date, stationary_block_id ORDER BY timestamp)
    ELSE 0
  END AS time_in_stationary_state
FROM duration_prep
WINDOW w AS (PARTITION BY device_id, device_date ORDER BY timestamp);]
(Background on this error at: https://sqlalche.me/e/20/f405)
2025-09-03 06:17:39,489 - ERROR - ❌ [Worker-127239043659456] Failed during SQL execution: (psycopg2.errors.SyntaxError) syntax error at or near "%"
LINE 17: WHERE t.device_date = ANY(%(chunk_list)s);
                                   ^

[SQL: -- This is a template script executed by each parallel Python worker.
-- It processes an assigned subset of device_date chunks.

-- Step A: Create a temporary table with base features FOR THIS CHUNK ONLY.
-- This uses the two-step materialization pattern correctly to avoid nested window function errors.
CREATE TEMP TABLE temp_base AS
SELECT
    t.*,
    ST_Z(t.current_position::geometry) AS altitude,
    (t.current_speed < 0.5) AS is_stationary,
    CASE
        WHEN ST_Z(t.current_position::geometry) < 200 THEN 'Loading Zone A'
        WHEN ST_Z(t.current_position::geometry) > 250 THEN 'Dump Zone B'
        ELSE 'Haul Road / Other'
    END AS location_type
FROM "02_raw_telemetry_transformed" t
WHERE t.device_date = ANY(%%(chunk_list)s);

-- Index the small temp table for the next step.
CREATE INDEX ON temp_base (device_id, device_date, timestamp);

-- Step B: Create the final results table FOR THIS CHUNK ONLY.
-- This table is uniquely named for each worker to avoid conflicts.
CREATE TEMP TABLE temp_features_127239043659456 AS
WITH
  basic_windows AS (
    SELECT
      *,
      LAG(is_stationary, 1, is_stationary) OVER w AS prev_stationary,
      COALESCE(EXTRACT(EPOCH FROM (timestamp - LAG(timestamp, 1) OVER w)), 0) AS time_delta,
      AVG(load_weight) OVER (w ROWS BETWEEN 2 PRECEDING AND 2 FOLLOWING) AS smoothed_load_weight
    FROM temp_base
    WINDOW w AS (PARTITION BY device_id, device_date ORDER BY timestamp)
  ),
  duration_prep AS (
    SELECT
      *,
      SUM(CASE WHEN is_stationary != prev_stationary THEN 1 ELSE 0 END) OVER w AS stationary_block_id
    FROM basic_windows
    WINDOW w AS (PARTITION BY device_id, device_date ORDER BY timestamp)
  )
SELECT
  -- Select all final columns and calculate the remaining window functions
  timestamp, ingested_at, raw_event_hash_id, device_id, device_date, system_engaged,
  parking_brake_applied, current_position, current_speed, load_weight, state,
  software_state, prndl, extras, location_type, is_stationary, altitude,
  (altitude - LAG(altitude, 1) OVER w) AS altitude_rate_of_change,
  AVG(current_speed) OVER (w ROWS BETWEEN 2 PRECEDING AND 2 FOLLOWING) AS speed_rolling_avg_5s,
  smoothed_load_weight AS load_weight_smoothed,
  (smoothed_load_weight - LAG(smoothed_load_weight, 1) OVER w) AS load_weight_rate_of_change,
  (STDDEV(load_weight) OVER (PARTITION BY device_id) > 1000) AS has_reliable_payload,
  CASE
    WHEN is_stationary THEN SUM(time_delta) OVER (PARTITION BY device_id, device_date, stationary_block_id ORDER BY timestamp)
    ELSE 0
  END AS time_in_stationary_state
FROM duration_prep
WINDOW w AS (PARTITION BY device_id, device_date ORDER BY timestamp);]
(Background on this error at: https://sqlalche.me/e/20/f405)
2025-09-03 06:17:39,490 - INFO - [Worker-127239052052160] Started, assigned 3 chunks.
2025-09-03 06:17:39,491 - ERROR - ❌ [Worker-127239026874048] Failed during SQL execution: (psycopg2.errors.SyntaxError) syntax error at or near "%"
LINE 17: WHERE t.device_date = ANY(%(chunk_list)s);
                                   ^

[SQL: -- This is a template script executed by each parallel Python worker.
-- It processes an assigned subset of device_date chunks.

-- Step A: Create a temporary table with base features FOR THIS CHUNK ONLY.
-- This uses the two-step materialization pattern correctly to avoid nested window function errors.
CREATE TEMP TABLE temp_base AS
SELECT
    t.*,
    ST_Z(t.current_position::geometry) AS altitude,
    (t.current_speed < 0.5) AS is_stationary,
    CASE
        WHEN ST_Z(t.current_position::geometry) < 200 THEN 'Loading Zone A'
        WHEN ST_Z(t.current_position::geometry) > 250 THEN 'Dump Zone B'
        ELSE 'Haul Road / Other'
    END AS location_type
FROM "02_raw_telemetry_transformed" t
WHERE t.device_date = ANY(%%(chunk_list)s);

-- Index the small temp table for the next step.
CREATE INDEX ON temp_base (device_id, device_date, timestamp);

-- Step B: Create the final results table FOR THIS CHUNK ONLY.
-- This table is uniquely named for each worker to avoid conflicts.
CREATE TEMP TABLE temp_features_127239026874048 AS
WITH
  basic_windows AS (
    SELECT
      *,
      LAG(is_stationary, 1, is_stationary) OVER w AS prev_stationary,
      COALESCE(EXTRACT(EPOCH FROM (timestamp - LAG(timestamp, 1) OVER w)), 0) AS time_delta,
      AVG(load_weight) OVER (w ROWS BETWEEN 2 PRECEDING AND 2 FOLLOWING) AS smoothed_load_weight
    FROM temp_base
    WINDOW w AS (PARTITION BY device_id, device_date ORDER BY timestamp)
  ),
  duration_prep AS (
    SELECT
      *,
      SUM(CASE WHEN is_stationary != prev_stationary THEN 1 ELSE 0 END) OVER w AS stationary_block_id
    FROM basic_windows
    WINDOW w AS (PARTITION BY device_id, device_date ORDER BY timestamp)
  )
SELECT
  -- Select all final columns and calculate the remaining window functions
  timestamp, ingested_at, raw_event_hash_id, device_id, device_date, system_engaged,
  parking_brake_applied, current_position, current_speed, load_weight, state,
  software_state, prndl, extras, location_type, is_stationary, altitude,
  (altitude - LAG(altitude, 1) OVER w) AS altitude_rate_of_change,
  AVG(current_speed) OVER (w ROWS BETWEEN 2 PRECEDING AND 2 FOLLOWING) AS speed_rolling_avg_5s,
  smoothed_load_weight AS load_weight_smoothed,
  (smoothed_load_weight - LAG(smoothed_load_weight, 1) OVER w) AS load_weight_rate_of_change,
  (STDDEV(load_weight) OVER (PARTITION BY device_id) > 1000) AS has_reliable_payload,
  CASE
    WHEN is_stationary THEN SUM(time_delta) OVER (PARTITION BY device_id, device_date, stationary_block_id ORDER BY timestamp)
    ELSE 0
  END AS time_in_stationary_state
FROM duration_prep
WINDOW w AS (PARTITION BY device_id, device_date ORDER BY timestamp);]
(Background on this error at: https://sqlalche.me/e/20/f405)
2025-09-03 06:17:39,492 - ERROR - ❌ [Worker-127239035266752] Failed during SQL execution: (psycopg2.errors.SyntaxError) syntax error at or near "%"
LINE 17: WHERE t.device_date = ANY(%(chunk_list)s);
                                   ^

[SQL: -- This is a template script executed by each parallel Python worker.
-- It processes an assigned subset of device_date chunks.

-- Step A: Create a temporary table with base features FOR THIS CHUNK ONLY.
-- This uses the two-step materialization pattern correctly to avoid nested window function errors.
CREATE TEMP TABLE temp_base AS
SELECT
    t.*,
    ST_Z(t.current_position::geometry) AS altitude,
    (t.current_speed < 0.5) AS is_stationary,
    CASE
        WHEN ST_Z(t.current_position::geometry) < 200 THEN 'Loading Zone A'
        WHEN ST_Z(t.current_position::geometry) > 250 THEN 'Dump Zone B'
        ELSE 'Haul Road / Other'
    END AS location_type
FROM "02_raw_telemetry_transformed" t
WHERE t.device_date = ANY(%%(chunk_list)s);

-- Index the small temp table for the next step.
CREATE INDEX ON temp_base (device_id, device_date, timestamp);

-- Step B: Create the final results table FOR THIS CHUNK ONLY.
-- This table is uniquely named for each worker to avoid conflicts.
CREATE TEMP TABLE temp_features_127239035266752 AS
WITH
  basic_windows AS (
    SELECT
      *,
      LAG(is_stationary, 1, is_stationary) OVER w AS prev_stationary,
      COALESCE(EXTRACT(EPOCH FROM (timestamp - LAG(timestamp, 1) OVER w)), 0) AS time_delta,
      AVG(load_weight) OVER (w ROWS BETWEEN 2 PRECEDING AND 2 FOLLOWING) AS smoothed_load_weight
    FROM temp_base
    WINDOW w AS (PARTITION BY device_id, device_date ORDER BY timestamp)
  ),
  duration_prep AS (
    SELECT
      *,
      SUM(CASE WHEN is_stationary != prev_stationary THEN 1 ELSE 0 END) OVER w AS stationary_block_id
    FROM basic_windows
    WINDOW w AS (PARTITION BY device_id, device_date ORDER BY timestamp)
  )
SELECT
  -- Select all final columns and calculate the remaining window functions
  timestamp, ingested_at, raw_event_hash_id, device_id, device_date, system_engaged,
  parking_brake_applied, current_position, current_speed, load_weight, state,
  software_state, prndl, extras, location_type, is_stationary, altitude,
  (altitude - LAG(altitude, 1) OVER w) AS altitude_rate_of_change,
  AVG(current_speed) OVER (w ROWS BETWEEN 2 PRECEDING AND 2 FOLLOWING) AS speed_rolling_avg_5s,
  smoothed_load_weight AS load_weight_smoothed,
  (smoothed_load_weight - LAG(smoothed_load_weight, 1) OVER w) AS load_weight_rate_of_change,
  (STDDEV(load_weight) OVER (PARTITION BY device_id) > 1000) AS has_reliable_payload,
  CASE
    WHEN is_stationary THEN SUM(time_delta) OVER (PARTITION BY device_id, device_date, stationary_block_id ORDER BY timestamp)
    ELSE 0
  END AS time_in_stationary_state
FROM duration_prep
WINDOW w AS (PARTITION BY device_id, device_date ORDER BY timestamp);]
(Background on this error at: https://sqlalche.me/e/20/f405)
2025-09-03 06:17:39,492 - INFO - [Worker-127239448422080] Started, assigned 3 chunks.
2025-09-03 06:17:39,492 - INFO - [Worker-127237995095744] Started, assigned 3 chunks.
2025-09-03 06:17:39,492 - INFO - [Worker-127237986703040] Started, assigned 3 chunks.
2025-09-03 06:17:39,492 - ERROR - ❌ [Worker-127238523573952] Failed during SQL execution: (psycopg2.errors.SyntaxError) syntax error at or near "%"
LINE 17: WHERE t.device_date = ANY(%(chunk_list)s);
                                   ^

[SQL: -- This is a template script executed by each parallel Python worker.
-- It processes an assigned subset of device_date chunks.

-- Step A: Create a temporary table with base features FOR THIS CHUNK ONLY.
-- This uses the two-step materialization pattern correctly to avoid nested window function errors.
CREATE TEMP TABLE temp_base AS
SELECT
    t.*,
    ST_Z(t.current_position::geometry) AS altitude,
    (t.current_speed < 0.5) AS is_stationary,
    CASE
        WHEN ST_Z(t.current_position::geometry) < 200 THEN 'Loading Zone A'
        WHEN ST_Z(t.current_position::geometry) > 250 THEN 'Dump Zone B'
        ELSE 'Haul Road / Other'
    END AS location_type
FROM "02_raw_telemetry_transformed" t
WHERE t.device_date = ANY(%%(chunk_list)s);

-- Index the small temp table for the next step.
CREATE INDEX ON temp_base (device_id, device_date, timestamp);

-- Step B: Create the final results table FOR THIS CHUNK ONLY.
-- This table is uniquely named for each worker to avoid conflicts.
CREATE TEMP TABLE temp_features_127238523573952 AS
WITH
  basic_windows AS (
    SELECT
      *,
      LAG(is_stationary, 1, is_stationary) OVER w AS prev_stationary,
      COALESCE(EXTRACT(EPOCH FROM (timestamp - LAG(timestamp, 1) OVER w)), 0) AS time_delta,
      AVG(load_weight) OVER (w ROWS BETWEEN 2 PRECEDING AND 2 FOLLOWING) AS smoothed_load_weight
    FROM temp_base
    WINDOW w AS (PARTITION BY device_id, device_date ORDER BY timestamp)
  ),
  duration_prep AS (
    SELECT
      *,
      SUM(CASE WHEN is_stationary != prev_stationary THEN 1 ELSE 0 END) OVER w AS stationary_block_id
    FROM basic_windows
    WINDOW w AS (PARTITION BY device_id, device_date ORDER BY timestamp)
  )
SELECT
  -- Select all final columns and calculate the remaining window functions
  timestamp, ingested_at, raw_event_hash_id, device_id, device_date, system_engaged,
  parking_brake_applied, current_position, current_speed, load_weight, state,
  software_state, prndl, extras, location_type, is_stationary, altitude,
  (altitude - LAG(altitude, 1) OVER w) AS altitude_rate_of_change,
  AVG(current_speed) OVER (w ROWS BETWEEN 2 PRECEDING AND 2 FOLLOWING) AS speed_rolling_avg_5s,
  smoothed_load_weight AS load_weight_smoothed,
  (smoothed_load_weight - LAG(smoothed_load_weight, 1) OVER w) AS load_weight_rate_of_change,
  (STDDEV(load_weight) OVER (PARTITION BY device_id) > 1000) AS has_reliable_payload,
  CASE
    WHEN is_stationary THEN SUM(time_delta) OVER (PARTITION BY device_id, device_date, stationary_block_id ORDER BY timestamp)
    ELSE 0
  END AS time_in_stationary_state
FROM duration_prep
WINDOW w AS (PARTITION BY device_id, device_date ORDER BY timestamp);]
(Background on this error at: https://sqlalche.me/e/20/f405)
2025-09-03 06:17:39,494 - ERROR - ❌ [Worker-127238515181248] Failed during SQL execution: (psycopg2.errors.SyntaxError) syntax error at or near "%"
LINE 17: WHERE t.device_date = ANY(%(chunk_list)s);
                                   ^

[SQL: -- This is a template script executed by each parallel Python worker.
-- It processes an assigned subset of device_date chunks.

-- Step A: Create a temporary table with base features FOR THIS CHUNK ONLY.
-- This uses the two-step materialization pattern correctly to avoid nested window function errors.
CREATE TEMP TABLE temp_base AS
SELECT
    t.*,
    ST_Z(t.current_position::geometry) AS altitude,
    (t.current_speed < 0.5) AS is_stationary,
    CASE
        WHEN ST_Z(t.current_position::geometry) < 200 THEN 'Loading Zone A'
        WHEN ST_Z(t.current_position::geometry) > 250 THEN 'Dump Zone B'
        ELSE 'Haul Road / Other'
    END AS location_type
FROM "02_raw_telemetry_transformed" t
WHERE t.device_date = ANY(%%(chunk_list)s);

-- Index the small temp table for the next step.
CREATE INDEX ON temp_base (device_id, device_date, timestamp);

-- Step B: Create the final results table FOR THIS CHUNK ONLY.
-- This table is uniquely named for each worker to avoid conflicts.
CREATE TEMP TABLE temp_features_127238515181248 AS
WITH
  basic_windows AS (
    SELECT
      *,
      LAG(is_stationary, 1, is_stationary) OVER w AS prev_stationary,
      COALESCE(EXTRACT(EPOCH FROM (timestamp - LAG(timestamp, 1) OVER w)), 0) AS time_delta,
      AVG(load_weight) OVER (w ROWS BETWEEN 2 PRECEDING AND 2 FOLLOWING) AS smoothed_load_weight
    FROM temp_base
    WINDOW w AS (PARTITION BY device_id, device_date ORDER BY timestamp)
  ),
  duration_prep AS (
    SELECT
      *,
      SUM(CASE WHEN is_stationary != prev_stationary THEN 1 ELSE 0 END) OVER w AS stationary_block_id
    FROM basic_windows
    WINDOW w AS (PARTITION BY device_id, device_date ORDER BY timestamp)
  )
SELECT
  -- Select all final columns and calculate the remaining window functions
  timestamp, ingested_at, raw_event_hash_id, device_id, device_date, system_engaged,
  parking_brake_applied, current_position, current_speed, load_weight, state,
  software_state, prndl, extras, location_type, is_stationary, altitude,
  (altitude - LAG(altitude, 1) OVER w) AS altitude_rate_of_change,
  AVG(current_speed) OVER (w ROWS BETWEEN 2 PRECEDING AND 2 FOLLOWING) AS speed_rolling_avg_5s,
  smoothed_load_weight AS load_weight_smoothed,
  (smoothed_load_weight - LAG(smoothed_load_weight, 1) OVER w) AS load_weight_rate_of_change,
  (STDDEV(load_weight) OVER (PARTITION BY device_id) > 1000) AS has_reliable_payload,
  CASE
    WHEN is_stationary THEN SUM(time_delta) OVER (PARTITION BY device_id, device_date, stationary_block_id ORDER BY timestamp)
    ELSE 0
  END AS time_in_stationary_state
FROM duration_prep
WINDOW w AS (PARTITION BY device_id, device_date ORDER BY timestamp);]
(Background on this error at: https://sqlalche.me/e/20/f405)
2025-09-03 06:17:39,494 - INFO - [Worker-127237978310336] Started, assigned 3 chunks.
2025-09-03 06:17:39,494 - ERROR - ❌ [Worker-127238531966656] Failed during SQL execution: (psycopg2.errors.SyntaxError) syntax error at or near "%"
LINE 17: WHERE t.device_date = ANY(%(chunk_list)s);
                                   ^

[SQL: -- This is a template script executed by each parallel Python worker.
-- It processes an assigned subset of device_date chunks.

-- Step A: Create a temporary table with base features FOR THIS CHUNK ONLY.
-- This uses the two-step materialization pattern correctly to avoid nested window function errors.
CREATE TEMP TABLE temp_base AS
SELECT
    t.*,
    ST_Z(t.current_position::geometry) AS altitude,
    (t.current_speed < 0.5) AS is_stationary,
    CASE
        WHEN ST_Z(t.current_position::geometry) < 200 THEN 'Loading Zone A'
        WHEN ST_Z(t.current_position::geometry) > 250 THEN 'Dump Zone B'
        ELSE 'Haul Road / Other'
    END AS location_type
FROM "02_raw_telemetry_transformed" t
WHERE t.device_date = ANY(%%(chunk_list)s);

-- Index the small temp table for the next step.
CREATE INDEX ON temp_base (device_id, device_date, timestamp);

-- Step B: Create the final results table FOR THIS CHUNK ONLY.
-- This table is uniquely named for each worker to avoid conflicts.
CREATE TEMP TABLE temp_features_127238531966656 AS
WITH
  basic_windows AS (
    SELECT
      *,
      LAG(is_stationary, 1, is_stationary) OVER w AS prev_stationary,
      COALESCE(EXTRACT(EPOCH FROM (timestamp - LAG(timestamp, 1) OVER w)), 0) AS time_delta,
      AVG(load_weight) OVER (w ROWS BETWEEN 2 PRECEDING AND 2 FOLLOWING) AS smoothed_load_weight
    FROM temp_base
    WINDOW w AS (PARTITION BY device_id, device_date ORDER BY timestamp)
  ),
  duration_prep AS (
    SELECT
      *,
      SUM(CASE WHEN is_stationary != prev_stationary THEN 1 ELSE 0 END) OVER w AS stationary_block_id
    FROM basic_windows
    WINDOW w AS (PARTITION BY device_id, device_date ORDER BY timestamp)
  )
SELECT
  -- Select all final columns and calculate the remaining window functions
  timestamp, ingested_at, raw_event_hash_id, device_id, device_date, system_engaged,
  parking_brake_applied, current_position, current_speed, load_weight, state,
  software_state, prndl, extras, location_type, is_stationary, altitude,
  (altitude - LAG(altitude, 1) OVER w) AS altitude_rate_of_change,
  AVG(current_speed) OVER (w ROWS BETWEEN 2 PRECEDING AND 2 FOLLOWING) AS speed_rolling_avg_5s,
  smoothed_load_weight AS load_weight_smoothed,
  (smoothed_load_weight - LAG(smoothed_load_weight, 1) OVER w) AS load_weight_rate_of_change,
  (STDDEV(load_weight) OVER (PARTITION BY device_id) > 1000) AS has_reliable_payload,
  CASE
    WHEN is_stationary THEN SUM(time_delta) OVER (PARTITION BY device_id, device_date, stationary_block_id ORDER BY timestamp)
    ELSE 0
  END AS time_in_stationary_state
FROM duration_prep
WINDOW w AS (PARTITION BY device_id, device_date ORDER BY timestamp);]
(Background on this error at: https://sqlalche.me/e/20/f405)
2025-09-03 06:17:39,494 - INFO - [Worker-127239077230272] Started, assigned 3 chunks.
2025-09-03 06:17:39,495 - INFO - [Worker-127237969917632] Started, assigned 3 chunks.
2025-09-03 06:17:39,495 - ERROR - ❌ [Worker-127238540359360] Failed during SQL execution: (psycopg2.errors.SyntaxError) syntax error at or near "%"
LINE 17: WHERE t.device_date = ANY(%(chunk_list)s);
                                   ^

[SQL: -- This is a template script executed by each parallel Python worker.
-- It processes an assigned subset of device_date chunks.

-- Step A: Create a temporary table with base features FOR THIS CHUNK ONLY.
-- This uses the two-step materialization pattern correctly to avoid nested window function errors.
CREATE TEMP TABLE temp_base AS
SELECT
    t.*,
    ST_Z(t.current_position::geometry) AS altitude,
    (t.current_speed < 0.5) AS is_stationary,
    CASE
        WHEN ST_Z(t.current_position::geometry) < 200 THEN 'Loading Zone A'
        WHEN ST_Z(t.current_position::geometry) > 250 THEN 'Dump Zone B'
        ELSE 'Haul Road / Other'
    END AS location_type
FROM "02_raw_telemetry_transformed" t
WHERE t.device_date = ANY(%%(chunk_list)s);

-- Index the small temp table for the next step.
CREATE INDEX ON temp_base (device_id, device_date, timestamp);

-- Step B: Create the final results table FOR THIS CHUNK ONLY.
-- This table is uniquely named for each worker to avoid conflicts.
CREATE TEMP TABLE temp_features_127238540359360 AS
WITH
  basic_windows AS (
    SELECT
      *,
      LAG(is_stationary, 1, is_stationary) OVER w AS prev_stationary,
      COALESCE(EXTRACT(EPOCH FROM (timestamp - LAG(timestamp, 1) OVER w)), 0) AS time_delta,
      AVG(load_weight) OVER (w ROWS BETWEEN 2 PRECEDING AND 2 FOLLOWING) AS smoothed_load_weight
    FROM temp_base
    WINDOW w AS (PARTITION BY device_id, device_date ORDER BY timestamp)
  ),
  duration_prep AS (
    SELECT
      *,
      SUM(CASE WHEN is_stationary != prev_stationary THEN 1 ELSE 0 END) OVER w AS stationary_block_id
    FROM basic_windows
    WINDOW w AS (PARTITION BY device_id, device_date ORDER BY timestamp)
  )
SELECT
  -- Select all final columns and calculate the remaining window functions
  timestamp, ingested_at, raw_event_hash_id, device_id, device_date, system_engaged,
  parking_brake_applied, current_position, current_speed, load_weight, state,
  software_state, prndl, extras, location_type, is_stationary, altitude,
  (altitude - LAG(altitude, 1) OVER w) AS altitude_rate_of_change,
  AVG(current_speed) OVER (w ROWS BETWEEN 2 PRECEDING AND 2 FOLLOWING) AS speed_rolling_avg_5s,
  smoothed_load_weight AS load_weight_smoothed,
  (smoothed_load_weight - LAG(smoothed_load_weight, 1) OVER w) AS load_weight_rate_of_change,
  (STDDEV(load_weight) OVER (PARTITION BY device_id) > 1000) AS has_reliable_payload,
  CASE
    WHEN is_stationary THEN SUM(time_delta) OVER (PARTITION BY device_id, device_date, stationary_block_id ORDER BY timestamp)
    ELSE 0
  END AS time_in_stationary_state
FROM duration_prep
WINDOW w AS (PARTITION BY device_id, device_date ORDER BY timestamp);]
(Background on this error at: https://sqlalche.me/e/20/f405)
2025-09-03 06:17:39,497 - ERROR - ❌ [Worker-127238498395840] Failed during SQL execution: (psycopg2.errors.SyntaxError) syntax error at or near "%"
LINE 17: WHERE t.device_date = ANY(%(chunk_list)s);
                                   ^

[SQL: -- This is a template script executed by each parallel Python worker.
-- It processes an assigned subset of device_date chunks.

-- Step A: Create a temporary table with base features FOR THIS CHUNK ONLY.
-- This uses the two-step materialization pattern correctly to avoid nested window function errors.
CREATE TEMP TABLE temp_base AS
SELECT
    t.*,
    ST_Z(t.current_position::geometry) AS altitude,
    (t.current_speed < 0.5) AS is_stationary,
    CASE
        WHEN ST_Z(t.current_position::geometry) < 200 THEN 'Loading Zone A'
        WHEN ST_Z(t.current_position::geometry) > 250 THEN 'Dump Zone B'
        ELSE 'Haul Road / Other'
    END AS location_type
FROM "02_raw_telemetry_transformed" t
WHERE t.device_date = ANY(%%(chunk_list)s);

-- Index the small temp table for the next step.
CREATE INDEX ON temp_base (device_id, device_date, timestamp);

-- Step B: Create the final results table FOR THIS CHUNK ONLY.
-- This table is uniquely named for each worker to avoid conflicts.
CREATE TEMP TABLE temp_features_127238498395840 AS
WITH
  basic_windows AS (
    SELECT
      *,
      LAG(is_stationary, 1, is_stationary) OVER w AS prev_stationary,
      COALESCE(EXTRACT(EPOCH FROM (timestamp - LAG(timestamp, 1) OVER w)), 0) AS time_delta,
      AVG(load_weight) OVER (w ROWS BETWEEN 2 PRECEDING AND 2 FOLLOWING) AS smoothed_load_weight
    FROM temp_base
    WINDOW w AS (PARTITION BY device_id, device_date ORDER BY timestamp)
  ),
  duration_prep AS (
    SELECT
      *,
      SUM(CASE WHEN is_stationary != prev_stationary THEN 1 ELSE 0 END) OVER w AS stationary_block_id
    FROM basic_windows
    WINDOW w AS (PARTITION BY device_id, device_date ORDER BY timestamp)
  )
SELECT
  -- Select all final columns and calculate the remaining window functions
  timestamp, ingested_at, raw_event_hash_id, device_id, device_date, system_engaged,
  parking_brake_applied, current_position, current_speed, load_weight, state,
  software_state, prndl, extras, location_type, is_stationary, altitude,
  (altitude - LAG(altitude, 1) OVER w) AS altitude_rate_of_change,
  AVG(current_speed) OVER (w ROWS BETWEEN 2 PRECEDING AND 2 FOLLOWING) AS speed_rolling_avg_5s,
  smoothed_load_weight AS load_weight_smoothed,
  (smoothed_load_weight - LAG(smoothed_load_weight, 1) OVER w) AS load_weight_rate_of_change,
  (STDDEV(load_weight) OVER (PARTITION BY device_id) > 1000) AS has_reliable_payload,
  CASE
    WHEN is_stationary THEN SUM(time_delta) OVER (PARTITION BY device_id, device_date, stationary_block_id ORDER BY timestamp)
    ELSE 0
  END AS time_in_stationary_state
FROM duration_prep
WINDOW w AS (PARTITION BY device_id, device_date ORDER BY timestamp);]
(Background on this error at: https://sqlalche.me/e/20/f405)
2025-09-03 06:17:39,499 - ERROR - ❌ [Worker-127238490003136] Failed during SQL execution: (psycopg2.errors.SyntaxError) syntax error at or near "%"
LINE 17: WHERE t.device_date = ANY(%(chunk_list)s);
                                   ^

[SQL: -- This is a template script executed by each parallel Python worker.
-- It processes an assigned subset of device_date chunks.

-- Step A: Create a temporary table with base features FOR THIS CHUNK ONLY.
-- This uses the two-step materialization pattern correctly to avoid nested window function errors.
CREATE TEMP TABLE temp_base AS
SELECT
    t.*,
    ST_Z(t.current_position::geometry) AS altitude,
    (t.current_speed < 0.5) AS is_stationary,
    CASE
        WHEN ST_Z(t.current_position::geometry) < 200 THEN 'Loading Zone A'
        WHEN ST_Z(t.current_position::geometry) > 250 THEN 'Dump Zone B'
        ELSE 'Haul Road / Other'
    END AS location_type
FROM "02_raw_telemetry_transformed" t
WHERE t.device_date = ANY(%%(chunk_list)s);

-- Index the small temp table for the next step.
CREATE INDEX ON temp_base (device_id, device_date, timestamp);

-- Step B: Create the final results table FOR THIS CHUNK ONLY.
-- This table is uniquely named for each worker to avoid conflicts.
CREATE TEMP TABLE temp_features_127238490003136 AS
WITH
  basic_windows AS (
    SELECT
      *,
      LAG(is_stationary, 1, is_stationary) OVER w AS prev_stationary,
      COALESCE(EXTRACT(EPOCH FROM (timestamp - LAG(timestamp, 1) OVER w)), 0) AS time_delta,
      AVG(load_weight) OVER (w ROWS BETWEEN 2 PRECEDING AND 2 FOLLOWING) AS smoothed_load_weight
    FROM temp_base
    WINDOW w AS (PARTITION BY device_id, device_date ORDER BY timestamp)
  ),
  duration_prep AS (
    SELECT
      *,
      SUM(CASE WHEN is_stationary != prev_stationary THEN 1 ELSE 0 END) OVER w AS stationary_block_id
    FROM basic_windows
    WINDOW w AS (PARTITION BY device_id, device_date ORDER BY timestamp)
  )
SELECT
  -- Select all final columns and calculate the remaining window functions
  timestamp, ingested_at, raw_event_hash_id, device_id, device_date, system_engaged,
  parking_brake_applied, current_position, current_speed, load_weight, state,
  software_state, prndl, extras, location_type, is_stationary, altitude,
  (altitude - LAG(altitude, 1) OVER w) AS altitude_rate_of_change,
  AVG(current_speed) OVER (w ROWS BETWEEN 2 PRECEDING AND 2 FOLLOWING) AS speed_rolling_avg_5s,
  smoothed_load_weight AS load_weight_smoothed,
  (smoothed_load_weight - LAG(smoothed_load_weight, 1) OVER w) AS load_weight_rate_of_change,
  (STDDEV(load_weight) OVER (PARTITION BY device_id) > 1000) AS has_reliable_payload,
  CASE
    WHEN is_stationary THEN SUM(time_delta) OVER (PARTITION BY device_id, device_date, stationary_block_id ORDER BY timestamp)
    ELSE 0
  END AS time_in_stationary_state
FROM duration_prep
WINDOW w AS (PARTITION BY device_id, device_date ORDER BY timestamp);]
(Background on this error at: https://sqlalche.me/e/20/f405)
2025-09-03 06:17:39,499 - ERROR - ❌ [Worker-127238506788544] Failed during SQL execution: (psycopg2.errors.SyntaxError) syntax error at or near "%"
LINE 17: WHERE t.device_date = ANY(%(chunk_list)s);
                                   ^

[SQL: -- This is a template script executed by each parallel Python worker.
-- It processes an assigned subset of device_date chunks.

-- Step A: Create a temporary table with base features FOR THIS CHUNK ONLY.
-- This uses the two-step materialization pattern correctly to avoid nested window function errors.
CREATE TEMP TABLE temp_base AS
SELECT
    t.*,
    ST_Z(t.current_position::geometry) AS altitude,
    (t.current_speed < 0.5) AS is_stationary,
    CASE
        WHEN ST_Z(t.current_position::geometry) < 200 THEN 'Loading Zone A'
        WHEN ST_Z(t.current_position::geometry) > 250 THEN 'Dump Zone B'
        ELSE 'Haul Road / Other'
    END AS location_type
FROM "02_raw_telemetry_transformed" t
WHERE t.device_date = ANY(%%(chunk_list)s);

-- Index the small temp table for the next step.
CREATE INDEX ON temp_base (device_id, device_date, timestamp);

-- Step B: Create the final results table FOR THIS CHUNK ONLY.
-- This table is uniquely named for each worker to avoid conflicts.
CREATE TEMP TABLE temp_features_127238506788544 AS
WITH
  basic_windows AS (
    SELECT
      *,
      LAG(is_stationary, 1, is_stationary) OVER w AS prev_stationary,
      COALESCE(EXTRACT(EPOCH FROM (timestamp - LAG(timestamp, 1) OVER w)), 0) AS time_delta,
      AVG(load_weight) OVER (w ROWS BETWEEN 2 PRECEDING AND 2 FOLLOWING) AS smoothed_load_weight
    FROM temp_base
    WINDOW w AS (PARTITION BY device_id, device_date ORDER BY timestamp)
  ),
  duration_prep AS (
    SELECT
      *,
      SUM(CASE WHEN is_stationary != prev_stationary THEN 1 ELSE 0 END) OVER w AS stationary_block_id
    FROM basic_windows
    WINDOW w AS (PARTITION BY device_id, device_date ORDER BY timestamp)
  )
SELECT
  -- Select all final columns and calculate the remaining window functions
  timestamp, ingested_at, raw_event_hash_id, device_id, device_date, system_engaged,
  parking_brake_applied, current_position, current_speed, load_weight, state,
  software_state, prndl, extras, location_type, is_stationary, altitude,
  (altitude - LAG(altitude, 1) OVER w) AS altitude_rate_of_change,
  AVG(current_speed) OVER (w ROWS BETWEEN 2 PRECEDING AND 2 FOLLOWING) AS speed_rolling_avg_5s,
  smoothed_load_weight AS load_weight_smoothed,
  (smoothed_load_weight - LAG(smoothed_load_weight, 1) OVER w) AS load_weight_rate_of_change,
  (STDDEV(load_weight) OVER (PARTITION BY device_id) > 1000) AS has_reliable_payload,
  CASE
    WHEN is_stationary THEN SUM(time_delta) OVER (PARTITION BY device_id, device_date, stationary_block_id ORDER BY timestamp)
    ELSE 0
  END AS time_in_stationary_state
FROM duration_prep
WINDOW w AS (PARTITION BY device_id, device_date ORDER BY timestamp);]
(Background on this error at: https://sqlalche.me/e/20/f405)
2025-09-03 06:17:39,503 - ERROR - ❌ [Worker-127238003488448] Failed during SQL execution: (psycopg2.errors.SyntaxError) syntax error at or near "%"
LINE 17: WHERE t.device_date = ANY(%(chunk_list)s);
                                   ^

[SQL: -- This is a template script executed by each parallel Python worker.
-- It processes an assigned subset of device_date chunks.

-- Step A: Create a temporary table with base features FOR THIS CHUNK ONLY.
-- This uses the two-step materialization pattern correctly to avoid nested window function errors.
CREATE TEMP TABLE temp_base AS
SELECT
    t.*,
    ST_Z(t.current_position::geometry) AS altitude,
    (t.current_speed < 0.5) AS is_stationary,
    CASE
        WHEN ST_Z(t.current_position::geometry) < 200 THEN 'Loading Zone A'
        WHEN ST_Z(t.current_position::geometry) > 250 THEN 'Dump Zone B'
        ELSE 'Haul Road / Other'
    END AS location_type
FROM "02_raw_telemetry_transformed" t
WHERE t.device_date = ANY(%%(chunk_list)s);

-- Index the small temp table for the next step.
CREATE INDEX ON temp_base (device_id, device_date, timestamp);

-- Step B: Create the final results table FOR THIS CHUNK ONLY.
-- This table is uniquely named for each worker to avoid conflicts.
CREATE TEMP TABLE temp_features_127238003488448 AS
WITH
  basic_windows AS (
    SELECT
      *,
      LAG(is_stationary, 1, is_stationary) OVER w AS prev_stationary,
      COALESCE(EXTRACT(EPOCH FROM (timestamp - LAG(timestamp, 1) OVER w)), 0) AS time_delta,
      AVG(load_weight) OVER (w ROWS BETWEEN 2 PRECEDING AND 2 FOLLOWING) AS smoothed_load_weight
    FROM temp_base
    WINDOW w AS (PARTITION BY device_id, device_date ORDER BY timestamp)
  ),
  duration_prep AS (
    SELECT
      *,
      SUM(CASE WHEN is_stationary != prev_stationary THEN 1 ELSE 0 END) OVER w AS stationary_block_id
    FROM basic_windows
    WINDOW w AS (PARTITION BY device_id, device_date ORDER BY timestamp)
  )
SELECT
  -- Select all final columns and calculate the remaining window functions
  timestamp, ingested_at, raw_event_hash_id, device_id, device_date, system_engaged,
  parking_brake_applied, current_position, current_speed, load_weight, state,
  software_state, prndl, extras, location_type, is_stationary, altitude,
  (altitude - LAG(altitude, 1) OVER w) AS altitude_rate_of_change,
  AVG(current_speed) OVER (w ROWS BETWEEN 2 PRECEDING AND 2 FOLLOWING) AS speed_rolling_avg_5s,
  smoothed_load_weight AS load_weight_smoothed,
  (smoothed_load_weight - LAG(smoothed_load_weight, 1) OVER w) AS load_weight_rate_of_change,
  (STDDEV(load_weight) OVER (PARTITION BY device_id) > 1000) AS has_reliable_payload,
  CASE
    WHEN is_stationary THEN SUM(time_delta) OVER (PARTITION BY device_id, device_date, stationary_block_id ORDER BY timestamp)
    ELSE 0
  END AS time_in_stationary_state
FROM duration_prep
WINDOW w AS (PARTITION BY device_id, device_date ORDER BY timestamp);]
(Background on this error at: https://sqlalche.me/e/20/f405)
2025-09-03 06:17:39,507 - ERROR - ❌ [Worker-127239431636672] Failed during SQL execution: (psycopg2.errors.SyntaxError) syntax error at or near "%"
LINE 17: WHERE t.device_date = ANY(%(chunk_list)s);
                                   ^

[SQL: -- This is a template script executed by each parallel Python worker.
-- It processes an assigned subset of device_date chunks.

-- Step A: Create a temporary table with base features FOR THIS CHUNK ONLY.
-- This uses the two-step materialization pattern correctly to avoid nested window function errors.
CREATE TEMP TABLE temp_base AS
SELECT
    t.*,
    ST_Z(t.current_position::geometry) AS altitude,
    (t.current_speed < 0.5) AS is_stationary,
    CASE
        WHEN ST_Z(t.current_position::geometry) < 200 THEN 'Loading Zone A'
        WHEN ST_Z(t.current_position::geometry) > 250 THEN 'Dump Zone B'
        ELSE 'Haul Road / Other'
    END AS location_type
FROM "02_raw_telemetry_transformed" t
WHERE t.device_date = ANY(%%(chunk_list)s);

-- Index the small temp table for the next step.
CREATE INDEX ON temp_base (device_id, device_date, timestamp);

-- Step B: Create the final results table FOR THIS CHUNK ONLY.
-- This table is uniquely named for each worker to avoid conflicts.
CREATE TEMP TABLE temp_features_127239431636672 AS
WITH
  basic_windows AS (
    SELECT
      *,
      LAG(is_stationary, 1, is_stationary) OVER w AS prev_stationary,
      COALESCE(EXTRACT(EPOCH FROM (timestamp - LAG(timestamp, 1) OVER w)), 0) AS time_delta,
      AVG(load_weight) OVER (w ROWS BETWEEN 2 PRECEDING AND 2 FOLLOWING) AS smoothed_load_weight
    FROM temp_base
    WINDOW w AS (PARTITION BY device_id, device_date ORDER BY timestamp)
  ),
  duration_prep AS (
    SELECT
      *,
      SUM(CASE WHEN is_stationary != prev_stationary THEN 1 ELSE 0 END) OVER w AS stationary_block_id
    FROM basic_windows
    WINDOW w AS (PARTITION BY device_id, device_date ORDER BY timestamp)
  )
SELECT
  -- Select all final columns and calculate the remaining window functions
  timestamp, ingested_at, raw_event_hash_id, device_id, device_date, system_engaged,
  parking_brake_applied, current_position, current_speed, load_weight, state,
  software_state, prndl, extras, location_type, is_stationary, altitude,
  (altitude - LAG(altitude, 1) OVER w) AS altitude_rate_of_change,
  AVG(current_speed) OVER (w ROWS BETWEEN 2 PRECEDING AND 2 FOLLOWING) AS speed_rolling_avg_5s,
  smoothed_load_weight AS load_weight_smoothed,
  (smoothed_load_weight - LAG(smoothed_load_weight, 1) OVER w) AS load_weight_rate_of_change,
  (STDDEV(load_weight) OVER (PARTITION BY device_id) > 1000) AS has_reliable_payload,
  CASE
    WHEN is_stationary THEN SUM(time_delta) OVER (PARTITION BY device_id, device_date, stationary_block_id ORDER BY timestamp)
    ELSE 0
  END AS time_in_stationary_state
FROM duration_prep
WINDOW w AS (PARTITION BY device_id, device_date ORDER BY timestamp);]
(Background on this error at: https://sqlalche.me/e/20/f405)
2025-09-03 06:17:39,509 - ERROR - ❌ [Worker-127239052052160] Failed during SQL execution: (psycopg2.errors.SyntaxError) syntax error at or near "%"
LINE 17: WHERE t.device_date = ANY(%(chunk_list)s);
                                   ^

[SQL: -- This is a template script executed by each parallel Python worker.
-- It processes an assigned subset of device_date chunks.

-- Step A: Create a temporary table with base features FOR THIS CHUNK ONLY.
-- This uses the two-step materialization pattern correctly to avoid nested window function errors.
CREATE TEMP TABLE temp_base AS
SELECT
    t.*,
    ST_Z(t.current_position::geometry) AS altitude,
    (t.current_speed < 0.5) AS is_stationary,
    CASE
        WHEN ST_Z(t.current_position::geometry) < 200 THEN 'Loading Zone A'
        WHEN ST_Z(t.current_position::geometry) > 250 THEN 'Dump Zone B'
        ELSE 'Haul Road / Other'
    END AS location_type
FROM "02_raw_telemetry_transformed" t
WHERE t.device_date = ANY(%%(chunk_list)s);

-- Index the small temp table for the next step.
CREATE INDEX ON temp_base (device_id, device_date, timestamp);

-- Step B: Create the final results table FOR THIS CHUNK ONLY.
-- This table is uniquely named for each worker to avoid conflicts.
CREATE TEMP TABLE temp_features_127239052052160 AS
WITH
  basic_windows AS (
    SELECT
      *,
      LAG(is_stationary, 1, is_stationary) OVER w AS prev_stationary,
      COALESCE(EXTRACT(EPOCH FROM (timestamp - LAG(timestamp, 1) OVER w)), 0) AS time_delta,
      AVG(load_weight) OVER (w ROWS BETWEEN 2 PRECEDING AND 2 FOLLOWING) AS smoothed_load_weight
    FROM temp_base
    WINDOW w AS (PARTITION BY device_id, device_date ORDER BY timestamp)
  ),
  duration_prep AS (
    SELECT
      *,
      SUM(CASE WHEN is_stationary != prev_stationary THEN 1 ELSE 0 END) OVER w AS stationary_block_id
    FROM basic_windows
    WINDOW w AS (PARTITION BY device_id, device_date ORDER BY timestamp)
  )
SELECT
  -- Select all final columns and calculate the remaining window functions
  timestamp, ingested_at, raw_event_hash_id, device_id, device_date, system_engaged,
  parking_brake_applied, current_position, current_speed, load_weight, state,
  software_state, prndl, extras, location_type, is_stationary, altitude,
  (altitude - LAG(altitude, 1) OVER w) AS altitude_rate_of_change,
  AVG(current_speed) OVER (w ROWS BETWEEN 2 PRECEDING AND 2 FOLLOWING) AS speed_rolling_avg_5s,
  smoothed_load_weight AS load_weight_smoothed,
  (smoothed_load_weight - LAG(smoothed_load_weight, 1) OVER w) AS load_weight_rate_of_change,
  (STDDEV(load_weight) OVER (PARTITION BY device_id) > 1000) AS has_reliable_payload,
  CASE
    WHEN is_stationary THEN SUM(time_delta) OVER (PARTITION BY device_id, device_date, stationary_block_id ORDER BY timestamp)
    ELSE 0
  END AS time_in_stationary_state
FROM duration_prep
WINDOW w AS (PARTITION BY device_id, device_date ORDER BY timestamp);]
(Background on this error at: https://sqlalche.me/e/20/f405)
2025-09-03 06:17:39,513 - ERROR - ❌ [Worker-127237995095744] Failed during SQL execution: (psycopg2.errors.SyntaxError) syntax error at or near "%"
LINE 17: WHERE t.device_date = ANY(%(chunk_list)s);
                                   ^

[SQL: -- This is a template script executed by each parallel Python worker.
-- It processes an assigned subset of device_date chunks.

-- Step A: Create a temporary table with base features FOR THIS CHUNK ONLY.
-- This uses the two-step materialization pattern correctly to avoid nested window function errors.
CREATE TEMP TABLE temp_base AS
SELECT
    t.*,
    ST_Z(t.current_position::geometry) AS altitude,
    (t.current_speed < 0.5) AS is_stationary,
    CASE
        WHEN ST_Z(t.current_position::geometry) < 200 THEN 'Loading Zone A'
        WHEN ST_Z(t.current_position::geometry) > 250 THEN 'Dump Zone B'
        ELSE 'Haul Road / Other'
    END AS location_type
FROM "02_raw_telemetry_transformed" t
WHERE t.device_date = ANY(%%(chunk_list)s);

-- Index the small temp table for the next step.
CREATE INDEX ON temp_base (device_id, device_date, timestamp);

-- Step B: Create the final results table FOR THIS CHUNK ONLY.
-- This table is uniquely named for each worker to avoid conflicts.
CREATE TEMP TABLE temp_features_127237995095744 AS
WITH
  basic_windows AS (
    SELECT
      *,
      LAG(is_stationary, 1, is_stationary) OVER w AS prev_stationary,
      COALESCE(EXTRACT(EPOCH FROM (timestamp - LAG(timestamp, 1) OVER w)), 0) AS time_delta,
      AVG(load_weight) OVER (w ROWS BETWEEN 2 PRECEDING AND 2 FOLLOWING) AS smoothed_load_weight
    FROM temp_base
    WINDOW w AS (PARTITION BY device_id, device_date ORDER BY timestamp)
  ),
  duration_prep AS (
    SELECT
      *,
      SUM(CASE WHEN is_stationary != prev_stationary THEN 1 ELSE 0 END) OVER w AS stationary_block_id
    FROM basic_windows
    WINDOW w AS (PARTITION BY device_id, device_date ORDER BY timestamp)
  )
SELECT
  -- Select all final columns and calculate the remaining window functions
  timestamp, ingested_at, raw_event_hash_id, device_id, device_date, system_engaged,
  parking_brake_applied, current_position, current_speed, load_weight, state,
  software_state, prndl, extras, location_type, is_stationary, altitude,
  (altitude - LAG(altitude, 1) OVER w) AS altitude_rate_of_change,
  AVG(current_speed) OVER (w ROWS BETWEEN 2 PRECEDING AND 2 FOLLOWING) AS speed_rolling_avg_5s,
  smoothed_load_weight AS load_weight_smoothed,
  (smoothed_load_weight - LAG(smoothed_load_weight, 1) OVER w) AS load_weight_rate_of_change,
  (STDDEV(load_weight) OVER (PARTITION BY device_id) > 1000) AS has_reliable_payload,
  CASE
    WHEN is_stationary THEN SUM(time_delta) OVER (PARTITION BY device_id, device_date, stationary_block_id ORDER BY timestamp)
    ELSE 0
  END AS time_in_stationary_state
FROM duration_prep
WINDOW w AS (PARTITION BY device_id, device_date ORDER BY timestamp);]
(Background on this error at: https://sqlalche.me/e/20/f405)
2025-09-03 06:17:39,515 - ERROR - ❌ [Worker-127239448422080] Failed during SQL execution: (psycopg2.errors.SyntaxError) syntax error at or near "%"
LINE 17: WHERE t.device_date = ANY(%(chunk_list)s);
                                   ^

[SQL: -- This is a template script executed by each parallel Python worker.
-- It processes an assigned subset of device_date chunks.

-- Step A: Create a temporary table with base features FOR THIS CHUNK ONLY.
-- This uses the two-step materialization pattern correctly to avoid nested window function errors.
CREATE TEMP TABLE temp_base AS
SELECT
    t.*,
    ST_Z(t.current_position::geometry) AS altitude,
    (t.current_speed < 0.5) AS is_stationary,
    CASE
        WHEN ST_Z(t.current_position::geometry) < 200 THEN 'Loading Zone A'
        WHEN ST_Z(t.current_position::geometry) > 250 THEN 'Dump Zone B'
        ELSE 'Haul Road / Other'
    END AS location_type
FROM "02_raw_telemetry_transformed" t
WHERE t.device_date = ANY(%%(chunk_list)s);

-- Index the small temp table for the next step.
CREATE INDEX ON temp_base (device_id, device_date, timestamp);

-- Step B: Create the final results table FOR THIS CHUNK ONLY.
-- This table is uniquely named for each worker to avoid conflicts.
CREATE TEMP TABLE temp_features_127239448422080 AS
WITH
  basic_windows AS (
    SELECT
      *,
      LAG(is_stationary, 1, is_stationary) OVER w AS prev_stationary,
      COALESCE(EXTRACT(EPOCH FROM (timestamp - LAG(timestamp, 1) OVER w)), 0) AS time_delta,
      AVG(load_weight) OVER (w ROWS BETWEEN 2 PRECEDING AND 2 FOLLOWING) AS smoothed_load_weight
    FROM temp_base
    WINDOW w AS (PARTITION BY device_id, device_date ORDER BY timestamp)
  ),
  duration_prep AS (
    SELECT
      *,
      SUM(CASE WHEN is_stationary != prev_stationary THEN 1 ELSE 0 END) OVER w AS stationary_block_id
    FROM basic_windows
    WINDOW w AS (PARTITION BY device_id, device_date ORDER BY timestamp)
  )
SELECT
  -- Select all final columns and calculate the remaining window functions
  timestamp, ingested_at, raw_event_hash_id, device_id, device_date, system_engaged,
  parking_brake_applied, current_position, current_speed, load_weight, state,
  software_state, prndl, extras, location_type, is_stationary, altitude,
  (altitude - LAG(altitude, 1) OVER w) AS altitude_rate_of_change,
  AVG(current_speed) OVER (w ROWS BETWEEN 2 PRECEDING AND 2 FOLLOWING) AS speed_rolling_avg_5s,
  smoothed_load_weight AS load_weight_smoothed,
  (smoothed_load_weight - LAG(smoothed_load_weight, 1) OVER w) AS load_weight_rate_of_change,
  (STDDEV(load_weight) OVER (PARTITION BY device_id) > 1000) AS has_reliable_payload,
  CASE
    WHEN is_stationary THEN SUM(time_delta) OVER (PARTITION BY device_id, device_date, stationary_block_id ORDER BY timestamp)
    ELSE 0
  END AS time_in_stationary_state
FROM duration_prep
WINDOW w AS (PARTITION BY device_id, device_date ORDER BY timestamp);]
(Background on this error at: https://sqlalche.me/e/20/f405)
2025-09-03 06:17:39,516 - ERROR - ❌ [Worker-127237978310336] Failed during SQL execution: (psycopg2.errors.SyntaxError) syntax error at or near "%"
LINE 17: WHERE t.device_date = ANY(%(chunk_list)s);
                                   ^

[SQL: -- This is a template script executed by each parallel Python worker.
-- It processes an assigned subset of device_date chunks.

-- Step A: Create a temporary table with base features FOR THIS CHUNK ONLY.
-- This uses the two-step materialization pattern correctly to avoid nested window function errors.
CREATE TEMP TABLE temp_base AS
SELECT
    t.*,
    ST_Z(t.current_position::geometry) AS altitude,
    (t.current_speed < 0.5) AS is_stationary,
    CASE
        WHEN ST_Z(t.current_position::geometry) < 200 THEN 'Loading Zone A'
        WHEN ST_Z(t.current_position::geometry) > 250 THEN 'Dump Zone B'
        ELSE 'Haul Road / Other'
    END AS location_type
FROM "02_raw_telemetry_transformed" t
WHERE t.device_date = ANY(%%(chunk_list)s);

-- Index the small temp table for the next step.
CREATE INDEX ON temp_base (device_id, device_date, timestamp);

-- Step B: Create the final results table FOR THIS CHUNK ONLY.
-- This table is uniquely named for each worker to avoid conflicts.
CREATE TEMP TABLE temp_features_127237978310336 AS
WITH
  basic_windows AS (
    SELECT
      *,
      LAG(is_stationary, 1, is_stationary) OVER w AS prev_stationary,
      COALESCE(EXTRACT(EPOCH FROM (timestamp - LAG(timestamp, 1) OVER w)), 0) AS time_delta,
      AVG(load_weight) OVER (w ROWS BETWEEN 2 PRECEDING AND 2 FOLLOWING) AS smoothed_load_weight
    FROM temp_base
    WINDOW w AS (PARTITION BY device_id, device_date ORDER BY timestamp)
  ),
  duration_prep AS (
    SELECT
      *,
      SUM(CASE WHEN is_stationary != prev_stationary THEN 1 ELSE 0 END) OVER w AS stationary_block_id
    FROM basic_windows
    WINDOW w AS (PARTITION BY device_id, device_date ORDER BY timestamp)
  )
SELECT
  -- Select all final columns and calculate the remaining window functions
  timestamp, ingested_at, raw_event_hash_id, device_id, device_date, system_engaged,
  parking_brake_applied, current_position, current_speed, load_weight, state,
  software_state, prndl, extras, location_type, is_stationary, altitude,
  (altitude - LAG(altitude, 1) OVER w) AS altitude_rate_of_change,
  AVG(current_speed) OVER (w ROWS BETWEEN 2 PRECEDING AND 2 FOLLOWING) AS speed_rolling_avg_5s,
  smoothed_load_weight AS load_weight_smoothed,
  (smoothed_load_weight - LAG(smoothed_load_weight, 1) OVER w) AS load_weight_rate_of_change,
  (STDDEV(load_weight) OVER (PARTITION BY device_id) > 1000) AS has_reliable_payload,
  CASE
    WHEN is_stationary THEN SUM(time_delta) OVER (PARTITION BY device_id, device_date, stationary_block_id ORDER BY timestamp)
    ELSE 0
  END AS time_in_stationary_state
FROM duration_prep
WINDOW w AS (PARTITION BY device_id, device_date ORDER BY timestamp);]
(Background on this error at: https://sqlalche.me/e/20/f405)
2025-09-03 06:17:39,517 - ERROR - ❌ [Worker-127237986703040] Failed during SQL execution: (psycopg2.errors.SyntaxError) syntax error at or near "%"
LINE 17: WHERE t.device_date = ANY(%(chunk_list)s);
                                   ^

[SQL: -- This is a template script executed by each parallel Python worker.
-- It processes an assigned subset of device_date chunks.

-- Step A: Create a temporary table with base features FOR THIS CHUNK ONLY.
-- This uses the two-step materialization pattern correctly to avoid nested window function errors.
CREATE TEMP TABLE temp_base AS
SELECT
    t.*,
    ST_Z(t.current_position::geometry) AS altitude,
    (t.current_speed < 0.5) AS is_stationary,
    CASE
        WHEN ST_Z(t.current_position::geometry) < 200 THEN 'Loading Zone A'
        WHEN ST_Z(t.current_position::geometry) > 250 THEN 'Dump Zone B'
        ELSE 'Haul Road / Other'
    END AS location_type
FROM "02_raw_telemetry_transformed" t
WHERE t.device_date = ANY(%%(chunk_list)s);

-- Index the small temp table for the next step.
CREATE INDEX ON temp_base (device_id, device_date, timestamp);

-- Step B: Create the final results table FOR THIS CHUNK ONLY.
-- This table is uniquely named for each worker to avoid conflicts.
CREATE TEMP TABLE temp_features_127237986703040 AS
WITH
  basic_windows AS (
    SELECT
      *,
      LAG(is_stationary, 1, is_stationary) OVER w AS prev_stationary,
      COALESCE(EXTRACT(EPOCH FROM (timestamp - LAG(timestamp, 1) OVER w)), 0) AS time_delta,
      AVG(load_weight) OVER (w ROWS BETWEEN 2 PRECEDING AND 2 FOLLOWING) AS smoothed_load_weight
    FROM temp_base
    WINDOW w AS (PARTITION BY device_id, device_date ORDER BY timestamp)
  ),
  duration_prep AS (
    SELECT
      *,
      SUM(CASE WHEN is_stationary != prev_stationary THEN 1 ELSE 0 END) OVER w AS stationary_block_id
    FROM basic_windows
    WINDOW w AS (PARTITION BY device_id, device_date ORDER BY timestamp)
  )
SELECT
  -- Select all final columns and calculate the remaining window functions
  timestamp, ingested_at, raw_event_hash_id, device_id, device_date, system_engaged,
  parking_brake_applied, current_position, current_speed, load_weight, state,
  software_state, prndl, extras, location_type, is_stationary, altitude,
  (altitude - LAG(altitude, 1) OVER w) AS altitude_rate_of_change,
  AVG(current_speed) OVER (w ROWS BETWEEN 2 PRECEDING AND 2 FOLLOWING) AS speed_rolling_avg_5s,
  smoothed_load_weight AS load_weight_smoothed,
  (smoothed_load_weight - LAG(smoothed_load_weight, 1) OVER w) AS load_weight_rate_of_change,
  (STDDEV(load_weight) OVER (PARTITION BY device_id) > 1000) AS has_reliable_payload,
  CASE
    WHEN is_stationary THEN SUM(time_delta) OVER (PARTITION BY device_id, device_date, stationary_block_id ORDER BY timestamp)
    ELSE 0
  END AS time_in_stationary_state
FROM duration_prep
WINDOW w AS (PARTITION BY device_id, device_date ORDER BY timestamp);]
(Background on this error at: https://sqlalche.me/e/20/f405)
2025-09-03 06:17:39,518 - ERROR - ❌ [Worker-127239077230272] Failed during SQL execution: (psycopg2.errors.SyntaxError) syntax error at or near "%"
LINE 17: WHERE t.device_date = ANY(%(chunk_list)s);
                                   ^

[SQL: -- This is a template script executed by each parallel Python worker.
-- It processes an assigned subset of device_date chunks.

-- Step A: Create a temporary table with base features FOR THIS CHUNK ONLY.
-- This uses the two-step materialization pattern correctly to avoid nested window function errors.
CREATE TEMP TABLE temp_base AS
SELECT
    t.*,
    ST_Z(t.current_position::geometry) AS altitude,
    (t.current_speed < 0.5) AS is_stationary,
    CASE
        WHEN ST_Z(t.current_position::geometry) < 200 THEN 'Loading Zone A'
        WHEN ST_Z(t.current_position::geometry) > 250 THEN 'Dump Zone B'
        ELSE 'Haul Road / Other'
    END AS location_type
FROM "02_raw_telemetry_transformed" t
WHERE t.device_date = ANY(%%(chunk_list)s);

-- Index the small temp table for the next step.
CREATE INDEX ON temp_base (device_id, device_date, timestamp);

-- Step B: Create the final results table FOR THIS CHUNK ONLY.
-- This table is uniquely named for each worker to avoid conflicts.
CREATE TEMP TABLE temp_features_127239077230272 AS
WITH
  basic_windows AS (
    SELECT
      *,
      LAG(is_stationary, 1, is_stationary) OVER w AS prev_stationary,
      COALESCE(EXTRACT(EPOCH FROM (timestamp - LAG(timestamp, 1) OVER w)), 0) AS time_delta,
      AVG(load_weight) OVER (w ROWS BETWEEN 2 PRECEDING AND 2 FOLLOWING) AS smoothed_load_weight
    FROM temp_base
    WINDOW w AS (PARTITION BY device_id, device_date ORDER BY timestamp)
  ),
  duration_prep AS (
    SELECT
      *,
      SUM(CASE WHEN is_stationary != prev_stationary THEN 1 ELSE 0 END) OVER w AS stationary_block_id
    FROM basic_windows
    WINDOW w AS (PARTITION BY device_id, device_date ORDER BY timestamp)
  )
SELECT
  -- Select all final columns and calculate the remaining window functions
  timestamp, ingested_at, raw_event_hash_id, device_id, device_date, system_engaged,
  parking_brake_applied, current_position, current_speed, load_weight, state,
  software_state, prndl, extras, location_type, is_stationary, altitude,
  (altitude - LAG(altitude, 1) OVER w) AS altitude_rate_of_change,
  AVG(current_speed) OVER (w ROWS BETWEEN 2 PRECEDING AND 2 FOLLOWING) AS speed_rolling_avg_5s,
  smoothed_load_weight AS load_weight_smoothed,
  (smoothed_load_weight - LAG(smoothed_load_weight, 1) OVER w) AS load_weight_rate_of_change,
  (STDDEV(load_weight) OVER (PARTITION BY device_id) > 1000) AS has_reliable_payload,
  CASE
    WHEN is_stationary THEN SUM(time_delta) OVER (PARTITION BY device_id, device_date, stationary_block_id ORDER BY timestamp)
    ELSE 0
  END AS time_in_stationary_state
FROM duration_prep
WINDOW w AS (PARTITION BY device_id, device_date ORDER BY timestamp);]
(Background on this error at: https://sqlalche.me/e/20/f405)
2025-09-03 06:17:39,518 - ERROR - ❌ [Worker-127237969917632] Failed during SQL execution: (psycopg2.errors.SyntaxError) syntax error at or near "%"
LINE 17: WHERE t.device_date = ANY(%(chunk_list)s);
                                   ^

[SQL: -- This is a template script executed by each parallel Python worker.
-- It processes an assigned subset of device_date chunks.

-- Step A: Create a temporary table with base features FOR THIS CHUNK ONLY.
-- This uses the two-step materialization pattern correctly to avoid nested window function errors.
CREATE TEMP TABLE temp_base AS
SELECT
    t.*,
    ST_Z(t.current_position::geometry) AS altitude,
    (t.current_speed < 0.5) AS is_stationary,
    CASE
        WHEN ST_Z(t.current_position::geometry) < 200 THEN 'Loading Zone A'
        WHEN ST_Z(t.current_position::geometry) > 250 THEN 'Dump Zone B'
        ELSE 'Haul Road / Other'
    END AS location_type
FROM "02_raw_telemetry_transformed" t
WHERE t.device_date = ANY(%%(chunk_list)s);

-- Index the small temp table for the next step.
CREATE INDEX ON temp_base (device_id, device_date, timestamp);

-- Step B: Create the final results table FOR THIS CHUNK ONLY.
-- This table is uniquely named for each worker to avoid conflicts.
CREATE TEMP TABLE temp_features_127237969917632 AS
WITH
  basic_windows AS (
    SELECT
      *,
      LAG(is_stationary, 1, is_stationary) OVER w AS prev_stationary,
      COALESCE(EXTRACT(EPOCH FROM (timestamp - LAG(timestamp, 1) OVER w)), 0) AS time_delta,
      AVG(load_weight) OVER (w ROWS BETWEEN 2 PRECEDING AND 2 FOLLOWING) AS smoothed_load_weight
    FROM temp_base
    WINDOW w AS (PARTITION BY device_id, device_date ORDER BY timestamp)
  ),
  duration_prep AS (
    SELECT
      *,
      SUM(CASE WHEN is_stationary != prev_stationary THEN 1 ELSE 0 END) OVER w AS stationary_block_id
    FROM basic_windows
    WINDOW w AS (PARTITION BY device_id, device_date ORDER BY timestamp)
  )
SELECT
  -- Select all final columns and calculate the remaining window functions
  timestamp, ingested_at, raw_event_hash_id, device_id, device_date, system_engaged,
  parking_brake_applied, current_position, current_speed, load_weight, state,
  software_state, prndl, extras, location_type, is_stationary, altitude,
  (altitude - LAG(altitude, 1) OVER w) AS altitude_rate_of_change,
  AVG(current_speed) OVER (w ROWS BETWEEN 2 PRECEDING AND 2 FOLLOWING) AS speed_rolling_avg_5s,
  smoothed_load_weight AS load_weight_smoothed,
  (smoothed_load_weight - LAG(smoothed_load_weight, 1) OVER w) AS load_weight_rate_of_change,
  (STDDEV(load_weight) OVER (PARTITION BY device_id) > 1000) AS has_reliable_payload,
  CASE
    WHEN is_stationary THEN SUM(time_delta) OVER (PARTITION BY device_id, device_date, stationary_block_id ORDER BY timestamp)
    ELSE 0
  END AS time_in_stationary_state
FROM duration_prep
WINDOW w AS (PARTITION BY device_id, device_date ORDER BY timestamp);]
(Background on this error at: https://sqlalche.me/e/20/f405)
2025-09-03 06:17:39,519 - ERROR - ❌ Pipeline execution failed: (psycopg2.errors.SyntaxError) syntax error at or near "%"
LINE 17: WHERE t.device_date = ANY(%(chunk_list)s);
                                   ^

[SQL: -- This is a template script executed by each parallel Python worker.
-- It processes an assigned subset of device_date chunks.

-- Step A: Create a temporary table with base features FOR THIS CHUNK ONLY.
-- This uses the two-step materialization pattern correctly to avoid nested window function errors.
CREATE TEMP TABLE temp_base AS
SELECT
    t.*,
    ST_Z(t.current_position::geometry) AS altitude,
    (t.current_speed < 0.5) AS is_stationary,
    CASE
        WHEN ST_Z(t.current_position::geometry) < 200 THEN 'Loading Zone A'
        WHEN ST_Z(t.current_position::geometry) > 250 THEN 'Dump Zone B'
        ELSE 'Haul Road / Other'
    END AS location_type
FROM "02_raw_telemetry_transformed" t
WHERE t.device_date = ANY(%%(chunk_list)s);

-- Index the small temp table for the next step.
CREATE INDEX ON temp_base (device_id, device_date, timestamp);

-- Step B: Create the final results table FOR THIS CHUNK ONLY.
-- This table is uniquely named for each worker to avoid conflicts.
CREATE TEMP TABLE temp_features_127239456814784 AS
WITH
  basic_windows AS (
    SELECT
      *,
      LAG(is_stationary, 1, is_stationary) OVER w AS prev_stationary,
      COALESCE(EXTRACT(EPOCH FROM (timestamp - LAG(timestamp, 1) OVER w)), 0) AS time_delta,
      AVG(load_weight) OVER (w ROWS BETWEEN 2 PRECEDING AND 2 FOLLOWING) AS smoothed_load_weight
    FROM temp_base
    WINDOW w AS (PARTITION BY device_id, device_date ORDER BY timestamp)
  ),
  duration_prep AS (
    SELECT
      *,
      SUM(CASE WHEN is_stationary != prev_stationary THEN 1 ELSE 0 END) OVER w AS stationary_block_id
    FROM basic_windows
    WINDOW w AS (PARTITION BY device_id, device_date ORDER BY timestamp)
  )
SELECT
  -- Select all final columns and calculate the remaining window functions
  timestamp, ingested_at, raw_event_hash_id, device_id, device_date, system_engaged,
  parking_brake_applied, current_position, current_speed, load_weight, state,
  software_state, prndl, extras, location_type, is_stationary, altitude,
  (altitude - LAG(altitude, 1) OVER w) AS altitude_rate_of_change,
  AVG(current_speed) OVER (w ROWS BETWEEN 2 PRECEDING AND 2 FOLLOWING) AS speed_rolling_avg_5s,
  smoothed_load_weight AS load_weight_smoothed,
  (smoothed_load_weight - LAG(smoothed_load_weight, 1) OVER w) AS load_weight_rate_of_change,
  (STDDEV(load_weight) OVER (PARTITION BY device_id) > 1000) AS has_reliable_payload,
  CASE
    WHEN is_stationary THEN SUM(time_delta) OVER (PARTITION BY device_id, device_date, stationary_block_id ORDER BY timestamp)
    ELSE 0
  END AS time_in_stationary_state
FROM duration_prep
WINDOW w AS (PARTITION BY device_id, device_date ORDER BY timestamp);]
(Background on this error at: https://sqlalche.me/e/20/f405)
2025-09-03 06:22:59,790 - INFO - Connected to database with high-performance configuration
2025-09-03 06:22:59,790 - INFO - Initialized with 32 parallel workers.
2025-09-03 06:22:59,790 - INFO - ======================================================================
2025-09-03 06:22:59,790 - INFO - STARTING MASSIVELY PARALLEL FEATURE ENGINEERING
2025-09-03 06:22:59,790 - INFO - ======================================================================
2025-09-03 06:22:59,790 - INFO - Fetching all device-date chunks to be processed...
2025-09-03 06:23:00,245 - INFO - ✓ Divided 96 total chunks into 32 batches for parallel processing.
2025-09-03 06:23:00,245 - INFO - 🚀 Launching parallel workers...
2025-09-03 06:23:00,247 - INFO - [Worker-128987978266304] Started, assigned 3 chunks.
2025-09-03 06:23:00,248 - INFO - [Worker-128987969873600] Started, assigned 3 chunks.
2025-09-03 06:23:00,248 - INFO - [Worker-128987961480896] Started, assigned 3 chunks.
2025-09-03 06:23:00,248 - INFO - [Worker-128987953088192] Started, assigned 3 chunks.
2025-09-03 06:23:00,249 - INFO - [Worker-128987944695488] Started, assigned 3 chunks.
2025-09-03 06:23:00,250 - INFO - [Worker-128987598681792] Started, assigned 3 chunks.
2025-09-03 06:23:00,251 - INFO - [Worker-128987590289088] Started, assigned 3 chunks.
2025-09-03 06:23:00,252 - INFO - [Worker-128987581896384] Started, assigned 3 chunks.
2025-09-03 06:23:00,254 - INFO - [Worker-128987573503680] Started, assigned 3 chunks.
2025-09-03 06:23:00,254 - INFO - [Worker-128987565110976] Started, assigned 3 chunks.
2025-09-03 06:23:00,255 - INFO - [Worker-128987556718272] Started, assigned 3 chunks.
2025-09-03 06:23:00,255 - INFO - [Worker-128987548325568] Started, assigned 3 chunks.
2025-09-03 06:23:00,257 - INFO - [Worker-128987061810880] Started, assigned 3 chunks.
2025-09-03 06:23:00,258 - INFO - [Worker-128987053418176] Started, assigned 3 chunks.
2025-09-03 06:23:00,258 - INFO - [Worker-128987045025472] Started, assigned 3 chunks.
2025-09-03 06:23:00,258 - INFO - [Worker-128987036632768] Started, assigned 3 chunks.
2025-09-03 06:23:00,259 - INFO - [Worker-128987028240064] Started, assigned 3 chunks.
2025-09-03 06:23:00,260 - INFO - [Worker-128987019847360] Started, assigned 3 chunks.
2025-09-03 06:23:00,262 - INFO - [Worker-128987011454656] Started, assigned 3 chunks.
2025-09-03 06:23:00,264 - INFO - [Worker-128986524939968] Started, assigned 3 chunks.
2025-09-03 06:23:00,266 - INFO - [Worker-128986516547264] Started, assigned 3 chunks.
2025-09-03 06:23:00,267 - ERROR - ❌ [Worker-128987969873600] Failed during SQL execution: (psycopg2.errors.WrongObjectType) op ANY/ALL (array) requires array on right side
LINE 17: WHERE t.device_date = ANY(('lake-605-10-0050_2025-08-02', 'l...
                             ^

[SQL: -- This is a template script executed by each parallel Python worker.
-- It processes an assigned subset of device_date chunks.

-- Step A: Create a temporary table with base features FOR THIS CHUNK ONLY.
-- This uses the two-step materialization pattern correctly to avoid nested window function errors.
CREATE TEMP TABLE temp_base AS
SELECT
    t.*,
    ST_Z(t.current_position::geometry) AS altitude,
    (t.current_speed < 0.5) AS is_stationary,
    CASE
        WHEN ST_Z(t.current_position::geometry) < 200 THEN 'Loading Zone A'
        WHEN ST_Z(t.current_position::geometry) > 250 THEN 'Dump Zone B'
        ELSE 'Haul Road / Other'
    END AS location_type
FROM "02_raw_telemetry_transformed" t
WHERE t.device_date = ANY(%(chunk_list)s);

-- Index the small temp table for the next step.
CREATE INDEX ON temp_base (device_id, device_date, timestamp);

-- Step B: Create the final results table FOR THIS CHUNK ONLY.
-- This table is uniquely named for each worker to avoid conflicts.
CREATE TEMP TABLE temp_features_128987969873600 AS
WITH
  basic_windows AS (
    SELECT
      *,
      LAG(is_stationary, 1, is_stationary) OVER w AS prev_stationary,
      COALESCE(EXTRACT(EPOCH FROM (timestamp - LAG(timestamp, 1) OVER w)), 0) AS time_delta,
      AVG(load_weight) OVER (w ROWS BETWEEN 2 PRECEDING AND 2 FOLLOWING) AS smoothed_load_weight
    FROM temp_base
    WINDOW w AS (PARTITION BY device_id, device_date ORDER BY timestamp)
  ),
  duration_prep AS (
    SELECT
      *,
      SUM(CASE WHEN is_stationary != prev_stationary THEN 1 ELSE 0 END) OVER w AS stationary_block_id
    FROM basic_windows
    WINDOW w AS (PARTITION BY device_id, device_date ORDER BY timestamp)
  )
SELECT
  -- Select all final columns and calculate the remaining window functions
  timestamp, ingested_at, raw_event_hash_id, device_id, device_date, system_engaged,
  parking_brake_applied, current_position, current_speed, load_weight, state,
  software_state, prndl, extras, location_type, is_stationary, altitude,
  (altitude - LAG(altitude, 1) OVER w) AS altitude_rate_of_change,
  AVG(current_speed) OVER (w ROWS BETWEEN 2 PRECEDING AND 2 FOLLOWING) AS speed_rolling_avg_5s,
  smoothed_load_weight AS load_weight_smoothed,
  (smoothed_load_weight - LAG(smoothed_load_weight, 1) OVER w) AS load_weight_rate_of_change,
  (STDDEV(load_weight) OVER (PARTITION BY device_id) > 1000) AS has_reliable_payload,
  CASE
    WHEN is_stationary THEN SUM(time_delta) OVER (PARTITION BY device_id, device_date, stationary_block_id ORDER BY timestamp)
    ELSE 0
  END AS time_in_stationary_state
FROM duration_prep
WINDOW w AS (PARTITION BY device_id, device_date ORDER BY timestamp);]
[parameters: {'chunk_list': ('lake-605-10-0050_2025-08-02', 'lake-605-10-0050_2025-08-03', 'lake-605-10-0050_2025-08-04')}]
(Background on this error at: https://sqlalche.me/e/20/f405)
2025-09-03 06:23:00,268 - INFO - [Worker-128986508154560] Started, assigned 3 chunks.
2025-09-03 06:23:00,272 - INFO - [Worker-128986499761856] Started, assigned 3 chunks.
2025-09-03 06:23:00,275 - ERROR - ❌ [Worker-128987944695488] Failed during SQL execution: (psycopg2.errors.WrongObjectType) op ANY/ALL (array) requires array on right side
LINE 17: WHERE t.device_date = ANY(('lake-605-10-0050_2025-08-11', 'l...
                             ^

[SQL: -- This is a template script executed by each parallel Python worker.
-- It processes an assigned subset of device_date chunks.

-- Step A: Create a temporary table with base features FOR THIS CHUNK ONLY.
-- This uses the two-step materialization pattern correctly to avoid nested window function errors.
CREATE TEMP TABLE temp_base AS
SELECT
    t.*,
    ST_Z(t.current_position::geometry) AS altitude,
    (t.current_speed < 0.5) AS is_stationary,
    CASE
        WHEN ST_Z(t.current_position::geometry) < 200 THEN 'Loading Zone A'
        WHEN ST_Z(t.current_position::geometry) > 250 THEN 'Dump Zone B'
        ELSE 'Haul Road / Other'
    END AS location_type
FROM "02_raw_telemetry_transformed" t
WHERE t.device_date = ANY(%(chunk_list)s);

-- Index the small temp table for the next step.
CREATE INDEX ON temp_base (device_id, device_date, timestamp);

-- Step B: Create the final results table FOR THIS CHUNK ONLY.
-- This table is uniquely named for each worker to avoid conflicts.
CREATE TEMP TABLE temp_features_128987944695488 AS
WITH
  basic_windows AS (
    SELECT
      *,
      LAG(is_stationary, 1, is_stationary) OVER w AS prev_stationary,
      COALESCE(EXTRACT(EPOCH FROM (timestamp - LAG(timestamp, 1) OVER w)), 0) AS time_delta,
      AVG(load_weight) OVER (w ROWS BETWEEN 2 PRECEDING AND 2 FOLLOWING) AS smoothed_load_weight
    FROM temp_base
    WINDOW w AS (PARTITION BY device_id, device_date ORDER BY timestamp)
  ),
  duration_prep AS (
    SELECT
      *,
      SUM(CASE WHEN is_stationary != prev_stationary THEN 1 ELSE 0 END) OVER w AS stationary_block_id
    FROM basic_windows
    WINDOW w AS (PARTITION BY device_id, device_date ORDER BY timestamp)
  )
SELECT
  -- Select all final columns and calculate the remaining window functions
  timestamp, ingested_at, raw_event_hash_id, device_id, device_date, system_engaged,
  parking_brake_applied, current_position, current_speed, load_weight, state,
  software_state, prndl, extras, location_type, is_stationary, altitude,
  (altitude - LAG(altitude, 1) OVER w) AS altitude_rate_of_change,
  AVG(current_speed) OVER (w ROWS BETWEEN 2 PRECEDING AND 2 FOLLOWING) AS speed_rolling_avg_5s,
  smoothed_load_weight AS load_weight_smoothed,
  (smoothed_load_weight - LAG(smoothed_load_weight, 1) OVER w) AS load_weight_rate_of_change,
  (STDDEV(load_weight) OVER (PARTITION BY device_id) > 1000) AS has_reliable_payload,
  CASE
    WHEN is_stationary THEN SUM(time_delta) OVER (PARTITION BY device_id, device_date, stationary_block_id ORDER BY timestamp)
    ELSE 0
  END AS time_in_stationary_state
FROM duration_prep
WINDOW w AS (PARTITION BY device_id, device_date ORDER BY timestamp);]
[parameters: {'chunk_list': ('lake-605-10-0050_2025-08-11', 'lake-605-10-0050_2025-08-12', 'lake-605-8-0883_2025-07-30')}]
(Background on this error at: https://sqlalche.me/e/20/f405)
2025-09-03 06:23:00,276 - INFO - [Worker-128987969873600] Started, assigned 3 chunks.
2025-09-03 06:23:00,276 - ERROR - ❌ [Worker-128987598681792] Failed during SQL execution: (psycopg2.errors.WrongObjectType) op ANY/ALL (array) requires array on right side
LINE 17: WHERE t.device_date = ANY(('lake-605-8-0883_2025-07-31', 'la...
                             ^

[SQL: -- This is a template script executed by each parallel Python worker.
-- It processes an assigned subset of device_date chunks.

-- Step A: Create a temporary table with base features FOR THIS CHUNK ONLY.
-- This uses the two-step materialization pattern correctly to avoid nested window function errors.
CREATE TEMP TABLE temp_base AS
SELECT
    t.*,
    ST_Z(t.current_position::geometry) AS altitude,
    (t.current_speed < 0.5) AS is_stationary,
    CASE
        WHEN ST_Z(t.current_position::geometry) < 200 THEN 'Loading Zone A'
        WHEN ST_Z(t.current_position::geometry) > 250 THEN 'Dump Zone B'
        ELSE 'Haul Road / Other'
    END AS location_type
FROM "02_raw_telemetry_transformed" t
WHERE t.device_date = ANY(%(chunk_list)s);

-- Index the small temp table for the next step.
CREATE INDEX ON temp_base (device_id, device_date, timestamp);

-- Step B: Create the final results table FOR THIS CHUNK ONLY.
-- This table is uniquely named for each worker to avoid conflicts.
CREATE TEMP TABLE temp_features_128987598681792 AS
WITH
  basic_windows AS (
    SELECT
      *,
      LAG(is_stationary, 1, is_stationary) OVER w AS prev_stationary,
      COALESCE(EXTRACT(EPOCH FROM (timestamp - LAG(timestamp, 1) OVER w)), 0) AS time_delta,
      AVG(load_weight) OVER (w ROWS BETWEEN 2 PRECEDING AND 2 FOLLOWING) AS smoothed_load_weight
    FROM temp_base
    WINDOW w AS (PARTITION BY device_id, device_date ORDER BY timestamp)
  ),
  duration_prep AS (
    SELECT
      *,
      SUM(CASE WHEN is_stationary != prev_stationary THEN 1 ELSE 0 END) OVER w AS stationary_block_id
    FROM basic_windows
    WINDOW w AS (PARTITION BY device_id, device_date ORDER BY timestamp)
  )
SELECT
  -- Select all final columns and calculate the remaining window functions
  timestamp, ingested_at, raw_event_hash_id, device_id, device_date, system_engaged,
  parking_brake_applied, current_position, current_speed, load_weight, state,
  software_state, prndl, extras, location_type, is_stationary, altitude,
  (altitude - LAG(altitude, 1) OVER w) AS altitude_rate_of_change,
  AVG(current_speed) OVER (w ROWS BETWEEN 2 PRECEDING AND 2 FOLLOWING) AS speed_rolling_avg_5s,
  smoothed_load_weight AS load_weight_smoothed,
  (smoothed_load_weight - LAG(smoothed_load_weight, 1) OVER w) AS load_weight_rate_of_change,
  (STDDEV(load_weight) OVER (PARTITION BY device_id) > 1000) AS has_reliable_payload,
  CASE
    WHEN is_stationary THEN SUM(time_delta) OVER (PARTITION BY device_id, device_date, stationary_block_id ORDER BY timestamp)
    ELSE 0
  END AS time_in_stationary_state
FROM duration_prep
WINDOW w AS (PARTITION BY device_id, device_date ORDER BY timestamp);]
[parameters: {'chunk_list': ('lake-605-8-0883_2025-07-31', 'lake-605-8-0883_2025-08-01', 'lake-605-8-0883_2025-08-02')}]
(Background on this error at: https://sqlalche.me/e/20/f405)
2025-09-03 06:23:00,279 - ERROR - ❌ [Worker-128987978266304] Failed during SQL execution: (psycopg2.errors.WrongObjectType) op ANY/ALL (array) requires array on right side
LINE 17: WHERE t.device_date = ANY(('lake-605-10-0050_2025-07-30', 'l...
                             ^

[SQL: -- This is a template script executed by each parallel Python worker.
-- It processes an assigned subset of device_date chunks.

-- Step A: Create a temporary table with base features FOR THIS CHUNK ONLY.
-- This uses the two-step materialization pattern correctly to avoid nested window function errors.
CREATE TEMP TABLE temp_base AS
SELECT
    t.*,
    ST_Z(t.current_position::geometry) AS altitude,
    (t.current_speed < 0.5) AS is_stationary,
    CASE
        WHEN ST_Z(t.current_position::geometry) < 200 THEN 'Loading Zone A'
        WHEN ST_Z(t.current_position::geometry) > 250 THEN 'Dump Zone B'
        ELSE 'Haul Road / Other'
    END AS location_type
FROM "02_raw_telemetry_transformed" t
WHERE t.device_date = ANY(%(chunk_list)s);

-- Index the small temp table for the next step.
CREATE INDEX ON temp_base (device_id, device_date, timestamp);

-- Step B: Create the final results table FOR THIS CHUNK ONLY.
-- This table is uniquely named for each worker to avoid conflicts.
CREATE TEMP TABLE temp_features_128987978266304 AS
WITH
  basic_windows AS (
    SELECT
      *,
      LAG(is_stationary, 1, is_stationary) OVER w AS prev_stationary,
      COALESCE(EXTRACT(EPOCH FROM (timestamp - LAG(timestamp, 1) OVER w)), 0) AS time_delta,
      AVG(load_weight) OVER (w ROWS BETWEEN 2 PRECEDING AND 2 FOLLOWING) AS smoothed_load_weight
    FROM temp_base
    WINDOW w AS (PARTITION BY device_id, device_date ORDER BY timestamp)
  ),
  duration_prep AS (
    SELECT
      *,
      SUM(CASE WHEN is_stationary != prev_stationary THEN 1 ELSE 0 END) OVER w AS stationary_block_id
    FROM basic_windows
    WINDOW w AS (PARTITION BY device_id, device_date ORDER BY timestamp)
  )
SELECT
  -- Select all final columns and calculate the remaining window functions
  timestamp, ingested_at, raw_event_hash_id, device_id, device_date, system_engaged,
  parking_brake_applied, current_position, current_speed, load_weight, state,
  software_state, prndl, extras, location_type, is_stationary, altitude,
  (altitude - LAG(altitude, 1) OVER w) AS altitude_rate_of_change,
  AVG(current_speed) OVER (w ROWS BETWEEN 2 PRECEDING AND 2 FOLLOWING) AS speed_rolling_avg_5s,
  smoothed_load_weight AS load_weight_smoothed,
  (smoothed_load_weight - LAG(smoothed_load_weight, 1) OVER w) AS load_weight_rate_of_change,
  (STDDEV(load_weight) OVER (PARTITION BY device_id) > 1000) AS has_reliable_payload,
  CASE
    WHEN is_stationary THEN SUM(time_delta) OVER (PARTITION BY device_id, device_date, stationary_block_id ORDER BY timestamp)
    ELSE 0
  END AS time_in_stationary_state
FROM duration_prep
WINDOW w AS (PARTITION BY device_id, device_date ORDER BY timestamp);]
[parameters: {'chunk_list': ('lake-605-10-0050_2025-07-30', 'lake-605-10-0050_2025-07-31', 'lake-605-10-0050_2025-08-01')}]
(Background on this error at: https://sqlalche.me/e/20/f405)
2025-09-03 06:23:00,279 - ERROR - ❌ [Worker-128987581896384] Failed during SQL execution: (psycopg2.errors.WrongObjectType) op ANY/ALL (array) requires array on right side
LINE 17: WHERE t.device_date = ANY(('lake-605-8-0883_2025-08-06', 'la...
                             ^

[SQL: -- This is a template script executed by each parallel Python worker.
-- It processes an assigned subset of device_date chunks.

-- Step A: Create a temporary table with base features FOR THIS CHUNK ONLY.
-- This uses the two-step materialization pattern correctly to avoid nested window function errors.
CREATE TEMP TABLE temp_base AS
SELECT
    t.*,
    ST_Z(t.current_position::geometry) AS altitude,
    (t.current_speed < 0.5) AS is_stationary,
    CASE
        WHEN ST_Z(t.current_position::geometry) < 200 THEN 'Loading Zone A'
        WHEN ST_Z(t.current_position::geometry) > 250 THEN 'Dump Zone B'
        ELSE 'Haul Road / Other'
    END AS location_type
FROM "02_raw_telemetry_transformed" t
WHERE t.device_date = ANY(%(chunk_list)s);

-- Index the small temp table for the next step.
CREATE INDEX ON temp_base (device_id, device_date, timestamp);

-- Step B: Create the final results table FOR THIS CHUNK ONLY.
-- This table is uniquely named for each worker to avoid conflicts.
CREATE TEMP TABLE temp_features_128987581896384 AS
WITH
  basic_windows AS (
    SELECT
      *,
      LAG(is_stationary, 1, is_stationary) OVER w AS prev_stationary,
      COALESCE(EXTRACT(EPOCH FROM (timestamp - LAG(timestamp, 1) OVER w)), 0) AS time_delta,
      AVG(load_weight) OVER (w ROWS BETWEEN 2 PRECEDING AND 2 FOLLOWING) AS smoothed_load_weight
    FROM temp_base
    WINDOW w AS (PARTITION BY device_id, device_date ORDER BY timestamp)
  ),
  duration_prep AS (
    SELECT
      *,
      SUM(CASE WHEN is_stationary != prev_stationary THEN 1 ELSE 0 END) OVER w AS stationary_block_id
    FROM basic_windows
    WINDOW w AS (PARTITION BY device_id, device_date ORDER BY timestamp)
  )
SELECT
  -- Select all final columns and calculate the remaining window functions
  timestamp, ingested_at, raw_event_hash_id, device_id, device_date, system_engaged,
  parking_brake_applied, current_position, current_speed, load_weight, state,
  software_state, prndl, extras, location_type, is_stationary, altitude,
  (altitude - LAG(altitude, 1) OVER w) AS altitude_rate_of_change,
  AVG(current_speed) OVER (w ROWS BETWEEN 2 PRECEDING AND 2 FOLLOWING) AS speed_rolling_avg_5s,
  smoothed_load_weight AS load_weight_smoothed,
  (smoothed_load_weight - LAG(smoothed_load_weight, 1) OVER w) AS load_weight_rate_of_change,
  (STDDEV(load_weight) OVER (PARTITION BY device_id) > 1000) AS has_reliable_payload,
  CASE
    WHEN is_stationary THEN SUM(time_delta) OVER (PARTITION BY device_id, device_date, stationary_block_id ORDER BY timestamp)
    ELSE 0
  END AS time_in_stationary_state
FROM duration_prep
WINDOW w AS (PARTITION BY device_id, device_date ORDER BY timestamp);]
[parameters: {'chunk_list': ('lake-605-8-0883_2025-08-06', 'lake-605-8-0883_2025-08-07', 'lake-605-8-0883_2025-08-08')}]
(Background on this error at: https://sqlalche.me/e/20/f405)
2025-09-03 06:23:00,281 - ERROR - ❌ [Worker-128987590289088] Failed during SQL execution: (psycopg2.errors.WrongObjectType) op ANY/ALL (array) requires array on right side
LINE 17: WHERE t.device_date = ANY(('lake-605-8-0883_2025-08-03', 'la...
                             ^

[SQL: -- This is a template script executed by each parallel Python worker.
-- It processes an assigned subset of device_date chunks.

-- Step A: Create a temporary table with base features FOR THIS CHUNK ONLY.
-- This uses the two-step materialization pattern correctly to avoid nested window function errors.
CREATE TEMP TABLE temp_base AS
SELECT
    t.*,
    ST_Z(t.current_position::geometry) AS altitude,
    (t.current_speed < 0.5) AS is_stationary,
    CASE
        WHEN ST_Z(t.current_position::geometry) < 200 THEN 'Loading Zone A'
        WHEN ST_Z(t.current_position::geometry) > 250 THEN 'Dump Zone B'
        ELSE 'Haul Road / Other'
    END AS location_type
FROM "02_raw_telemetry_transformed" t
WHERE t.device_date = ANY(%(chunk_list)s);

-- Index the small temp table for the next step.
CREATE INDEX ON temp_base (device_id, device_date, timestamp);

-- Step B: Create the final results table FOR THIS CHUNK ONLY.
-- This table is uniquely named for each worker to avoid conflicts.
CREATE TEMP TABLE temp_features_128987590289088 AS
WITH
  basic_windows AS (
    SELECT
      *,
      LAG(is_stationary, 1, is_stationary) OVER w AS prev_stationary,
      COALESCE(EXTRACT(EPOCH FROM (timestamp - LAG(timestamp, 1) OVER w)), 0) AS time_delta,
      AVG(load_weight) OVER (w ROWS BETWEEN 2 PRECEDING AND 2 FOLLOWING) AS smoothed_load_weight
    FROM temp_base
    WINDOW w AS (PARTITION BY device_id, device_date ORDER BY timestamp)
  ),
  duration_prep AS (
    SELECT
      *,
      SUM(CASE WHEN is_stationary != prev_stationary THEN 1 ELSE 0 END) OVER w AS stationary_block_id
    FROM basic_windows
    WINDOW w AS (PARTITION BY device_id, device_date ORDER BY timestamp)
  )
SELECT
  -- Select all final columns and calculate the remaining window functions
  timestamp, ingested_at, raw_event_hash_id, device_id, device_date, system_engaged,
  parking_brake_applied, current_position, current_speed, load_weight, state,
  software_state, prndl, extras, location_type, is_stationary, altitude,
  (altitude - LAG(altitude, 1) OVER w) AS altitude_rate_of_change,
  AVG(current_speed) OVER (w ROWS BETWEEN 2 PRECEDING AND 2 FOLLOWING) AS speed_rolling_avg_5s,
  smoothed_load_weight AS load_weight_smoothed,
  (smoothed_load_weight - LAG(smoothed_load_weight, 1) OVER w) AS load_weight_rate_of_change,
  (STDDEV(load_weight) OVER (PARTITION BY device_id) > 1000) AS has_reliable_payload,
  CASE
    WHEN is_stationary THEN SUM(time_delta) OVER (PARTITION BY device_id, device_date, stationary_block_id ORDER BY timestamp)
    ELSE 0
  END AS time_in_stationary_state
FROM duration_prep
WINDOW w AS (PARTITION BY device_id, device_date ORDER BY timestamp);]
[parameters: {'chunk_list': ('lake-605-8-0883_2025-08-03', 'lake-605-8-0883_2025-08-04', 'lake-605-8-0883_2025-08-05')}]
(Background on this error at: https://sqlalche.me/e/20/f405)
2025-09-03 06:23:00,281 - ERROR - ❌ [Worker-128987953088192] Failed during SQL execution: (psycopg2.errors.WrongObjectType) op ANY/ALL (array) requires array on right side
LINE 17: WHERE t.device_date = ANY(('lake-605-10-0050_2025-08-08', 'l...
                             ^

[SQL: -- This is a template script executed by each parallel Python worker.
-- It processes an assigned subset of device_date chunks.

-- Step A: Create a temporary table with base features FOR THIS CHUNK ONLY.
-- This uses the two-step materialization pattern correctly to avoid nested window function errors.
CREATE TEMP TABLE temp_base AS
SELECT
    t.*,
    ST_Z(t.current_position::geometry) AS altitude,
    (t.current_speed < 0.5) AS is_stationary,
    CASE
        WHEN ST_Z(t.current_position::geometry) < 200 THEN 'Loading Zone A'
        WHEN ST_Z(t.current_position::geometry) > 250 THEN 'Dump Zone B'
        ELSE 'Haul Road / Other'
    END AS location_type
FROM "02_raw_telemetry_transformed" t
WHERE t.device_date = ANY(%(chunk_list)s);

-- Index the small temp table for the next step.
CREATE INDEX ON temp_base (device_id, device_date, timestamp);

-- Step B: Create the final results table FOR THIS CHUNK ONLY.
-- This table is uniquely named for each worker to avoid conflicts.
CREATE TEMP TABLE temp_features_128987953088192 AS
WITH
  basic_windows AS (
    SELECT
      *,
      LAG(is_stationary, 1, is_stationary) OVER w AS prev_stationary,
      COALESCE(EXTRACT(EPOCH FROM (timestamp - LAG(timestamp, 1) OVER w)), 0) AS time_delta,
      AVG(load_weight) OVER (w ROWS BETWEEN 2 PRECEDING AND 2 FOLLOWING) AS smoothed_load_weight
    FROM temp_base
    WINDOW w AS (PARTITION BY device_id, device_date ORDER BY timestamp)
  ),
  duration_prep AS (
    SELECT
      *,
      SUM(CASE WHEN is_stationary != prev_stationary THEN 1 ELSE 0 END) OVER w AS stationary_block_id
    FROM basic_windows
    WINDOW w AS (PARTITION BY device_id, device_date ORDER BY timestamp)
  )
SELECT
  -- Select all final columns and calculate the remaining window functions
  timestamp, ingested_at, raw_event_hash_id, device_id, device_date, system_engaged,
  parking_brake_applied, current_position, current_speed, load_weight, state,
  software_state, prndl, extras, location_type, is_stationary, altitude,
  (altitude - LAG(altitude, 1) OVER w) AS altitude_rate_of_change,
  AVG(current_speed) OVER (w ROWS BETWEEN 2 PRECEDING AND 2 FOLLOWING) AS speed_rolling_avg_5s,
  smoothed_load_weight AS load_weight_smoothed,
  (smoothed_load_weight - LAG(smoothed_load_weight, 1) OVER w) AS load_weight_rate_of_change,
  (STDDEV(load_weight) OVER (PARTITION BY device_id) > 1000) AS has_reliable_payload,
  CASE
    WHEN is_stationary THEN SUM(time_delta) OVER (PARTITION BY device_id, device_date, stationary_block_id ORDER BY timestamp)
    ELSE 0
  END AS time_in_stationary_state
FROM duration_prep
WINDOW w AS (PARTITION BY device_id, device_date ORDER BY timestamp);]
[parameters: {'chunk_list': ('lake-605-10-0050_2025-08-08', 'lake-605-10-0050_2025-08-09', 'lake-605-10-0050_2025-08-10')}]
(Background on this error at: https://sqlalche.me/e/20/f405)
2025-09-03 06:23:00,282 - ERROR - ❌ [Worker-128987565110976] Failed during SQL execution: (psycopg2.errors.WrongObjectType) op ANY/ALL (array) requires array on right side
LINE 17: WHERE t.device_date = ANY(('lake-605-8-0883_2025-08-12', 'la...
                             ^

[SQL: -- This is a template script executed by each parallel Python worker.
-- It processes an assigned subset of device_date chunks.

-- Step A: Create a temporary table with base features FOR THIS CHUNK ONLY.
-- This uses the two-step materialization pattern correctly to avoid nested window function errors.
CREATE TEMP TABLE temp_base AS
SELECT
    t.*,
    ST_Z(t.current_position::geometry) AS altitude,
    (t.current_speed < 0.5) AS is_stationary,
    CASE
        WHEN ST_Z(t.current_position::geometry) < 200 THEN 'Loading Zone A'
        WHEN ST_Z(t.current_position::geometry) > 250 THEN 'Dump Zone B'
        ELSE 'Haul Road / Other'
    END AS location_type
FROM "02_raw_telemetry_transformed" t
WHERE t.device_date = ANY(%(chunk_list)s);

-- Index the small temp table for the next step.
CREATE INDEX ON temp_base (device_id, device_date, timestamp);

-- Step B: Create the final results table FOR THIS CHUNK ONLY.
-- This table is uniquely named for each worker to avoid conflicts.
CREATE TEMP TABLE temp_features_128987565110976 AS
WITH
  basic_windows AS (
    SELECT
      *,
      LAG(is_stationary, 1, is_stationary) OVER w AS prev_stationary,
      COALESCE(EXTRACT(EPOCH FROM (timestamp - LAG(timestamp, 1) OVER w)), 0) AS time_delta,
      AVG(load_weight) OVER (w ROWS BETWEEN 2 PRECEDING AND 2 FOLLOWING) AS smoothed_load_weight
    FROM temp_base
    WINDOW w AS (PARTITION BY device_id, device_date ORDER BY timestamp)
  ),
  duration_prep AS (
    SELECT
      *,
      SUM(CASE WHEN is_stationary != prev_stationary THEN 1 ELSE 0 END) OVER w AS stationary_block_id
    FROM basic_windows
    WINDOW w AS (PARTITION BY device_id, device_date ORDER BY timestamp)
  )
SELECT
  -- Select all final columns and calculate the remaining window functions
  timestamp, ingested_at, raw_event_hash_id, device_id, device_date, system_engaged,
  parking_brake_applied, current_position, current_speed, load_weight, state,
  software_state, prndl, extras, location_type, is_stationary, altitude,
  (altitude - LAG(altitude, 1) OVER w) AS altitude_rate_of_change,
  AVG(current_speed) OVER (w ROWS BETWEEN 2 PRECEDING AND 2 FOLLOWING) AS speed_rolling_avg_5s,
  smoothed_load_weight AS load_weight_smoothed,
  (smoothed_load_weight - LAG(smoothed_load_weight, 1) OVER w) AS load_weight_rate_of_change,
  (STDDEV(load_weight) OVER (PARTITION BY device_id) > 1000) AS has_reliable_payload,
  CASE
    WHEN is_stationary THEN SUM(time_delta) OVER (PARTITION BY device_id, device_date, stationary_block_id ORDER BY timestamp)
    ELSE 0
  END AS time_in_stationary_state
FROM duration_prep
WINDOW w AS (PARTITION BY device_id, device_date ORDER BY timestamp);]
[parameters: {'chunk_list': ('lake-605-8-0883_2025-08-12', 'lake-605-8-0896_2025-07-30', 'lake-605-8-0896_2025-07-31')}]
(Background on this error at: https://sqlalche.me/e/20/f405)
2025-09-03 06:23:00,284 - ERROR - ❌ [Worker-128987961480896] Failed during SQL execution: (psycopg2.errors.WrongObjectType) op ANY/ALL (array) requires array on right side
LINE 17: WHERE t.device_date = ANY(('lake-605-10-0050_2025-08-05', 'l...
                             ^

[SQL: -- This is a template script executed by each parallel Python worker.
-- It processes an assigned subset of device_date chunks.

-- Step A: Create a temporary table with base features FOR THIS CHUNK ONLY.
-- This uses the two-step materialization pattern correctly to avoid nested window function errors.
CREATE TEMP TABLE temp_base AS
SELECT
    t.*,
    ST_Z(t.current_position::geometry) AS altitude,
    (t.current_speed < 0.5) AS is_stationary,
    CASE
        WHEN ST_Z(t.current_position::geometry) < 200 THEN 'Loading Zone A'
        WHEN ST_Z(t.current_position::geometry) > 250 THEN 'Dump Zone B'
        ELSE 'Haul Road / Other'
    END AS location_type
FROM "02_raw_telemetry_transformed" t
WHERE t.device_date = ANY(%(chunk_list)s);

-- Index the small temp table for the next step.
CREATE INDEX ON temp_base (device_id, device_date, timestamp);

-- Step B: Create the final results table FOR THIS CHUNK ONLY.
-- This table is uniquely named for each worker to avoid conflicts.
CREATE TEMP TABLE temp_features_128987961480896 AS
WITH
  basic_windows AS (
    SELECT
      *,
      LAG(is_stationary, 1, is_stationary) OVER w AS prev_stationary,
      COALESCE(EXTRACT(EPOCH FROM (timestamp - LAG(timestamp, 1) OVER w)), 0) AS time_delta,
      AVG(load_weight) OVER (w ROWS BETWEEN 2 PRECEDING AND 2 FOLLOWING) AS smoothed_load_weight
    FROM temp_base
    WINDOW w AS (PARTITION BY device_id, device_date ORDER BY timestamp)
  ),
  duration_prep AS (
    SELECT
      *,
      SUM(CASE WHEN is_stationary != prev_stationary THEN 1 ELSE 0 END) OVER w AS stationary_block_id
    FROM basic_windows
    WINDOW w AS (PARTITION BY device_id, device_date ORDER BY timestamp)
  )
SELECT
  -- Select all final columns and calculate the remaining window functions
  timestamp, ingested_at, raw_event_hash_id, device_id, device_date, system_engaged,
  parking_brake_applied, current_position, current_speed, load_weight, state,
  software_state, prndl, extras, location_type, is_stationary, altitude,
  (altitude - LAG(altitude, 1) OVER w) AS altitude_rate_of_change,
  AVG(current_speed) OVER (w ROWS BETWEEN 2 PRECEDING AND 2 FOLLOWING) AS speed_rolling_avg_5s,
  smoothed_load_weight AS load_weight_smoothed,
  (smoothed_load_weight - LAG(smoothed_load_weight, 1) OVER w) AS load_weight_rate_of_change,
  (STDDEV(load_weight) OVER (PARTITION BY device_id) > 1000) AS has_reliable_payload,
  CASE
    WHEN is_stationary THEN SUM(time_delta) OVER (PARTITION BY device_id, device_date, stationary_block_id ORDER BY timestamp)
    ELSE 0
  END AS time_in_stationary_state
FROM duration_prep
WINDOW w AS (PARTITION BY device_id, device_date ORDER BY timestamp);]
[parameters: {'chunk_list': ('lake-605-10-0050_2025-08-05', 'lake-605-10-0050_2025-08-06', 'lake-605-10-0050_2025-08-07')}]
(Background on this error at: https://sqlalche.me/e/20/f405)
2025-09-03 06:23:00,285 - INFO - [Worker-128986491369152] Started, assigned 3 chunks.
2025-09-03 06:23:00,286 - ERROR - ❌ [Worker-128987019847360] Failed during SQL execution: (psycopg2.errors.WrongObjectType) op ANY/ALL (array) requires array on right side
LINE 17: WHERE t.device_date = ANY(('lake-605-8-0898_2025-08-08', 'la...
                             ^

[SQL: -- This is a template script executed by each parallel Python worker.
-- It processes an assigned subset of device_date chunks.

-- Step A: Create a temporary table with base features FOR THIS CHUNK ONLY.
-- This uses the two-step materialization pattern correctly to avoid nested window function errors.
CREATE TEMP TABLE temp_base AS
SELECT
    t.*,
    ST_Z(t.current_position::geometry) AS altitude,
    (t.current_speed < 0.5) AS is_stationary,
    CASE
        WHEN ST_Z(t.current_position::geometry) < 200 THEN 'Loading Zone A'
        WHEN ST_Z(t.current_position::geometry) > 250 THEN 'Dump Zone B'
        ELSE 'Haul Road / Other'
    END AS location_type
FROM "02_raw_telemetry_transformed" t
WHERE t.device_date = ANY(%(chunk_list)s);

-- Index the small temp table for the next step.
CREATE INDEX ON temp_base (device_id, device_date, timestamp);

-- Step B: Create the final results table FOR THIS CHUNK ONLY.
-- This table is uniquely named for each worker to avoid conflicts.
CREATE TEMP TABLE temp_features_128987019847360 AS
WITH
  basic_windows AS (
    SELECT
      *,
      LAG(is_stationary, 1, is_stationary) OVER w AS prev_stationary,
      COALESCE(EXTRACT(EPOCH FROM (timestamp - LAG(timestamp, 1) OVER w)), 0) AS time_delta,
      AVG(load_weight) OVER (w ROWS BETWEEN 2 PRECEDING AND 2 FOLLOWING) AS smoothed_load_weight
    FROM temp_base
    WINDOW w AS (PARTITION BY device_id, device_date ORDER BY timestamp)
  ),
  duration_prep AS (
    SELECT
      *,
      SUM(CASE WHEN is_stationary != prev_stationary THEN 1 ELSE 0 END) OVER w AS stationary_block_id
    FROM basic_windows
    WINDOW w AS (PARTITION BY device_id, device_date ORDER BY timestamp)
  )
SELECT
  -- Select all final columns and calculate the remaining window functions
  timestamp, ingested_at, raw_event_hash_id, device_id, device_date, system_engaged,
  parking_brake_applied, current_position, current_speed, load_weight, state,
  software_state, prndl, extras, location_type, is_stationary, altitude,
  (altitude - LAG(altitude, 1) OVER w) AS altitude_rate_of_change,
  AVG(current_speed) OVER (w ROWS BETWEEN 2 PRECEDING AND 2 FOLLOWING) AS speed_rolling_avg_5s,
  smoothed_load_weight AS load_weight_smoothed,
  (smoothed_load_weight - LAG(smoothed_load_weight, 1) OVER w) AS load_weight_rate_of_change,
  (STDDEV(load_weight) OVER (PARTITION BY device_id) > 1000) AS has_reliable_payload,
  CASE
    WHEN is_stationary THEN SUM(time_delta) OVER (PARTITION BY device_id, device_date, stationary_block_id ORDER BY timestamp)
    ELSE 0
  END AS time_in_stationary_state
FROM duration_prep
WINDOW w AS (PARTITION BY device_id, device_date ORDER BY timestamp);]
[parameters: {'chunk_list': ('lake-605-8-0898_2025-08-08', 'lake-605-8-0898_2025-08-09', 'lake-605-8-0898_2025-08-10')}]
(Background on this error at: https://sqlalche.me/e/20/f405)
2025-09-03 06:23:00,286 - INFO - [Worker-128987581896384] Started, assigned 3 chunks.
2025-09-03 06:23:00,287 - ERROR - ❌ [Worker-128987548325568] Failed during SQL execution: (psycopg2.errors.WrongObjectType) op ANY/ALL (array) requires array on right side
LINE 17: WHERE t.device_date = ANY(('lake-605-8-0896_2025-08-04', 'la...
                             ^

[SQL: -- This is a template script executed by each parallel Python worker.
-- It processes an assigned subset of device_date chunks.

-- Step A: Create a temporary table with base features FOR THIS CHUNK ONLY.
-- This uses the two-step materialization pattern correctly to avoid nested window function errors.
CREATE TEMP TABLE temp_base AS
SELECT
    t.*,
    ST_Z(t.current_position::geometry) AS altitude,
    (t.current_speed < 0.5) AS is_stationary,
    CASE
        WHEN ST_Z(t.current_position::geometry) < 200 THEN 'Loading Zone A'
        WHEN ST_Z(t.current_position::geometry) > 250 THEN 'Dump Zone B'
        ELSE 'Haul Road / Other'
    END AS location_type
FROM "02_raw_telemetry_transformed" t
WHERE t.device_date = ANY(%(chunk_list)s);

-- Index the small temp table for the next step.
CREATE INDEX ON temp_base (device_id, device_date, timestamp);

-- Step B: Create the final results table FOR THIS CHUNK ONLY.
-- This table is uniquely named for each worker to avoid conflicts.
CREATE TEMP TABLE temp_features_128987548325568 AS
WITH
  basic_windows AS (
    SELECT
      *,
      LAG(is_stationary, 1, is_stationary) OVER w AS prev_stationary,
      COALESCE(EXTRACT(EPOCH FROM (timestamp - LAG(timestamp, 1) OVER w)), 0) AS time_delta,
      AVG(load_weight) OVER (w ROWS BETWEEN 2 PRECEDING AND 2 FOLLOWING) AS smoothed_load_weight
    FROM temp_base
    WINDOW w AS (PARTITION BY device_id, device_date ORDER BY timestamp)
  ),
  duration_prep AS (
    SELECT
      *,
      SUM(CASE WHEN is_stationary != prev_stationary THEN 1 ELSE 0 END) OVER w AS stationary_block_id
    FROM basic_windows
    WINDOW w AS (PARTITION BY device_id, device_date ORDER BY timestamp)
  )
SELECT
  -- Select all final columns and calculate the remaining window functions
  timestamp, ingested_at, raw_event_hash_id, device_id, device_date, system_engaged,
  parking_brake_applied, current_position, current_speed, load_weight, state,
  software_state, prndl, extras, location_type, is_stationary, altitude,
  (altitude - LAG(altitude, 1) OVER w) AS altitude_rate_of_change,
  AVG(current_speed) OVER (w ROWS BETWEEN 2 PRECEDING AND 2 FOLLOWING) AS speed_rolling_avg_5s,
  smoothed_load_weight AS load_weight_smoothed,
  (smoothed_load_weight - LAG(smoothed_load_weight, 1) OVER w) AS load_weight_rate_of_change,
  (STDDEV(load_weight) OVER (PARTITION BY device_id) > 1000) AS has_reliable_payload,
  CASE
    WHEN is_stationary THEN SUM(time_delta) OVER (PARTITION BY device_id, device_date, stationary_block_id ORDER BY timestamp)
    ELSE 0
  END AS time_in_stationary_state
FROM duration_prep
WINDOW w AS (PARTITION BY device_id, device_date ORDER BY timestamp);]
[parameters: {'chunk_list': ('lake-605-8-0896_2025-08-04', 'lake-605-8-0896_2025-08-05', 'lake-605-8-0896_2025-08-06')}]
(Background on this error at: https://sqlalche.me/e/20/f405)
2025-09-03 06:23:00,288 - INFO - [Worker-128987019847360] Started, assigned 3 chunks.
2025-09-03 06:23:00,288 - INFO - [Worker-128987590289088] Started, assigned 3 chunks.
2025-09-03 06:23:00,288 - INFO - [Worker-128987565110976] Started, assigned 3 chunks.
2025-09-03 06:23:00,291 - ERROR - ❌ [Worker-128987011454656] Failed during SQL execution: (psycopg2.errors.WrongObjectType) op ANY/ALL (array) requires array on right side
LINE 17: WHERE t.device_date = ANY(('lake-605-8-0898_2025-08-11', 'la...
                             ^

[SQL: -- This is a template script executed by each parallel Python worker.
-- It processes an assigned subset of device_date chunks.

-- Step A: Create a temporary table with base features FOR THIS CHUNK ONLY.
-- This uses the two-step materialization pattern correctly to avoid nested window function errors.
CREATE TEMP TABLE temp_base AS
SELECT
    t.*,
    ST_Z(t.current_position::geometry) AS altitude,
    (t.current_speed < 0.5) AS is_stationary,
    CASE
        WHEN ST_Z(t.current_position::geometry) < 200 THEN 'Loading Zone A'
        WHEN ST_Z(t.current_position::geometry) > 250 THEN 'Dump Zone B'
        ELSE 'Haul Road / Other'
    END AS location_type
FROM "02_raw_telemetry_transformed" t
WHERE t.device_date = ANY(%(chunk_list)s);

-- Index the small temp table for the next step.
CREATE INDEX ON temp_base (device_id, device_date, timestamp);

-- Step B: Create the final results table FOR THIS CHUNK ONLY.
-- This table is uniquely named for each worker to avoid conflicts.
CREATE TEMP TABLE temp_features_128987011454656 AS
WITH
  basic_windows AS (
    SELECT
      *,
      LAG(is_stationary, 1, is_stationary) OVER w AS prev_stationary,
      COALESCE(EXTRACT(EPOCH FROM (timestamp - LAG(timestamp, 1) OVER w)), 0) AS time_delta,
      AVG(load_weight) OVER (w ROWS BETWEEN 2 PRECEDING AND 2 FOLLOWING) AS smoothed_load_weight
    FROM temp_base
    WINDOW w AS (PARTITION BY device_id, device_date ORDER BY timestamp)
  ),
  duration_prep AS (
    SELECT
      *,
      SUM(CASE WHEN is_stationary != prev_stationary THEN 1 ELSE 0 END) OVER w AS stationary_block_id
    FROM basic_windows
    WINDOW w AS (PARTITION BY device_id, device_date ORDER BY timestamp)
  )
SELECT
  -- Select all final columns and calculate the remaining window functions
  timestamp, ingested_at, raw_event_hash_id, device_id, device_date, system_engaged,
  parking_brake_applied, current_position, current_speed, load_weight, state,
  software_state, prndl, extras, location_type, is_stationary, altitude,
  (altitude - LAG(altitude, 1) OVER w) AS altitude_rate_of_change,
  AVG(current_speed) OVER (w ROWS BETWEEN 2 PRECEDING AND 2 FOLLOWING) AS speed_rolling_avg_5s,
  smoothed_load_weight AS load_weight_smoothed,
  (smoothed_load_weight - LAG(smoothed_load_weight, 1) OVER w) AS load_weight_rate_of_change,
  (STDDEV(load_weight) OVER (PARTITION BY device_id) > 1000) AS has_reliable_payload,
  CASE
    WHEN is_stationary THEN SUM(time_delta) OVER (PARTITION BY device_id, device_date, stationary_block_id ORDER BY timestamp)
    ELSE 0
  END AS time_in_stationary_state
FROM duration_prep
WINDOW w AS (PARTITION BY device_id, device_date ORDER BY timestamp);]
[parameters: {'chunk_list': ('lake-605-8-0898_2025-08-11', 'lake-605-8-0898_2025-08-12', 'lake-605-8-0902_2025-07-30')}]
(Background on this error at: https://sqlalche.me/e/20/f405)
2025-09-03 06:23:00,293 - ERROR - ❌ [Worker-128987045025472] Failed during SQL execution: (psycopg2.errors.WrongObjectType) op ANY/ALL (array) requires array on right side
LINE 17: WHERE t.device_date = ANY(('lake-605-8-0898_2025-07-30', 'la...
                             ^

[SQL: -- This is a template script executed by each parallel Python worker.
-- It processes an assigned subset of device_date chunks.

-- Step A: Create a temporary table with base features FOR THIS CHUNK ONLY.
-- This uses the two-step materialization pattern correctly to avoid nested window function errors.
CREATE TEMP TABLE temp_base AS
SELECT
    t.*,
    ST_Z(t.current_position::geometry) AS altitude,
    (t.current_speed < 0.5) AS is_stationary,
    CASE
        WHEN ST_Z(t.current_position::geometry) < 200 THEN 'Loading Zone A'
        WHEN ST_Z(t.current_position::geometry) > 250 THEN 'Dump Zone B'
        ELSE 'Haul Road / Other'
    END AS location_type
FROM "02_raw_telemetry_transformed" t
WHERE t.device_date = ANY(%(chunk_list)s);

-- Index the small temp table for the next step.
CREATE INDEX ON temp_base (device_id, device_date, timestamp);

-- Step B: Create the final results table FOR THIS CHUNK ONLY.
-- This table is uniquely named for each worker to avoid conflicts.
CREATE TEMP TABLE temp_features_128987045025472 AS
WITH
  basic_windows AS (
    SELECT
      *,
      LAG(is_stationary, 1, is_stationary) OVER w AS prev_stationary,
      COALESCE(EXTRACT(EPOCH FROM (timestamp - LAG(timestamp, 1) OVER w)), 0) AS time_delta,
      AVG(load_weight) OVER (w ROWS BETWEEN 2 PRECEDING AND 2 FOLLOWING) AS smoothed_load_weight
    FROM temp_base
    WINDOW w AS (PARTITION BY device_id, device_date ORDER BY timestamp)
  ),
  duration_prep AS (
    SELECT
      *,
      SUM(CASE WHEN is_stationary != prev_stationary THEN 1 ELSE 0 END) OVER w AS stationary_block_id
    FROM basic_windows
    WINDOW w AS (PARTITION BY device_id, device_date ORDER BY timestamp)
  )
SELECT
  -- Select all final columns and calculate the remaining window functions
  timestamp, ingested_at, raw_event_hash_id, device_id, device_date, system_engaged,
  parking_brake_applied, current_position, current_speed, load_weight, state,
  software_state, prndl, extras, location_type, is_stationary, altitude,
  (altitude - LAG(altitude, 1) OVER w) AS altitude_rate_of_change,
  AVG(current_speed) OVER (w ROWS BETWEEN 2 PRECEDING AND 2 FOLLOWING) AS speed_rolling_avg_5s,
  smoothed_load_weight AS load_weight_smoothed,
  (smoothed_load_weight - LAG(smoothed_load_weight, 1) OVER w) AS load_weight_rate_of_change,
  (STDDEV(load_weight) OVER (PARTITION BY device_id) > 1000) AS has_reliable_payload,
  CASE
    WHEN is_stationary THEN SUM(time_delta) OVER (PARTITION BY device_id, device_date, stationary_block_id ORDER BY timestamp)
    ELSE 0
  END AS time_in_stationary_state
FROM duration_prep
WINDOW w AS (PARTITION BY device_id, device_date ORDER BY timestamp);]
[parameters: {'chunk_list': ('lake-605-8-0898_2025-07-30', 'lake-605-8-0898_2025-07-31', 'lake-605-8-0898_2025-08-01')}]
(Background on this error at: https://sqlalche.me/e/20/f405)
2025-09-03 06:23:00,295 - ERROR - ❌ [Worker-128987061810880] Failed during SQL execution: (psycopg2.errors.WrongObjectType) op ANY/ALL (array) requires array on right side
LINE 17: WHERE t.device_date = ANY(('lake-605-8-0896_2025-08-07', 'la...
                             ^

[SQL: -- This is a template script executed by each parallel Python worker.
-- It processes an assigned subset of device_date chunks.

-- Step A: Create a temporary table with base features FOR THIS CHUNK ONLY.
-- This uses the two-step materialization pattern correctly to avoid nested window function errors.
CREATE TEMP TABLE temp_base AS
SELECT
    t.*,
    ST_Z(t.current_position::geometry) AS altitude,
    (t.current_speed < 0.5) AS is_stationary,
    CASE
        WHEN ST_Z(t.current_position::geometry) < 200 THEN 'Loading Zone A'
        WHEN ST_Z(t.current_position::geometry) > 250 THEN 'Dump Zone B'
        ELSE 'Haul Road / Other'
    END AS location_type
FROM "02_raw_telemetry_transformed" t
WHERE t.device_date = ANY(%(chunk_list)s);

-- Index the small temp table for the next step.
CREATE INDEX ON temp_base (device_id, device_date, timestamp);

-- Step B: Create the final results table FOR THIS CHUNK ONLY.
-- This table is uniquely named for each worker to avoid conflicts.
CREATE TEMP TABLE temp_features_128987061810880 AS
WITH
  basic_windows AS (
    SELECT
      *,
      LAG(is_stationary, 1, is_stationary) OVER w AS prev_stationary,
      COALESCE(EXTRACT(EPOCH FROM (timestamp - LAG(timestamp, 1) OVER w)), 0) AS time_delta,
      AVG(load_weight) OVER (w ROWS BETWEEN 2 PRECEDING AND 2 FOLLOWING) AS smoothed_load_weight
    FROM temp_base
    WINDOW w AS (PARTITION BY device_id, device_date ORDER BY timestamp)
  ),
  duration_prep AS (
    SELECT
      *,
      SUM(CASE WHEN is_stationary != prev_stationary THEN 1 ELSE 0 END) OVER w AS stationary_block_id
    FROM basic_windows
    WINDOW w AS (PARTITION BY device_id, device_date ORDER BY timestamp)
  )
SELECT
  -- Select all final columns and calculate the remaining window functions
  timestamp, ingested_at, raw_event_hash_id, device_id, device_date, system_engaged,
  parking_brake_applied, current_position, current_speed, load_weight, state,
  software_state, prndl, extras, location_type, is_stationary, altitude,
  (altitude - LAG(altitude, 1) OVER w) AS altitude_rate_of_change,
  AVG(current_speed) OVER (w ROWS BETWEEN 2 PRECEDING AND 2 FOLLOWING) AS speed_rolling_avg_5s,
  smoothed_load_weight AS load_weight_smoothed,
  (smoothed_load_weight - LAG(smoothed_load_weight, 1) OVER w) AS load_weight_rate_of_change,
  (STDDEV(load_weight) OVER (PARTITION BY device_id) > 1000) AS has_reliable_payload,
  CASE
    WHEN is_stationary THEN SUM(time_delta) OVER (PARTITION BY device_id, device_date, stationary_block_id ORDER BY timestamp)
    ELSE 0
  END AS time_in_stationary_state
FROM duration_prep
WINDOW w AS (PARTITION BY device_id, device_date ORDER BY timestamp);]
[parameters: {'chunk_list': ('lake-605-8-0896_2025-08-07', 'lake-605-8-0896_2025-08-08', 'lake-605-8-0896_2025-08-09')}]
(Background on this error at: https://sqlalche.me/e/20/f405)
2025-09-03 06:23:00,297 - ERROR - ❌ [Worker-128986499761856] Failed during SQL execution: (psycopg2.errors.WrongObjectType) op ANY/ALL (array) requires array on right side
LINE 17: WHERE t.device_date = ANY(('lake-605-8-0902_2025-08-09', 'la...
                             ^

[SQL: -- This is a template script executed by each parallel Python worker.
-- It processes an assigned subset of device_date chunks.

-- Step A: Create a temporary table with base features FOR THIS CHUNK ONLY.
-- This uses the two-step materialization pattern correctly to avoid nested window function errors.
CREATE TEMP TABLE temp_base AS
SELECT
    t.*,
    ST_Z(t.current_position::geometry) AS altitude,
    (t.current_speed < 0.5) AS is_stationary,
    CASE
        WHEN ST_Z(t.current_position::geometry) < 200 THEN 'Loading Zone A'
        WHEN ST_Z(t.current_position::geometry) > 250 THEN 'Dump Zone B'
        ELSE 'Haul Road / Other'
    END AS location_type
FROM "02_raw_telemetry_transformed" t
WHERE t.device_date = ANY(%(chunk_list)s);

-- Index the small temp table for the next step.
CREATE INDEX ON temp_base (device_id, device_date, timestamp);

-- Step B: Create the final results table FOR THIS CHUNK ONLY.
-- This table is uniquely named for each worker to avoid conflicts.
CREATE TEMP TABLE temp_features_128986499761856 AS
WITH
  basic_windows AS (
    SELECT
      *,
      LAG(is_stationary, 1, is_stationary) OVER w AS prev_stationary,
      COALESCE(EXTRACT(EPOCH FROM (timestamp - LAG(timestamp, 1) OVER w)), 0) AS time_delta,
      AVG(load_weight) OVER (w ROWS BETWEEN 2 PRECEDING AND 2 FOLLOWING) AS smoothed_load_weight
    FROM temp_base
    WINDOW w AS (PARTITION BY device_id, device_date ORDER BY timestamp)
  ),
  duration_prep AS (
    SELECT
      *,
      SUM(CASE WHEN is_stationary != prev_stationary THEN 1 ELSE 0 END) OVER w AS stationary_block_id
    FROM basic_windows
    WINDOW w AS (PARTITION BY device_id, device_date ORDER BY timestamp)
  )
SELECT
  -- Select all final columns and calculate the remaining window functions
  timestamp, ingested_at, raw_event_hash_id, device_id, device_date, system_engaged,
  parking_brake_applied, current_position, current_speed, load_weight, state,
  software_state, prndl, extras, location_type, is_stationary, altitude,
  (altitude - LAG(altitude, 1) OVER w) AS altitude_rate_of_change,
  AVG(current_speed) OVER (w ROWS BETWEEN 2 PRECEDING AND 2 FOLLOWING) AS speed_rolling_avg_5s,
  smoothed_load_weight AS load_weight_smoothed,
  (smoothed_load_weight - LAG(smoothed_load_weight, 1) OVER w) AS load_weight_rate_of_change,
  (STDDEV(load_weight) OVER (PARTITION BY device_id) > 1000) AS has_reliable_payload,
  CASE
    WHEN is_stationary THEN SUM(time_delta) OVER (PARTITION BY device_id, device_date, stationary_block_id ORDER BY timestamp)
    ELSE 0
  END AS time_in_stationary_state
FROM duration_prep
WINDOW w AS (PARTITION BY device_id, device_date ORDER BY timestamp);]
[parameters: {'chunk_list': ('lake-605-8-0902_2025-08-09', 'lake-605-8-0902_2025-08-10', 'lake-605-8-0902_2025-08-12')}]
(Background on this error at: https://sqlalche.me/e/20/f405)
2025-09-03 06:23:00,297 - ERROR - ❌ [Worker-128987573503680] Failed during SQL execution: (psycopg2.errors.WrongObjectType) op ANY/ALL (array) requires array on right side
LINE 17: WHERE t.device_date = ANY(('lake-605-8-0883_2025-08-09', 'la...
                             ^

[SQL: -- This is a template script executed by each parallel Python worker.
-- It processes an assigned subset of device_date chunks.

-- Step A: Create a temporary table with base features FOR THIS CHUNK ONLY.
-- This uses the two-step materialization pattern correctly to avoid nested window function errors.
CREATE TEMP TABLE temp_base AS
SELECT
    t.*,
    ST_Z(t.current_position::geometry) AS altitude,
    (t.current_speed < 0.5) AS is_stationary,
    CASE
        WHEN ST_Z(t.current_position::geometry) < 200 THEN 'Loading Zone A'
        WHEN ST_Z(t.current_position::geometry) > 250 THEN 'Dump Zone B'
        ELSE 'Haul Road / Other'
    END AS location_type
FROM "02_raw_telemetry_transformed" t
WHERE t.device_date = ANY(%(chunk_list)s);

-- Index the small temp table for the next step.
CREATE INDEX ON temp_base (device_id, device_date, timestamp);

-- Step B: Create the final results table FOR THIS CHUNK ONLY.
-- This table is uniquely named for each worker to avoid conflicts.
CREATE TEMP TABLE temp_features_128987573503680 AS
WITH
  basic_windows AS (
    SELECT
      *,
      LAG(is_stationary, 1, is_stationary) OVER w AS prev_stationary,
      COALESCE(EXTRACT(EPOCH FROM (timestamp - LAG(timestamp, 1) OVER w)), 0) AS time_delta,
      AVG(load_weight) OVER (w ROWS BETWEEN 2 PRECEDING AND 2 FOLLOWING) AS smoothed_load_weight
    FROM temp_base
    WINDOW w AS (PARTITION BY device_id, device_date ORDER BY timestamp)
  ),
  duration_prep AS (
    SELECT
      *,
      SUM(CASE WHEN is_stationary != prev_stationary THEN 1 ELSE 0 END) OVER w AS stationary_block_id
    FROM basic_windows
    WINDOW w AS (PARTITION BY device_id, device_date ORDER BY timestamp)
  )
SELECT
  -- Select all final columns and calculate the remaining window functions
  timestamp, ingested_at, raw_event_hash_id, device_id, device_date, system_engaged,
  parking_brake_applied, current_position, current_speed, load_weight, state,
  software_state, prndl, extras, location_type, is_stationary, altitude,
  (altitude - LAG(altitude, 1) OVER w) AS altitude_rate_of_change,
  AVG(current_speed) OVER (w ROWS BETWEEN 2 PRECEDING AND 2 FOLLOWING) AS speed_rolling_avg_5s,
  smoothed_load_weight AS load_weight_smoothed,
  (smoothed_load_weight - LAG(smoothed_load_weight, 1) OVER w) AS load_weight_rate_of_change,
  (STDDEV(load_weight) OVER (PARTITION BY device_id) > 1000) AS has_reliable_payload,
  CASE
    WHEN is_stationary THEN SUM(time_delta) OVER (PARTITION BY device_id, device_date, stationary_block_id ORDER BY timestamp)
    ELSE 0
  END AS time_in_stationary_state
FROM duration_prep
WINDOW w AS (PARTITION BY device_id, device_date ORDER BY timestamp);]
[parameters: {'chunk_list': ('lake-605-8-0883_2025-08-09', 'lake-605-8-0883_2025-08-10', 'lake-605-8-0883_2025-08-11')}]
(Background on this error at: https://sqlalche.me/e/20/f405)
2025-09-03 06:23:00,300 - ERROR - ❌ [Worker-128986524939968] Failed during SQL execution: (psycopg2.errors.WrongObjectType) op ANY/ALL (array) requires array on right side
LINE 17: WHERE t.device_date = ANY(('lake-605-8-0902_2025-07-31', 'la...
                             ^

[SQL: -- This is a template script executed by each parallel Python worker.
-- It processes an assigned subset of device_date chunks.

-- Step A: Create a temporary table with base features FOR THIS CHUNK ONLY.
-- This uses the two-step materialization pattern correctly to avoid nested window function errors.
CREATE TEMP TABLE temp_base AS
SELECT
    t.*,
    ST_Z(t.current_position::geometry) AS altitude,
    (t.current_speed < 0.5) AS is_stationary,
    CASE
        WHEN ST_Z(t.current_position::geometry) < 200 THEN 'Loading Zone A'
        WHEN ST_Z(t.current_position::geometry) > 250 THEN 'Dump Zone B'
        ELSE 'Haul Road / Other'
    END AS location_type
FROM "02_raw_telemetry_transformed" t
WHERE t.device_date = ANY(%(chunk_list)s);

-- Index the small temp table for the next step.
CREATE INDEX ON temp_base (device_id, device_date, timestamp);

-- Step B: Create the final results table FOR THIS CHUNK ONLY.
-- This table is uniquely named for each worker to avoid conflicts.
CREATE TEMP TABLE temp_features_128986524939968 AS
WITH
  basic_windows AS (
    SELECT
      *,
      LAG(is_stationary, 1, is_stationary) OVER w AS prev_stationary,
      COALESCE(EXTRACT(EPOCH FROM (timestamp - LAG(timestamp, 1) OVER w)), 0) AS time_delta,
      AVG(load_weight) OVER (w ROWS BETWEEN 2 PRECEDING AND 2 FOLLOWING) AS smoothed_load_weight
    FROM temp_base
    WINDOW w AS (PARTITION BY device_id, device_date ORDER BY timestamp)
  ),
  duration_prep AS (
    SELECT
      *,
      SUM(CASE WHEN is_stationary != prev_stationary THEN 1 ELSE 0 END) OVER w AS stationary_block_id
    FROM basic_windows
    WINDOW w AS (PARTITION BY device_id, device_date ORDER BY timestamp)
  )
SELECT
  -- Select all final columns and calculate the remaining window functions
  timestamp, ingested_at, raw_event_hash_id, device_id, device_date, system_engaged,
  parking_brake_applied, current_position, current_speed, load_weight, state,
  software_state, prndl, extras, location_type, is_stationary, altitude,
  (altitude - LAG(altitude, 1) OVER w) AS altitude_rate_of_change,
  AVG(current_speed) OVER (w ROWS BETWEEN 2 PRECEDING AND 2 FOLLOWING) AS speed_rolling_avg_5s,
  smoothed_load_weight AS load_weight_smoothed,
  (smoothed_load_weight - LAG(smoothed_load_weight, 1) OVER w) AS load_weight_rate_of_change,
  (STDDEV(load_weight) OVER (PARTITION BY device_id) > 1000) AS has_reliable_payload,
  CASE
    WHEN is_stationary THEN SUM(time_delta) OVER (PARTITION BY device_id, device_date, stationary_block_id ORDER BY timestamp)
    ELSE 0
  END AS time_in_stationary_state
FROM duration_prep
WINDOW w AS (PARTITION BY device_id, device_date ORDER BY timestamp);]
[parameters: {'chunk_list': ('lake-605-8-0902_2025-07-31', 'lake-605-8-0902_2025-08-01', 'lake-605-8-0902_2025-08-02')}]
(Background on this error at: https://sqlalche.me/e/20/f405)
2025-09-03 06:23:00,301 - ERROR - ❌ [Worker-128987028240064] Failed during SQL execution: (psycopg2.errors.WrongObjectType) op ANY/ALL (array) requires array on right side
LINE 17: WHERE t.device_date = ANY(('lake-605-8-0898_2025-08-05', 'la...
                             ^

[SQL: -- This is a template script executed by each parallel Python worker.
-- It processes an assigned subset of device_date chunks.

-- Step A: Create a temporary table with base features FOR THIS CHUNK ONLY.
-- This uses the two-step materialization pattern correctly to avoid nested window function errors.
CREATE TEMP TABLE temp_base AS
SELECT
    t.*,
    ST_Z(t.current_position::geometry) AS altitude,
    (t.current_speed < 0.5) AS is_stationary,
    CASE
        WHEN ST_Z(t.current_position::geometry) < 200 THEN 'Loading Zone A'
        WHEN ST_Z(t.current_position::geometry) > 250 THEN 'Dump Zone B'
        ELSE 'Haul Road / Other'
    END AS location_type
FROM "02_raw_telemetry_transformed" t
WHERE t.device_date = ANY(%(chunk_list)s);

-- Index the small temp table for the next step.
CREATE INDEX ON temp_base (device_id, device_date, timestamp);

-- Step B: Create the final results table FOR THIS CHUNK ONLY.
-- This table is uniquely named for each worker to avoid conflicts.
CREATE TEMP TABLE temp_features_128987028240064 AS
WITH
  basic_windows AS (
    SELECT
      *,
      LAG(is_stationary, 1, is_stationary) OVER w AS prev_stationary,
      COALESCE(EXTRACT(EPOCH FROM (timestamp - LAG(timestamp, 1) OVER w)), 0) AS time_delta,
      AVG(load_weight) OVER (w ROWS BETWEEN 2 PRECEDING AND 2 FOLLOWING) AS smoothed_load_weight
    FROM temp_base
    WINDOW w AS (PARTITION BY device_id, device_date ORDER BY timestamp)
  ),
  duration_prep AS (
    SELECT
      *,
      SUM(CASE WHEN is_stationary != prev_stationary THEN 1 ELSE 0 END) OVER w AS stationary_block_id
    FROM basic_windows
    WINDOW w AS (PARTITION BY device_id, device_date ORDER BY timestamp)
  )
SELECT
  -- Select all final columns and calculate the remaining window functions
  timestamp, ingested_at, raw_event_hash_id, device_id, device_date, system_engaged,
  parking_brake_applied, current_position, current_speed, load_weight, state,
  software_state, prndl, extras, location_type, is_stationary, altitude,
  (altitude - LAG(altitude, 1) OVER w) AS altitude_rate_of_change,
  AVG(current_speed) OVER (w ROWS BETWEEN 2 PRECEDING AND 2 FOLLOWING) AS speed_rolling_avg_5s,
  smoothed_load_weight AS load_weight_smoothed,
  (smoothed_load_weight - LAG(smoothed_load_weight, 1) OVER w) AS load_weight_rate_of_change,
  (STDDEV(load_weight) OVER (PARTITION BY device_id) > 1000) AS has_reliable_payload,
  CASE
    WHEN is_stationary THEN SUM(time_delta) OVER (PARTITION BY device_id, device_date, stationary_block_id ORDER BY timestamp)
    ELSE 0
  END AS time_in_stationary_state
FROM duration_prep
WINDOW w AS (PARTITION BY device_id, device_date ORDER BY timestamp);]
[parameters: {'chunk_list': ('lake-605-8-0898_2025-08-05', 'lake-605-8-0898_2025-08-06', 'lake-605-8-0898_2025-08-07')}]
(Background on this error at: https://sqlalche.me/e/20/f405)
2025-09-03 06:23:00,302 - ERROR - ❌ [Worker-128987053418176] Failed during SQL execution: (psycopg2.errors.WrongObjectType) op ANY/ALL (array) requires array on right side
LINE 17: WHERE t.device_date = ANY(('lake-605-8-0896_2025-08-10', 'la...
                             ^

[SQL: -- This is a template script executed by each parallel Python worker.
-- It processes an assigned subset of device_date chunks.

-- Step A: Create a temporary table with base features FOR THIS CHUNK ONLY.
-- This uses the two-step materialization pattern correctly to avoid nested window function errors.
CREATE TEMP TABLE temp_base AS
SELECT
    t.*,
    ST_Z(t.current_position::geometry) AS altitude,
    (t.current_speed < 0.5) AS is_stationary,
    CASE
        WHEN ST_Z(t.current_position::geometry) < 200 THEN 'Loading Zone A'
        WHEN ST_Z(t.current_position::geometry) > 250 THEN 'Dump Zone B'
        ELSE 'Haul Road / Other'
    END AS location_type
FROM "02_raw_telemetry_transformed" t
WHERE t.device_date = ANY(%(chunk_list)s);

-- Index the small temp table for the next step.
CREATE INDEX ON temp_base (device_id, device_date, timestamp);

-- Step B: Create the final results table FOR THIS CHUNK ONLY.
-- This table is uniquely named for each worker to avoid conflicts.
CREATE TEMP TABLE temp_features_128987053418176 AS
WITH
  basic_windows AS (
    SELECT
      *,
      LAG(is_stationary, 1, is_stationary) OVER w AS prev_stationary,
      COALESCE(EXTRACT(EPOCH FROM (timestamp - LAG(timestamp, 1) OVER w)), 0) AS time_delta,
      AVG(load_weight) OVER (w ROWS BETWEEN 2 PRECEDING AND 2 FOLLOWING) AS smoothed_load_weight
    FROM temp_base
    WINDOW w AS (PARTITION BY device_id, device_date ORDER BY timestamp)
  ),
  duration_prep AS (
    SELECT
      *,
      SUM(CASE WHEN is_stationary != prev_stationary THEN 1 ELSE 0 END) OVER w AS stationary_block_id
    FROM basic_windows
    WINDOW w AS (PARTITION BY device_id, device_date ORDER BY timestamp)
  )
SELECT
  -- Select all final columns and calculate the remaining window functions
  timestamp, ingested_at, raw_event_hash_id, device_id, device_date, system_engaged,
  parking_brake_applied, current_position, current_speed, load_weight, state,
  software_state, prndl, extras, location_type, is_stationary, altitude,
  (altitude - LAG(altitude, 1) OVER w) AS altitude_rate_of_change,
  AVG(current_speed) OVER (w ROWS BETWEEN 2 PRECEDING AND 2 FOLLOWING) AS speed_rolling_avg_5s,
  smoothed_load_weight AS load_weight_smoothed,
  (smoothed_load_weight - LAG(smoothed_load_weight, 1) OVER w) AS load_weight_rate_of_change,
  (STDDEV(load_weight) OVER (PARTITION BY device_id) > 1000) AS has_reliable_payload,
  CASE
    WHEN is_stationary THEN SUM(time_delta) OVER (PARTITION BY device_id, device_date, stationary_block_id ORDER BY timestamp)
    ELSE 0
  END AS time_in_stationary_state
FROM duration_prep
WINDOW w AS (PARTITION BY device_id, device_date ORDER BY timestamp);]
[parameters: {'chunk_list': ('lake-605-8-0896_2025-08-10', 'lake-605-8-0896_2025-08-11', 'lake-605-8-0896_2025-08-12')}]
(Background on this error at: https://sqlalche.me/e/20/f405)
2025-09-03 06:23:00,302 - ERROR - ❌ [Worker-128987556718272] Failed during SQL execution: (psycopg2.errors.WrongObjectType) op ANY/ALL (array) requires array on right side
LINE 17: WHERE t.device_date = ANY(('lake-605-8-0896_2025-08-01', 'la...
                             ^

[SQL: -- This is a template script executed by each parallel Python worker.
-- It processes an assigned subset of device_date chunks.

-- Step A: Create a temporary table with base features FOR THIS CHUNK ONLY.
-- This uses the two-step materialization pattern correctly to avoid nested window function errors.
CREATE TEMP TABLE temp_base AS
SELECT
    t.*,
    ST_Z(t.current_position::geometry) AS altitude,
    (t.current_speed < 0.5) AS is_stationary,
    CASE
        WHEN ST_Z(t.current_position::geometry) < 200 THEN 'Loading Zone A'
        WHEN ST_Z(t.current_position::geometry) > 250 THEN 'Dump Zone B'
        ELSE 'Haul Road / Other'
    END AS location_type
FROM "02_raw_telemetry_transformed" t
WHERE t.device_date = ANY(%(chunk_list)s);

-- Index the small temp table for the next step.
CREATE INDEX ON temp_base (device_id, device_date, timestamp);

-- Step B: Create the final results table FOR THIS CHUNK ONLY.
-- This table is uniquely named for each worker to avoid conflicts.
CREATE TEMP TABLE temp_features_128987556718272 AS
WITH
  basic_windows AS (
    SELECT
      *,
      LAG(is_stationary, 1, is_stationary) OVER w AS prev_stationary,
      COALESCE(EXTRACT(EPOCH FROM (timestamp - LAG(timestamp, 1) OVER w)), 0) AS time_delta,
      AVG(load_weight) OVER (w ROWS BETWEEN 2 PRECEDING AND 2 FOLLOWING) AS smoothed_load_weight
    FROM temp_base
    WINDOW w AS (PARTITION BY device_id, device_date ORDER BY timestamp)
  ),
  duration_prep AS (
    SELECT
      *,
      SUM(CASE WHEN is_stationary != prev_stationary THEN 1 ELSE 0 END) OVER w AS stationary_block_id
    FROM basic_windows
    WINDOW w AS (PARTITION BY device_id, device_date ORDER BY timestamp)
  )
SELECT
  -- Select all final columns and calculate the remaining window functions
  timestamp, ingested_at, raw_event_hash_id, device_id, device_date, system_engaged,
  parking_brake_applied, current_position, current_speed, load_weight, state,
  software_state, prndl, extras, location_type, is_stationary, altitude,
  (altitude - LAG(altitude, 1) OVER w) AS altitude_rate_of_change,
  AVG(current_speed) OVER (w ROWS BETWEEN 2 PRECEDING AND 2 FOLLOWING) AS speed_rolling_avg_5s,
  smoothed_load_weight AS load_weight_smoothed,
  (smoothed_load_weight - LAG(smoothed_load_weight, 1) OVER w) AS load_weight_rate_of_change,
  (STDDEV(load_weight) OVER (PARTITION BY device_id) > 1000) AS has_reliable_payload,
  CASE
    WHEN is_stationary THEN SUM(time_delta) OVER (PARTITION BY device_id, device_date, stationary_block_id ORDER BY timestamp)
    ELSE 0
  END AS time_in_stationary_state
FROM duration_prep
WINDOW w AS (PARTITION BY device_id, device_date ORDER BY timestamp);]
[parameters: {'chunk_list': ('lake-605-8-0896_2025-08-01', 'lake-605-8-0896_2025-08-02', 'lake-605-8-0896_2025-08-03')}]
(Background on this error at: https://sqlalche.me/e/20/f405)
2025-09-03 06:23:00,304 - ERROR - ❌ [Worker-128987036632768] Failed during SQL execution: (psycopg2.errors.WrongObjectType) op ANY/ALL (array) requires array on right side
LINE 17: WHERE t.device_date = ANY(('lake-605-8-0898_2025-08-02', 'la...
                             ^

[SQL: -- This is a template script executed by each parallel Python worker.
-- It processes an assigned subset of device_date chunks.

-- Step A: Create a temporary table with base features FOR THIS CHUNK ONLY.
-- This uses the two-step materialization pattern correctly to avoid nested window function errors.
CREATE TEMP TABLE temp_base AS
SELECT
    t.*,
    ST_Z(t.current_position::geometry) AS altitude,
    (t.current_speed < 0.5) AS is_stationary,
    CASE
        WHEN ST_Z(t.current_position::geometry) < 200 THEN 'Loading Zone A'
        WHEN ST_Z(t.current_position::geometry) > 250 THEN 'Dump Zone B'
        ELSE 'Haul Road / Other'
    END AS location_type
FROM "02_raw_telemetry_transformed" t
WHERE t.device_date = ANY(%(chunk_list)s);

-- Index the small temp table for the next step.
CREATE INDEX ON temp_base (device_id, device_date, timestamp);

-- Step B: Create the final results table FOR THIS CHUNK ONLY.
-- This table is uniquely named for each worker to avoid conflicts.
CREATE TEMP TABLE temp_features_128987036632768 AS
WITH
  basic_windows AS (
    SELECT
      *,
      LAG(is_stationary, 1, is_stationary) OVER w AS prev_stationary,
      COALESCE(EXTRACT(EPOCH FROM (timestamp - LAG(timestamp, 1) OVER w)), 0) AS time_delta,
      AVG(load_weight) OVER (w ROWS BETWEEN 2 PRECEDING AND 2 FOLLOWING) AS smoothed_load_weight
    FROM temp_base
    WINDOW w AS (PARTITION BY device_id, device_date ORDER BY timestamp)
  ),
  duration_prep AS (
    SELECT
      *,
      SUM(CASE WHEN is_stationary != prev_stationary THEN 1 ELSE 0 END) OVER w AS stationary_block_id
    FROM basic_windows
    WINDOW w AS (PARTITION BY device_id, device_date ORDER BY timestamp)
  )
SELECT
  -- Select all final columns and calculate the remaining window functions
  timestamp, ingested_at, raw_event_hash_id, device_id, device_date, system_engaged,
  parking_brake_applied, current_position, current_speed, load_weight, state,
  software_state, prndl, extras, location_type, is_stationary, altitude,
  (altitude - LAG(altitude, 1) OVER w) AS altitude_rate_of_change,
  AVG(current_speed) OVER (w ROWS BETWEEN 2 PRECEDING AND 2 FOLLOWING) AS speed_rolling_avg_5s,
  smoothed_load_weight AS load_weight_smoothed,
  (smoothed_load_weight - LAG(smoothed_load_weight, 1) OVER w) AS load_weight_rate_of_change,
  (STDDEV(load_weight) OVER (PARTITION BY device_id) > 1000) AS has_reliable_payload,
  CASE
    WHEN is_stationary THEN SUM(time_delta) OVER (PARTITION BY device_id, device_date, stationary_block_id ORDER BY timestamp)
    ELSE 0
  END AS time_in_stationary_state
FROM duration_prep
WINDOW w AS (PARTITION BY device_id, device_date ORDER BY timestamp);]
[parameters: {'chunk_list': ('lake-605-8-0898_2025-08-02', 'lake-605-8-0898_2025-08-03', 'lake-605-8-0898_2025-08-04')}]
(Background on this error at: https://sqlalche.me/e/20/f405)
2025-09-03 06:23:00,305 - ERROR - ❌ [Worker-128986516547264] Failed during SQL execution: (psycopg2.errors.WrongObjectType) op ANY/ALL (array) requires array on right side
LINE 17: WHERE t.device_date = ANY(('lake-605-8-0902_2025-08-03', 'la...
                             ^

[SQL: -- This is a template script executed by each parallel Python worker.
-- It processes an assigned subset of device_date chunks.

-- Step A: Create a temporary table with base features FOR THIS CHUNK ONLY.
-- This uses the two-step materialization pattern correctly to avoid nested window function errors.
CREATE TEMP TABLE temp_base AS
SELECT
    t.*,
    ST_Z(t.current_position::geometry) AS altitude,
    (t.current_speed < 0.5) AS is_stationary,
    CASE
        WHEN ST_Z(t.current_position::geometry) < 200 THEN 'Loading Zone A'
        WHEN ST_Z(t.current_position::geometry) > 250 THEN 'Dump Zone B'
        ELSE 'Haul Road / Other'
    END AS location_type
FROM "02_raw_telemetry_transformed" t
WHERE t.device_date = ANY(%(chunk_list)s);

-- Index the small temp table for the next step.
CREATE INDEX ON temp_base (device_id, device_date, timestamp);

-- Step B: Create the final results table FOR THIS CHUNK ONLY.
-- This table is uniquely named for each worker to avoid conflicts.
CREATE TEMP TABLE temp_features_128986516547264 AS
WITH
  basic_windows AS (
    SELECT
      *,
      LAG(is_stationary, 1, is_stationary) OVER w AS prev_stationary,
      COALESCE(EXTRACT(EPOCH FROM (timestamp - LAG(timestamp, 1) OVER w)), 0) AS time_delta,
      AVG(load_weight) OVER (w ROWS BETWEEN 2 PRECEDING AND 2 FOLLOWING) AS smoothed_load_weight
    FROM temp_base
    WINDOW w AS (PARTITION BY device_id, device_date ORDER BY timestamp)
  ),
  duration_prep AS (
    SELECT
      *,
      SUM(CASE WHEN is_stationary != prev_stationary THEN 1 ELSE 0 END) OVER w AS stationary_block_id
    FROM basic_windows
    WINDOW w AS (PARTITION BY device_id, device_date ORDER BY timestamp)
  )
SELECT
  -- Select all final columns and calculate the remaining window functions
  timestamp, ingested_at, raw_event_hash_id, device_id, device_date, system_engaged,
  parking_brake_applied, current_position, current_speed, load_weight, state,
  software_state, prndl, extras, location_type, is_stationary, altitude,
  (altitude - LAG(altitude, 1) OVER w) AS altitude_rate_of_change,
  AVG(current_speed) OVER (w ROWS BETWEEN 2 PRECEDING AND 2 FOLLOWING) AS speed_rolling_avg_5s,
  smoothed_load_weight AS load_weight_smoothed,
  (smoothed_load_weight - LAG(smoothed_load_weight, 1) OVER w) AS load_weight_rate_of_change,
  (STDDEV(load_weight) OVER (PARTITION BY device_id) > 1000) AS has_reliable_payload,
  CASE
    WHEN is_stationary THEN SUM(time_delta) OVER (PARTITION BY device_id, device_date, stationary_block_id ORDER BY timestamp)
    ELSE 0
  END AS time_in_stationary_state
FROM duration_prep
WINDOW w AS (PARTITION BY device_id, device_date ORDER BY timestamp);]
[parameters: {'chunk_list': ('lake-605-8-0902_2025-08-03', 'lake-605-8-0902_2025-08-04', 'lake-605-8-0902_2025-08-05')}]
(Background on this error at: https://sqlalche.me/e/20/f405)
2025-09-03 06:23:00,306 - ERROR - ❌ [Worker-128986508154560] Failed during SQL execution: (psycopg2.errors.WrongObjectType) op ANY/ALL (array) requires array on right side
LINE 17: WHERE t.device_date = ANY(('lake-605-8-0902_2025-08-06', 'la...
                             ^

[SQL: -- This is a template script executed by each parallel Python worker.
-- It processes an assigned subset of device_date chunks.

-- Step A: Create a temporary table with base features FOR THIS CHUNK ONLY.
-- This uses the two-step materialization pattern correctly to avoid nested window function errors.
CREATE TEMP TABLE temp_base AS
SELECT
    t.*,
    ST_Z(t.current_position::geometry) AS altitude,
    (t.current_speed < 0.5) AS is_stationary,
    CASE
        WHEN ST_Z(t.current_position::geometry) < 200 THEN 'Loading Zone A'
        WHEN ST_Z(t.current_position::geometry) > 250 THEN 'Dump Zone B'
        ELSE 'Haul Road / Other'
    END AS location_type
FROM "02_raw_telemetry_transformed" t
WHERE t.device_date = ANY(%(chunk_list)s);

-- Index the small temp table for the next step.
CREATE INDEX ON temp_base (device_id, device_date, timestamp);

-- Step B: Create the final results table FOR THIS CHUNK ONLY.
-- This table is uniquely named for each worker to avoid conflicts.
CREATE TEMP TABLE temp_features_128986508154560 AS
WITH
  basic_windows AS (
    SELECT
      *,
      LAG(is_stationary, 1, is_stationary) OVER w AS prev_stationary,
      COALESCE(EXTRACT(EPOCH FROM (timestamp - LAG(timestamp, 1) OVER w)), 0) AS time_delta,
      AVG(load_weight) OVER (w ROWS BETWEEN 2 PRECEDING AND 2 FOLLOWING) AS smoothed_load_weight
    FROM temp_base
    WINDOW w AS (PARTITION BY device_id, device_date ORDER BY timestamp)
  ),
  duration_prep AS (
    SELECT
      *,
      SUM(CASE WHEN is_stationary != prev_stationary THEN 1 ELSE 0 END) OVER w AS stationary_block_id
    FROM basic_windows
    WINDOW w AS (PARTITION BY device_id, device_date ORDER BY timestamp)
  )
SELECT
  -- Select all final columns and calculate the remaining window functions
  timestamp, ingested_at, raw_event_hash_id, device_id, device_date, system_engaged,
  parking_brake_applied, current_position, current_speed, load_weight, state,
  software_state, prndl, extras, location_type, is_stationary, altitude,
  (altitude - LAG(altitude, 1) OVER w) AS altitude_rate_of_change,
  AVG(current_speed) OVER (w ROWS BETWEEN 2 PRECEDING AND 2 FOLLOWING) AS speed_rolling_avg_5s,
  smoothed_load_weight AS load_weight_smoothed,
  (smoothed_load_weight - LAG(smoothed_load_weight, 1) OVER w) AS load_weight_rate_of_change,
  (STDDEV(load_weight) OVER (PARTITION BY device_id) > 1000) AS has_reliable_payload,
  CASE
    WHEN is_stationary THEN SUM(time_delta) OVER (PARTITION BY device_id, device_date, stationary_block_id ORDER BY timestamp)
    ELSE 0
  END AS time_in_stationary_state
FROM duration_prep
WINDOW w AS (PARTITION BY device_id, device_date ORDER BY timestamp);]
[parameters: {'chunk_list': ('lake-605-8-0902_2025-08-06', 'lake-605-8-0902_2025-08-07', 'lake-605-8-0902_2025-08-08')}]
(Background on this error at: https://sqlalche.me/e/20/f405)
2025-09-03 06:23:00,308 - ERROR - ❌ [Worker-128987969873600] Failed during SQL execution: (psycopg2.errors.WrongObjectType) op ANY/ALL (array) requires array on right side
LINE 17: WHERE t.device_date = ANY(('lake-775g-2-2262_2025-07-30', 'l...
                             ^

[SQL: -- This is a template script executed by each parallel Python worker.
-- It processes an assigned subset of device_date chunks.

-- Step A: Create a temporary table with base features FOR THIS CHUNK ONLY.
-- This uses the two-step materialization pattern correctly to avoid nested window function errors.
CREATE TEMP TABLE temp_base AS
SELECT
    t.*,
    ST_Z(t.current_position::geometry) AS altitude,
    (t.current_speed < 0.5) AS is_stationary,
    CASE
        WHEN ST_Z(t.current_position::geometry) < 200 THEN 'Loading Zone A'
        WHEN ST_Z(t.current_position::geometry) > 250 THEN 'Dump Zone B'
        ELSE 'Haul Road / Other'
    END AS location_type
FROM "02_raw_telemetry_transformed" t
WHERE t.device_date = ANY(%(chunk_list)s);

-- Index the small temp table for the next step.
CREATE INDEX ON temp_base (device_id, device_date, timestamp);

-- Step B: Create the final results table FOR THIS CHUNK ONLY.
-- This table is uniquely named for each worker to avoid conflicts.
CREATE TEMP TABLE temp_features_128987969873600 AS
WITH
  basic_windows AS (
    SELECT
      *,
      LAG(is_stationary, 1, is_stationary) OVER w AS prev_stationary,
      COALESCE(EXTRACT(EPOCH FROM (timestamp - LAG(timestamp, 1) OVER w)), 0) AS time_delta,
      AVG(load_weight) OVER (w ROWS BETWEEN 2 PRECEDING AND 2 FOLLOWING) AS smoothed_load_weight
    FROM temp_base
    WINDOW w AS (PARTITION BY device_id, device_date ORDER BY timestamp)
  ),
  duration_prep AS (
    SELECT
      *,
      SUM(CASE WHEN is_stationary != prev_stationary THEN 1 ELSE 0 END) OVER w AS stationary_block_id
    FROM basic_windows
    WINDOW w AS (PARTITION BY device_id, device_date ORDER BY timestamp)
  )
SELECT
  -- Select all final columns and calculate the remaining window functions
  timestamp, ingested_at, raw_event_hash_id, device_id, device_date, system_engaged,
  parking_brake_applied, current_position, current_speed, load_weight, state,
  software_state, prndl, extras, location_type, is_stationary, altitude,
  (altitude - LAG(altitude, 1) OVER w) AS altitude_rate_of_change,
  AVG(current_speed) OVER (w ROWS BETWEEN 2 PRECEDING AND 2 FOLLOWING) AS speed_rolling_avg_5s,
  smoothed_load_weight AS load_weight_smoothed,
  (smoothed_load_weight - LAG(smoothed_load_weight, 1) OVER w) AS load_weight_rate_of_change,
  (STDDEV(load_weight) OVER (PARTITION BY device_id) > 1000) AS has_reliable_payload,
  CASE
    WHEN is_stationary THEN SUM(time_delta) OVER (PARTITION BY device_id, device_date, stationary_block_id ORDER BY timestamp)
    ELSE 0
  END AS time_in_stationary_state
FROM duration_prep
WINDOW w AS (PARTITION BY device_id, device_date ORDER BY timestamp);]
[parameters: {'chunk_list': ('lake-775g-2-2262_2025-07-30', 'lake-775g-2-2262_2025-07-31', 'lake-775g-2-2262_2025-08-01')}]
(Background on this error at: https://sqlalche.me/e/20/f405)
2025-09-03 06:23:00,309 - ERROR - ❌ [Worker-128987581896384] Failed during SQL execution: (psycopg2.errors.WrongObjectType) op ANY/ALL (array) requires array on right side
LINE 17: WHERE t.device_date = ANY(('lake-775g-2-2262_2025-08-05', 'l...
                             ^

[SQL: -- This is a template script executed by each parallel Python worker.
-- It processes an assigned subset of device_date chunks.

-- Step A: Create a temporary table with base features FOR THIS CHUNK ONLY.
-- This uses the two-step materialization pattern correctly to avoid nested window function errors.
CREATE TEMP TABLE temp_base AS
SELECT
    t.*,
    ST_Z(t.current_position::geometry) AS altitude,
    (t.current_speed < 0.5) AS is_stationary,
    CASE
        WHEN ST_Z(t.current_position::geometry) < 200 THEN 'Loading Zone A'
        WHEN ST_Z(t.current_position::geometry) > 250 THEN 'Dump Zone B'
        ELSE 'Haul Road / Other'
    END AS location_type
FROM "02_raw_telemetry_transformed" t
WHERE t.device_date = ANY(%(chunk_list)s);

-- Index the small temp table for the next step.
CREATE INDEX ON temp_base (device_id, device_date, timestamp);

-- Step B: Create the final results table FOR THIS CHUNK ONLY.
-- This table is uniquely named for each worker to avoid conflicts.
CREATE TEMP TABLE temp_features_128987581896384 AS
WITH
  basic_windows AS (
    SELECT
      *,
      LAG(is_stationary, 1, is_stationary) OVER w AS prev_stationary,
      COALESCE(EXTRACT(EPOCH FROM (timestamp - LAG(timestamp, 1) OVER w)), 0) AS time_delta,
      AVG(load_weight) OVER (w ROWS BETWEEN 2 PRECEDING AND 2 FOLLOWING) AS smoothed_load_weight
    FROM temp_base
    WINDOW w AS (PARTITION BY device_id, device_date ORDER BY timestamp)
  ),
  duration_prep AS (
    SELECT
      *,
      SUM(CASE WHEN is_stationary != prev_stationary THEN 1 ELSE 0 END) OVER w AS stationary_block_id
    FROM basic_windows
    WINDOW w AS (PARTITION BY device_id, device_date ORDER BY timestamp)
  )
SELECT
  -- Select all final columns and calculate the remaining window functions
  timestamp, ingested_at, raw_event_hash_id, device_id, device_date, system_engaged,
  parking_brake_applied, current_position, current_speed, load_weight, state,
  software_state, prndl, extras, location_type, is_stationary, altitude,
  (altitude - LAG(altitude, 1) OVER w) AS altitude_rate_of_change,
  AVG(current_speed) OVER (w ROWS BETWEEN 2 PRECEDING AND 2 FOLLOWING) AS speed_rolling_avg_5s,
  smoothed_load_weight AS load_weight_smoothed,
  (smoothed_load_weight - LAG(smoothed_load_weight, 1) OVER w) AS load_weight_rate_of_change,
  (STDDEV(load_weight) OVER (PARTITION BY device_id) > 1000) AS has_reliable_payload,
  CASE
    WHEN is_stationary THEN SUM(time_delta) OVER (PARTITION BY device_id, device_date, stationary_block_id ORDER BY timestamp)
    ELSE 0
  END AS time_in_stationary_state
FROM duration_prep
WINDOW w AS (PARTITION BY device_id, device_date ORDER BY timestamp);]
[parameters: {'chunk_list': ('lake-775g-2-2262_2025-08-05', 'lake-775g-2-2262_2025-08-06', 'lake-775g-2-2262_2025-08-07')}]
(Background on this error at: https://sqlalche.me/e/20/f405)
2025-09-03 06:23:00,310 - ERROR - ❌ [Worker-128986491369152] Failed during SQL execution: (psycopg2.errors.WrongObjectType) op ANY/ALL (array) requires array on right side
LINE 17: WHERE t.device_date = ANY(('lake-775g-2-2262_2025-08-02', 'l...
                             ^

[SQL: -- This is a template script executed by each parallel Python worker.
-- It processes an assigned subset of device_date chunks.

-- Step A: Create a temporary table with base features FOR THIS CHUNK ONLY.
-- This uses the two-step materialization pattern correctly to avoid nested window function errors.
CREATE TEMP TABLE temp_base AS
SELECT
    t.*,
    ST_Z(t.current_position::geometry) AS altitude,
    (t.current_speed < 0.5) AS is_stationary,
    CASE
        WHEN ST_Z(t.current_position::geometry) < 200 THEN 'Loading Zone A'
        WHEN ST_Z(t.current_position::geometry) > 250 THEN 'Dump Zone B'
        ELSE 'Haul Road / Other'
    END AS location_type
FROM "02_raw_telemetry_transformed" t
WHERE t.device_date = ANY(%(chunk_list)s);

-- Index the small temp table for the next step.
CREATE INDEX ON temp_base (device_id, device_date, timestamp);

-- Step B: Create the final results table FOR THIS CHUNK ONLY.
-- This table is uniquely named for each worker to avoid conflicts.
CREATE TEMP TABLE temp_features_128986491369152 AS
WITH
  basic_windows AS (
    SELECT
      *,
      LAG(is_stationary, 1, is_stationary) OVER w AS prev_stationary,
      COALESCE(EXTRACT(EPOCH FROM (timestamp - LAG(timestamp, 1) OVER w)), 0) AS time_delta,
      AVG(load_weight) OVER (w ROWS BETWEEN 2 PRECEDING AND 2 FOLLOWING) AS smoothed_load_weight
    FROM temp_base
    WINDOW w AS (PARTITION BY device_id, device_date ORDER BY timestamp)
  ),
  duration_prep AS (
    SELECT
      *,
      SUM(CASE WHEN is_stationary != prev_stationary THEN 1 ELSE 0 END) OVER w AS stationary_block_id
    FROM basic_windows
    WINDOW w AS (PARTITION BY device_id, device_date ORDER BY timestamp)
  )
SELECT
  -- Select all final columns and calculate the remaining window functions
  timestamp, ingested_at, raw_event_hash_id, device_id, device_date, system_engaged,
  parking_brake_applied, current_position, current_speed, load_weight, state,
  software_state, prndl, extras, location_type, is_stationary, altitude,
  (altitude - LAG(altitude, 1) OVER w) AS altitude_rate_of_change,
  AVG(current_speed) OVER (w ROWS BETWEEN 2 PRECEDING AND 2 FOLLOWING) AS speed_rolling_avg_5s,
  smoothed_load_weight AS load_weight_smoothed,
  (smoothed_load_weight - LAG(smoothed_load_weight, 1) OVER w) AS load_weight_rate_of_change,
  (STDDEV(load_weight) OVER (PARTITION BY device_id) > 1000) AS has_reliable_payload,
  CASE
    WHEN is_stationary THEN SUM(time_delta) OVER (PARTITION BY device_id, device_date, stationary_block_id ORDER BY timestamp)
    ELSE 0
  END AS time_in_stationary_state
FROM duration_prep
WINDOW w AS (PARTITION BY device_id, device_date ORDER BY timestamp);]
[parameters: {'chunk_list': ('lake-775g-2-2262_2025-08-02', 'lake-775g-2-2262_2025-08-03', 'lake-775g-2-2262_2025-08-04')}]
(Background on this error at: https://sqlalche.me/e/20/f405)
2025-09-03 06:23:00,312 - ERROR - ❌ [Worker-128987019847360] Failed during SQL execution: (psycopg2.errors.WrongObjectType) op ANY/ALL (array) requires array on right side
LINE 17: WHERE t.device_date = ANY(('lake-775g-2-2262_2025-08-08', 'l...
                             ^

[SQL: -- This is a template script executed by each parallel Python worker.
-- It processes an assigned subset of device_date chunks.

-- Step A: Create a temporary table with base features FOR THIS CHUNK ONLY.
-- This uses the two-step materialization pattern correctly to avoid nested window function errors.
CREATE TEMP TABLE temp_base AS
SELECT
    t.*,
    ST_Z(t.current_position::geometry) AS altitude,
    (t.current_speed < 0.5) AS is_stationary,
    CASE
        WHEN ST_Z(t.current_position::geometry) < 200 THEN 'Loading Zone A'
        WHEN ST_Z(t.current_position::geometry) > 250 THEN 'Dump Zone B'
        ELSE 'Haul Road / Other'
    END AS location_type
FROM "02_raw_telemetry_transformed" t
WHERE t.device_date = ANY(%(chunk_list)s);

-- Index the small temp table for the next step.
CREATE INDEX ON temp_base (device_id, device_date, timestamp);

-- Step B: Create the final results table FOR THIS CHUNK ONLY.
-- This table is uniquely named for each worker to avoid conflicts.
CREATE TEMP TABLE temp_features_128987019847360 AS
WITH
  basic_windows AS (
    SELECT
      *,
      LAG(is_stationary, 1, is_stationary) OVER w AS prev_stationary,
      COALESCE(EXTRACT(EPOCH FROM (timestamp - LAG(timestamp, 1) OVER w)), 0) AS time_delta,
      AVG(load_weight) OVER (w ROWS BETWEEN 2 PRECEDING AND 2 FOLLOWING) AS smoothed_load_weight
    FROM temp_base
    WINDOW w AS (PARTITION BY device_id, device_date ORDER BY timestamp)
  ),
  duration_prep AS (
    SELECT
      *,
      SUM(CASE WHEN is_stationary != prev_stationary THEN 1 ELSE 0 END) OVER w AS stationary_block_id
    FROM basic_windows
    WINDOW w AS (PARTITION BY device_id, device_date ORDER BY timestamp)
  )
SELECT
  -- Select all final columns and calculate the remaining window functions
  timestamp, ingested_at, raw_event_hash_id, device_id, device_date, system_engaged,
  parking_brake_applied, current_position, current_speed, load_weight, state,
  software_state, prndl, extras, location_type, is_stationary, altitude,
  (altitude - LAG(altitude, 1) OVER w) AS altitude_rate_of_change,
  AVG(current_speed) OVER (w ROWS BETWEEN 2 PRECEDING AND 2 FOLLOWING) AS speed_rolling_avg_5s,
  smoothed_load_weight AS load_weight_smoothed,
  (smoothed_load_weight - LAG(smoothed_load_weight, 1) OVER w) AS load_weight_rate_of_change,
  (STDDEV(load_weight) OVER (PARTITION BY device_id) > 1000) AS has_reliable_payload,
  CASE
    WHEN is_stationary THEN SUM(time_delta) OVER (PARTITION BY device_id, device_date, stationary_block_id ORDER BY timestamp)
    ELSE 0
  END AS time_in_stationary_state
FROM duration_prep
WINDOW w AS (PARTITION BY device_id, device_date ORDER BY timestamp);]
[parameters: {'chunk_list': ('lake-775g-2-2262_2025-08-08', 'lake-775g-2-2262_2025-08-09', 'lake-775g-2-2262_2025-08-10')}]
(Background on this error at: https://sqlalche.me/e/20/f405)
2025-09-03 06:23:00,313 - ERROR - ❌ [Worker-128987590289088] Failed during SQL execution: (psycopg2.errors.WrongObjectType) op ANY/ALL (array) requires array on right side
LINE 17: WHERE t.device_date = ANY(('lake-775g-2-2262_2025-08-12', 'l...
                             ^

[SQL: -- This is a template script executed by each parallel Python worker.
-- It processes an assigned subset of device_date chunks.

-- Step A: Create a temporary table with base features FOR THIS CHUNK ONLY.
-- This uses the two-step materialization pattern correctly to avoid nested window function errors.
CREATE TEMP TABLE temp_base AS
SELECT
    t.*,
    ST_Z(t.current_position::geometry) AS altitude,
    (t.current_speed < 0.5) AS is_stationary,
    CASE
        WHEN ST_Z(t.current_position::geometry) < 200 THEN 'Loading Zone A'
        WHEN ST_Z(t.current_position::geometry) > 250 THEN 'Dump Zone B'
        ELSE 'Haul Road / Other'
    END AS location_type
FROM "02_raw_telemetry_transformed" t
WHERE t.device_date = ANY(%(chunk_list)s);

-- Index the small temp table for the next step.
CREATE INDEX ON temp_base (device_id, device_date, timestamp);

-- Step B: Create the final results table FOR THIS CHUNK ONLY.
-- This table is uniquely named for each worker to avoid conflicts.
CREATE TEMP TABLE temp_features_128987590289088 AS
WITH
  basic_windows AS (
    SELECT
      *,
      LAG(is_stationary, 1, is_stationary) OVER w AS prev_stationary,
      COALESCE(EXTRACT(EPOCH FROM (timestamp - LAG(timestamp, 1) OVER w)), 0) AS time_delta,
      AVG(load_weight) OVER (w ROWS BETWEEN 2 PRECEDING AND 2 FOLLOWING) AS smoothed_load_weight
    FROM temp_base
    WINDOW w AS (PARTITION BY device_id, device_date ORDER BY timestamp)
  ),
  duration_prep AS (
    SELECT
      *,
      SUM(CASE WHEN is_stationary != prev_stationary THEN 1 ELSE 0 END) OVER w AS stationary_block_id
    FROM basic_windows
    WINDOW w AS (PARTITION BY device_id, device_date ORDER BY timestamp)
  )
SELECT
  -- Select all final columns and calculate the remaining window functions
  timestamp, ingested_at, raw_event_hash_id, device_id, device_date, system_engaged,
  parking_brake_applied, current_position, current_speed, load_weight, state,
  software_state, prndl, extras, location_type, is_stationary, altitude,
  (altitude - LAG(altitude, 1) OVER w) AS altitude_rate_of_change,
  AVG(current_speed) OVER (w ROWS BETWEEN 2 PRECEDING AND 2 FOLLOWING) AS speed_rolling_avg_5s,
  smoothed_load_weight AS load_weight_smoothed,
  (smoothed_load_weight - LAG(smoothed_load_weight, 1) OVER w) AS load_weight_rate_of_change,
  (STDDEV(load_weight) OVER (PARTITION BY device_id) > 1000) AS has_reliable_payload,
  CASE
    WHEN is_stationary THEN SUM(time_delta) OVER (PARTITION BY device_id, device_date, stationary_block_id ORDER BY timestamp)
    ELSE 0
  END AS time_in_stationary_state
FROM duration_prep
WINDOW w AS (PARTITION BY device_id, device_date ORDER BY timestamp);]
[parameters: {'chunk_list': ('lake-775g-2-2262_2025-08-12', 'lake-775g-2-2266_2025-07-30', 'lake-775g-2-2266_2025-07-31')}]
(Background on this error at: https://sqlalche.me/e/20/f405)
2025-09-03 06:23:00,313 - ERROR - ❌ [Worker-128987565110976] Failed during SQL execution: (psycopg2.errors.WrongObjectType) op ANY/ALL (array) requires array on right side
LINE 17: WHERE t.device_date = ANY(('lake-775g-2-2266_2025-08-01', 'l...
                             ^

[SQL: -- This is a template script executed by each parallel Python worker.
-- It processes an assigned subset of device_date chunks.

-- Step A: Create a temporary table with base features FOR THIS CHUNK ONLY.
-- This uses the two-step materialization pattern correctly to avoid nested window function errors.
CREATE TEMP TABLE temp_base AS
SELECT
    t.*,
    ST_Z(t.current_position::geometry) AS altitude,
    (t.current_speed < 0.5) AS is_stationary,
    CASE
        WHEN ST_Z(t.current_position::geometry) < 200 THEN 'Loading Zone A'
        WHEN ST_Z(t.current_position::geometry) > 250 THEN 'Dump Zone B'
        ELSE 'Haul Road / Other'
    END AS location_type
FROM "02_raw_telemetry_transformed" t
WHERE t.device_date = ANY(%(chunk_list)s);

-- Index the small temp table for the next step.
CREATE INDEX ON temp_base (device_id, device_date, timestamp);

-- Step B: Create the final results table FOR THIS CHUNK ONLY.
-- This table is uniquely named for each worker to avoid conflicts.
CREATE TEMP TABLE temp_features_128987565110976 AS
WITH
  basic_windows AS (
    SELECT
      *,
      LAG(is_stationary, 1, is_stationary) OVER w AS prev_stationary,
      COALESCE(EXTRACT(EPOCH FROM (timestamp - LAG(timestamp, 1) OVER w)), 0) AS time_delta,
      AVG(load_weight) OVER (w ROWS BETWEEN 2 PRECEDING AND 2 FOLLOWING) AS smoothed_load_weight
    FROM temp_base
    WINDOW w AS (PARTITION BY device_id, device_date ORDER BY timestamp)
  ),
  duration_prep AS (
    SELECT
      *,
      SUM(CASE WHEN is_stationary != prev_stationary THEN 1 ELSE 0 END) OVER w AS stationary_block_id
    FROM basic_windows
    WINDOW w AS (PARTITION BY device_id, device_date ORDER BY timestamp)
  )
SELECT
  -- Select all final columns and calculate the remaining window functions
  timestamp, ingested_at, raw_event_hash_id, device_id, device_date, system_engaged,
  parking_brake_applied, current_position, current_speed, load_weight, state,
  software_state, prndl, extras, location_type, is_stationary, altitude,
  (altitude - LAG(altitude, 1) OVER w) AS altitude_rate_of_change,
  AVG(current_speed) OVER (w ROWS BETWEEN 2 PRECEDING AND 2 FOLLOWING) AS speed_rolling_avg_5s,
  smoothed_load_weight AS load_weight_smoothed,
  (smoothed_load_weight - LAG(smoothed_load_weight, 1) OVER w) AS load_weight_rate_of_change,
  (STDDEV(load_weight) OVER (PARTITION BY device_id) > 1000) AS has_reliable_payload,
  CASE
    WHEN is_stationary THEN SUM(time_delta) OVER (PARTITION BY device_id, device_date, stationary_block_id ORDER BY timestamp)
    ELSE 0
  END AS time_in_stationary_state
FROM duration_prep
WINDOW w AS (PARTITION BY device_id, device_date ORDER BY timestamp);]
[parameters: {'chunk_list': ('lake-775g-2-2266_2025-08-01', 'lake-775g-2-2266_2025-08-02', 'lake-775g-2-2266_2025-08-03')}]
(Background on this error at: https://sqlalche.me/e/20/f405)
2025-09-03 06:23:00,313 - ERROR - ❌ Pipeline execution failed: (psycopg2.errors.WrongObjectType) op ANY/ALL (array) requires array on right side
LINE 17: WHERE t.device_date = ANY(('lake-605-10-0050_2025-07-30', 'l...
                             ^

[SQL: -- This is a template script executed by each parallel Python worker.
-- It processes an assigned subset of device_date chunks.

-- Step A: Create a temporary table with base features FOR THIS CHUNK ONLY.
-- This uses the two-step materialization pattern correctly to avoid nested window function errors.
CREATE TEMP TABLE temp_base AS
SELECT
    t.*,
    ST_Z(t.current_position::geometry) AS altitude,
    (t.current_speed < 0.5) AS is_stationary,
    CASE
        WHEN ST_Z(t.current_position::geometry) < 200 THEN 'Loading Zone A'
        WHEN ST_Z(t.current_position::geometry) > 250 THEN 'Dump Zone B'
        ELSE 'Haul Road / Other'
    END AS location_type
FROM "02_raw_telemetry_transformed" t
WHERE t.device_date = ANY(%(chunk_list)s);

-- Index the small temp table for the next step.
CREATE INDEX ON temp_base (device_id, device_date, timestamp);

-- Step B: Create the final results table FOR THIS CHUNK ONLY.
-- This table is uniquely named for each worker to avoid conflicts.
CREATE TEMP TABLE temp_features_128987978266304 AS
WITH
  basic_windows AS (
    SELECT
      *,
      LAG(is_stationary, 1, is_stationary) OVER w AS prev_stationary,
      COALESCE(EXTRACT(EPOCH FROM (timestamp - LAG(timestamp, 1) OVER w)), 0) AS time_delta,
      AVG(load_weight) OVER (w ROWS BETWEEN 2 PRECEDING AND 2 FOLLOWING) AS smoothed_load_weight
    FROM temp_base
    WINDOW w AS (PARTITION BY device_id, device_date ORDER BY timestamp)
  ),
  duration_prep AS (
    SELECT
      *,
      SUM(CASE WHEN is_stationary != prev_stationary THEN 1 ELSE 0 END) OVER w AS stationary_block_id
    FROM basic_windows
    WINDOW w AS (PARTITION BY device_id, device_date ORDER BY timestamp)
  )
SELECT
  -- Select all final columns and calculate the remaining window functions
  timestamp, ingested_at, raw_event_hash_id, device_id, device_date, system_engaged,
  parking_brake_applied, current_position, current_speed, load_weight, state,
  software_state, prndl, extras, location_type, is_stationary, altitude,
  (altitude - LAG(altitude, 1) OVER w) AS altitude_rate_of_change,
  AVG(current_speed) OVER (w ROWS BETWEEN 2 PRECEDING AND 2 FOLLOWING) AS speed_rolling_avg_5s,
  smoothed_load_weight AS load_weight_smoothed,
  (smoothed_load_weight - LAG(smoothed_load_weight, 1) OVER w) AS load_weight_rate_of_change,
  (STDDEV(load_weight) OVER (PARTITION BY device_id) > 1000) AS has_reliable_payload,
  CASE
    WHEN is_stationary THEN SUM(time_delta) OVER (PARTITION BY device_id, device_date, stationary_block_id ORDER BY timestamp)
    ELSE 0
  END AS time_in_stationary_state
FROM duration_prep
WINDOW w AS (PARTITION BY device_id, device_date ORDER BY timestamp);]
[parameters: {'chunk_list': ('lake-605-10-0050_2025-07-30', 'lake-605-10-0050_2025-07-31', 'lake-605-10-0050_2025-08-01')}]
(Background on this error at: https://sqlalche.me/e/20/f405)
2025-09-03 06:23:10,600 - INFO - Connected to database with high-performance configuration
2025-09-03 06:23:10,600 - INFO - Initialized with 32 parallel workers.
2025-09-03 06:23:10,600 - INFO - ======================================================================
2025-09-03 06:23:10,600 - INFO - STARTING MASSIVELY PARALLEL FEATURE ENGINEERING
2025-09-03 06:23:10,600 - INFO - ======================================================================
2025-09-03 06:23:10,600 - INFO - Fetching all device-date chunks to be processed...
2025-09-03 06:23:11,052 - INFO - ✓ Divided 96 total chunks into 32 batches for parallel processing.
2025-09-03 06:23:11,052 - INFO - 🚀 Launching parallel workers...
2025-09-03 06:23:11,053 - INFO - [Worker-138409888708288] Started, assigned 3 chunks.
2025-09-03 06:23:11,054 - INFO - [Worker-138409880315584] Started, assigned 3 chunks.
2025-09-03 06:23:11,055 - INFO - [Worker-138409871922880] Started, assigned 3 chunks.
2025-09-03 06:23:11,056 - INFO - [Worker-138409863530176] Started, assigned 3 chunks.
2025-09-03 06:23:11,057 - INFO - [Worker-138409855137472] Started, assigned 3 chunks.
2025-09-03 06:23:11,058 - INFO - [Worker-138409846744768] Started, assigned 3 chunks.
2025-09-03 06:23:11,058 - INFO - [Worker-138409838352064] Started, assigned 3 chunks.
2025-09-03 06:23:11,060 - INFO - [Worker-138409829959360] Started, assigned 3 chunks.
2025-09-03 06:23:11,061 - INFO - [Worker-138409347643072] Started, assigned 3 chunks.
2025-09-03 06:23:11,061 - INFO - [Worker-138409339250368] Started, assigned 3 chunks.
2025-09-03 06:23:11,062 - INFO - [Worker-138409330857664] Started, assigned 3 chunks.
2025-09-03 06:23:11,063 - INFO - [Worker-138409322464960] Started, assigned 3 chunks.
2025-09-03 06:23:11,064 - INFO - [Worker-138409314072256] Started, assigned 3 chunks.
2025-09-03 06:23:11,065 - INFO - [Worker-138409305679552] Started, assigned 3 chunks.
2025-09-03 06:23:11,066 - INFO - [Worker-138409297286848] Started, assigned 3 chunks.
2025-09-03 06:23:11,067 - INFO - [Worker-138408743663296] Started, assigned 3 chunks.
2025-09-03 06:23:11,069 - INFO - [Worker-138408735270592] Started, assigned 3 chunks.
2025-09-03 06:23:11,072 - INFO - [Worker-138408726877888] Started, assigned 3 chunks.
2025-09-03 06:23:11,073 - INFO - [Worker-138408718485184] Started, assigned 3 chunks.
2025-09-03 06:23:11,076 - INFO - [Worker-138408710092480] Started, assigned 3 chunks.
2025-09-03 06:23:11,083 - INFO - [Worker-138408701699776] Started, assigned 3 chunks.
2025-09-03 06:23:11,084 - INFO - [Worker-138408693307072] Started, assigned 3 chunks.
2025-09-03 06:23:11,090 - INFO - [Worker-138408206792384] Started, assigned 3 chunks.
2025-09-03 06:23:11,093 - INFO - [Worker-138408198399680] Started, assigned 3 chunks.
2025-09-03 06:23:11,094 - INFO - [Worker-138408190006976] Started, assigned 3 chunks.
2025-09-03 06:23:11,095 - INFO - [Worker-138408181614272] Started, assigned 3 chunks.
2025-09-03 06:23:11,096 - INFO - [Worker-138408173221568] Started, assigned 3 chunks.
2025-09-03 06:23:11,097 - INFO - [Worker-138408164828864] Started, assigned 3 chunks.
2025-09-03 06:23:11,098 - INFO - [Worker-138408156436160] Started, assigned 3 chunks.
2025-09-03 06:23:11,101 - INFO - [Worker-138407669921472] Started, assigned 3 chunks.
2025-09-03 06:23:11,103 - INFO - [Worker-138407661528768] Started, assigned 3 chunks.
2025-09-03 06:23:11,106 - INFO - [Worker-138407653136064] Started, assigned 3 chunks.
2025-09-03 06:23:13,381 - INFO - ✓ [Worker-138408710092480] Successfully processed chunks and created temp_features_138408710092480.
2025-09-03 06:23:13,954 - INFO - ✓ [Worker-138407653136064] Successfully processed chunks and created temp_features_138407653136064.
2025-09-03 06:23:14,234 - INFO - ✓ [Worker-138409305679552] Successfully processed chunks and created temp_features_138409305679552.
2025-09-03 06:23:14,327 - INFO - ✓ [Worker-138409347643072] Successfully processed chunks and created temp_features_138409347643072.
2025-09-03 06:23:14,550 - INFO - ✓ [Worker-138408693307072] Successfully processed chunks and created temp_features_138408693307072.
2025-09-03 06:23:14,764 - INFO - ✓ [Worker-138409297286848] Successfully processed chunks and created temp_features_138409297286848.
2025-09-03 06:23:14,919 - INFO - ✓ [Worker-138409838352064] Successfully processed chunks and created temp_features_138409838352064.
2025-09-03 06:23:15,027 - INFO - ✓ [Worker-138409863530176] Successfully processed chunks and created temp_features_138409863530176.
2025-09-03 06:23:15,057 - INFO - ✓ [Worker-138409855137472] Successfully processed chunks and created temp_features_138409855137472.
2025-09-03 06:23:15,141 - INFO - ✓ [Worker-138409880315584] Successfully processed chunks and created temp_features_138409880315584.
2025-09-03 06:23:15,167 - INFO - ✓ [Worker-138409871922880] Successfully processed chunks and created temp_features_138409871922880.
2025-09-03 06:23:15,278 - INFO - ✓ [Worker-138409888708288] Successfully processed chunks and created temp_features_138409888708288.
2025-09-03 06:23:15,295 - INFO - ✓ [Worker-138409339250368] Successfully processed chunks and created temp_features_138409339250368.
2025-09-03 06:23:15,434 - INFO - ✓ [Worker-138409829959360] Successfully processed chunks and created temp_features_138409829959360.
2025-09-03 06:23:15,495 - INFO - ✓ [Worker-138408718485184] Successfully processed chunks and created temp_features_138408718485184.
2025-09-03 06:23:15,498 - INFO - ✓ [Worker-138408206792384] Successfully processed chunks and created temp_features_138408206792384.
2025-09-03 06:23:15,550 - INFO - ✓ [Worker-138407669921472] Successfully processed chunks and created temp_features_138407669921472.
2025-09-03 06:23:15,560 - INFO - ✓ [Worker-138409330857664] Successfully processed chunks and created temp_features_138409330857664.
2025-09-03 06:23:15,623 - INFO - ✓ [Worker-138408735270592] Successfully processed chunks and created temp_features_138408735270592.
2025-09-03 06:23:15,752 - INFO - ✓ [Worker-138408743663296] Successfully processed chunks and created temp_features_138408743663296.
2025-09-03 06:23:15,805 - INFO - ✓ [Worker-138408726877888] Successfully processed chunks and created temp_features_138408726877888.
2025-09-03 06:23:15,811 - INFO - ✓ [Worker-138408190006976] Successfully processed chunks and created temp_features_138408190006976.
2025-09-03 06:23:15,811 - INFO - ✓ [Worker-138408701699776] Successfully processed chunks and created temp_features_138408701699776.
2025-09-03 06:23:15,829 - INFO - ✓ [Worker-138408156436160] Successfully processed chunks and created temp_features_138408156436160.
2025-09-03 06:23:15,942 - INFO - ✓ [Worker-138408164828864] Successfully processed chunks and created temp_features_138408164828864.
2025-09-03 06:23:15,985 - INFO - ✓ [Worker-138409846744768] Successfully processed chunks and created temp_features_138409846744768.
2025-09-03 06:23:16,006 - INFO - ✓ [Worker-138409322464960] Successfully processed chunks and created temp_features_138409322464960.
2025-09-03 06:23:16,009 - INFO - ✓ [Worker-138408198399680] Successfully processed chunks and created temp_features_138408198399680.
2025-09-03 06:23:16,021 - INFO - ✓ [Worker-138408173221568] Successfully processed chunks and created temp_features_138408173221568.
2025-09-03 06:23:16,597 - INFO - ✓ [Worker-138409314072256] Successfully processed chunks and created temp_features_138409314072256.
2025-09-03 06:23:16,820 - INFO - ✓ [Worker-138407661528768] Successfully processed chunks and created temp_features_138407661528768.
2025-09-03 06:23:17,326 - INFO - ✓ [Worker-138408181614272] Successfully processed chunks and created temp_features_138408181614272.
2025-09-03 06:23:17,330 - INFO - 🤝 Assembling results from all parallel workers...
2025-09-03 06:23:17,330 - INFO - ✓ Loaded SQL script: final_assembly.sql
2025-09-03 06:23:17,332 - ERROR - ❌ Pipeline execution failed: (psycopg2.errors.UndefinedTable) relation "temp_features_138409888708288" does not exist
LINE 33:     SELECT * FROM temp_features_138409888708288
                           ^

[SQL: -- This script runs once at the end to assemble the results from all parallel workers.

DROP TABLE IF EXISTS "03_primary_feature_table";

CREATE TABLE "03_primary_feature_table" AS
SELECT
    -- Original Columns
    timestamp, ingested_at, raw_event_hash_id, device_id, device_date, system_engaged,
    parking_brake_applied, current_position, current_speed, load_weight, state,
    software_state, prndl, extras,
    
    -- Engineered Features
    location_type, is_stationary, altitude, altitude_rate_of_change, speed_rolling_avg_5s,
    load_weight_smoothed, load_weight_rate_of_change, has_reliable_payload, time_in_stationary_state,
    
    -- PRNDL one-hot encoding
    (prndl = 'park') AS prndl_park,
    (prndl = 'reverse') AS prndl_reverse, 
    (prndl = 'neutral') AS prndl_neutral,
    (prndl = 'drive') AS prndl_drive,
    (prndl = 'unknown') AS prndl_unknown,
    
    -- Interaction Features (calculated on the fly during assembly)
    (is_stationary AND prndl = 'neutral' AND parking_brake_applied) AS is_ready_for_load,
    (location_type = 'Haul Road / Other' AND NOT is_stationary) AS is_hauling,
    (is_stationary AND location_type LIKE '%%Loading%%') AS is_loading_position,
    (is_stationary AND location_type LIKE '%%Dump%%') AS is_dumping_position,
    (load_weight_smoothed > 50000) AS is_heavy_load

FROM (
    -- This placeholder will be replaced by the Python script
    -- with a UNION ALL of all the worker temp tables.
    SELECT * FROM temp_features_138409888708288
UNION ALL
SELECT * FROM temp_features_138409880315584
UNION ALL
SELECT * FROM temp_features_138409871922880
UNION ALL
SELECT * FROM temp_features_138409863530176
UNION ALL
SELECT * FROM temp_features_138409855137472
UNION ALL
SELECT * FROM temp_features_138409846744768
UNION ALL
SELECT * FROM temp_features_138409838352064
UNION ALL
SELECT * FROM temp_features_138409829959360
UNION ALL
SELECT * FROM temp_features_138409347643072
UNION ALL
SELECT * FROM temp_features_138409339250368
UNION ALL
SELECT * FROM temp_features_138409330857664
UNION ALL
SELECT * FROM temp_features_138409322464960
UNION ALL
SELECT * FROM temp_features_138409314072256
UNION ALL
SELECT * FROM temp_features_138409305679552
UNION ALL
SELECT * FROM temp_features_138409297286848
UNION ALL
SELECT * FROM temp_features_138408743663296
UNION ALL
SELECT * FROM temp_features_138408735270592
UNION ALL
SELECT * FROM temp_features_138408726877888
UNION ALL
SELECT * FROM temp_features_138408718485184
UNION ALL
SELECT * FROM temp_features_138408710092480
UNION ALL
SELECT * FROM temp_features_138408701699776
UNION ALL
SELECT * FROM temp_features_138408693307072
UNION ALL
SELECT * FROM temp_features_138408206792384
UNION ALL
SELECT * FROM temp_features_138408198399680
UNION ALL
SELECT * FROM temp_features_138408190006976
UNION ALL
SELECT * FROM temp_features_138408181614272
UNION ALL
SELECT * FROM temp_features_138408173221568
UNION ALL
SELECT * FROM temp_features_138408164828864
UNION ALL
SELECT * FROM temp_features_138408156436160
UNION ALL
SELECT * FROM temp_features_138407669921472
UNION ALL
SELECT * FROM temp_features_138407661528768
UNION ALL
SELECT * FROM temp_features_138407653136064
) AS combined_results
ORDER BY device_id, timestamp;

-- Apply final optimizations
ALTER TABLE "03_primary_feature_table" ADD PRIMARY KEY (raw_event_hash_id);

CREATE INDEX CONCURRENTLY idx_03_primary_device_timestamp 
ON "03_primary_feature_table" (device_id, timestamp);

CREATE INDEX CONCURRENTLY idx_03_primary_stationary 
ON "03_primary_feature_table" (is_stationary) 
WHERE is_stationary = true;

CREATE INDEX CONCURRENTLY idx_03_primary_location 
ON "03_primary_feature_table" (location_type);

CREATE INDEX CONCURRENTLY idx_03_primary_heavy_load
ON "03_primary_feature_table" (is_heavy_load)
WHERE is_heavy_load = true;

CREATE INDEX CONCURRENTLY idx_03_primary_ready_for_load
ON "03_primary_feature_table" (is_ready_for_load)
WHERE is_ready_for_load = true;

ANALYZE "03_primary_feature_table";]
(Background on this error at: https://sqlalche.me/e/20/f405)
2025-09-03 06:23:58,211 - INFO - Connected to database with high-performance configuration
2025-09-03 06:23:58,211 - INFO - Initialized with 32 parallel workers.
2025-09-03 06:23:58,211 - INFO - ======================================================================
2025-09-03 06:23:58,211 - INFO - STARTING MASSIVELY PARALLEL FEATURE ENGINEERING
2025-09-03 06:23:58,211 - INFO - ======================================================================
2025-09-03 06:23:58,211 - INFO - Fetching all device-date chunks to be processed...
2025-09-03 06:23:58,658 - INFO - ✓ Divided 96 total chunks into 32 batches for parallel processing.
2025-09-03 06:23:58,659 - INFO - 🚀 Launching parallel workers...
2025-09-03 06:23:58,660 - INFO - [Worker-135723088869056] Started, assigned 3 chunks.
2025-09-03 06:23:58,661 - INFO - [Worker-135723080476352] Started, assigned 3 chunks.
2025-09-03 06:23:58,661 - INFO - [Worker-135723072083648] Started, assigned 3 chunks.
2025-09-03 06:23:58,663 - INFO - [Worker-135723063690944] Started, assigned 3 chunks.
2025-09-03 06:23:58,663 - INFO - [Worker-135722845599424] Started, assigned 3 chunks.
2025-09-03 06:23:58,664 - INFO - [Worker-135722837206720] Started, assigned 3 chunks.
2025-09-03 06:23:58,664 - INFO - [Worker-135722828814016] Started, assigned 3 chunks.
2025-09-03 06:23:58,665 - INFO - [Worker-135722820421312] Started, assigned 3 chunks.
2025-09-03 06:23:58,666 - INFO - [Worker-135722812028608] Started, assigned 3 chunks.
2025-09-03 06:23:58,667 - INFO - [Worker-135722803635904] Started, assigned 3 chunks.
2025-09-03 06:23:58,668 - INFO - [Worker-135722795243200] Started, assigned 3 chunks.
2025-09-03 06:23:58,668 - INFO - [Worker-135722241619648] Started, assigned 3 chunks.
2025-09-03 06:23:58,669 - INFO - [Worker-135722233226944] Started, assigned 3 chunks.
2025-09-03 06:23:58,670 - INFO - [Worker-135722224834240] Started, assigned 3 chunks.
2025-09-03 06:23:58,671 - INFO - [Worker-135722216441536] Started, assigned 3 chunks.
2025-09-03 06:23:58,672 - INFO - [Worker-135722208048832] Started, assigned 3 chunks.
2025-09-03 06:23:58,673 - INFO - [Worker-135722199656128] Started, assigned 3 chunks.
2025-09-03 06:23:58,675 - INFO - [Worker-135722191263424] Started, assigned 3 chunks.
2025-09-03 06:23:58,676 - INFO - [Worker-135721704748736] Started, assigned 3 chunks.
2025-09-03 06:23:58,682 - INFO - [Worker-135721696356032] Started, assigned 3 chunks.
2025-09-03 06:23:58,686 - INFO - [Worker-135721687963328] Started, assigned 3 chunks.
2025-09-03 06:23:58,690 - INFO - [Worker-135721679570624] Started, assigned 3 chunks.
2025-09-03 06:23:58,694 - INFO - [Worker-135721671177920] Started, assigned 3 chunks.
2025-09-03 06:23:58,696 - INFO - [Worker-135721662785216] Started, assigned 3 chunks.
2025-09-03 06:23:58,700 - INFO - [Worker-135721654392512] Started, assigned 3 chunks.
2025-09-03 06:23:58,702 - INFO - [Worker-135721167877824] Started, assigned 3 chunks.
2025-09-03 06:23:58,702 - INFO - [Worker-135721159485120] Started, assigned 3 chunks.
2025-09-03 06:23:58,703 - INFO - [Worker-135721151092416] Started, assigned 3 chunks.
2025-09-03 06:23:58,705 - INFO - [Worker-135721142699712] Started, assigned 3 chunks.
2025-09-03 06:23:58,707 - INFO - [Worker-135721134307008] Started, assigned 3 chunks.
2025-09-03 06:23:58,707 - INFO - [Worker-135721125914304] Started, assigned 3 chunks.
2025-09-03 06:23:58,709 - INFO - [Worker-135721117521600] Started, assigned 3 chunks.
2025-09-03 06:24:19,981 - INFO - ✓ [Worker-135721696356032] Successfully processed chunks and created temp_features_135721696356032.
2025-09-03 06:24:28,314 - INFO - ✓ [Worker-135722828814016] Successfully processed chunks and created temp_features_135722828814016.
2025-09-03 06:24:34,169 - INFO - ✓ [Worker-135722803635904] Successfully processed chunks and created temp_features_135722803635904.
2025-09-03 06:24:41,442 - INFO - ✓ [Worker-135721159485120] Successfully processed chunks and created temp_features_135721159485120.
2025-09-03 06:24:48,575 - INFO - ✓ [Worker-135721167877824] Successfully processed chunks and created temp_features_135721167877824.
2025-09-03 06:24:51,006 - INFO - ✓ [Worker-135721117521600] Successfully processed chunks and created temp_features_135721117521600.
2025-09-03 06:24:55,006 - INFO - ✓ [Worker-135722191263424] Successfully processed chunks and created temp_features_135722191263424.
2025-09-03 06:24:59,272 - INFO - ✓ [Worker-135721662785216] Successfully processed chunks and created temp_features_135721662785216.
2025-09-03 06:25:03,788 - INFO - ✓ [Worker-135723072083648] Successfully processed chunks and created temp_features_135723072083648.
2025-09-03 06:25:09,841 - INFO - ✓ [Worker-135721125914304] Successfully processed chunks and created temp_features_135721125914304.
2025-09-03 06:25:14,742 - INFO - ✓ [Worker-135722241619648] Successfully processed chunks and created temp_features_135722241619648.
2025-09-03 06:25:18,800 - INFO - ✓ [Worker-135721687963328] Successfully processed chunks and created temp_features_135721687963328.
2025-09-03 06:25:23,860 - INFO - ✓ [Worker-135723088869056] Successfully processed chunks and created temp_features_135723088869056.
2025-09-03 06:25:28,112 - INFO - ✓ [Worker-135722795243200] Successfully processed chunks and created temp_features_135722795243200.
2025-09-03 06:25:31,118 - INFO - ✓ [Worker-135721679570624] Successfully processed chunks and created temp_features_135721679570624.
2025-09-03 06:31:55,753 - INFO - Connected to database with high-performance configuration
2025-09-03 06:31:55,753 - INFO - Initialized with 2 parallel workers.
2025-09-03 06:31:55,753 - INFO - ======================================================================
2025-09-03 06:31:55,753 - INFO - STARTING MASSIVELY PARALLEL FEATURE ENGINEERING
2025-09-03 06:31:55,753 - INFO - ======================================================================
2025-09-03 06:31:55,753 - INFO - Creating shared staging table...
2025-09-03 06:31:56,025 - INFO - ✓ Staging table created successfully.
2025-09-03 06:31:56,025 - INFO - Fetching all device-date chunks to be processed...
2025-09-03 06:31:56,458 - INFO - ✓ Divided 96 total chunks into 2 batches for parallel processing.
2025-09-03 06:31:56,458 - INFO - 🚀 Launching parallel workers...
2025-09-03 06:31:56,460 - INFO - [Worker-128130471687872] Started, assigned 48 chunks.
2025-09-03 06:31:56,460 - INFO - [Worker-128130463295168] Started, assigned 48 chunks.
2025-09-03 06:32:13,500 - ERROR - ❌ [Worker-128130463295168] Failed during SQL execution: (psycopg2.errors.DatatypeMismatch) column "raw_event_hash_id" is of type bigint but expression is of type character
LINE 39:   timestamp, ingested_at, raw_event_hash_id, device_id, devi...
                                   ^
HINT:  You will need to rewrite or cast the expression.

[SQL: -- Step A: Create a temporary table with base features FOR THIS CHUNK ONLY.
CREATE TEMP TABLE temp_base_for_worker AS
SELECT
    t.*,
    ST_Z(t.current_position::geometry) AS altitude,
    (t.current_speed < 0.5) AS is_stationary,
    CASE
        WHEN ST_Z(t.current_position::geometry) < 200 THEN 'Loading Zone A'
        WHEN ST_Z(t.current_position::geometry) > 250 THEN 'Dump Zone B'
        ELSE 'Haul Road / Other'
    END AS location_type
FROM "02_raw_telemetry_transformed" t
WHERE t.device_date IN %(chunk_list)s;

-- Index the small temp table for the next step.
CREATE INDEX ON temp_base_for_worker (device_id, device_date, timestamp);

-- Step B: Insert the final calculated features for this chunk into the shared staging table.
INSERT INTO feature_engineering_staging
WITH
  basic_windows AS (
    SELECT
      *,
      LAG(is_stationary, 1, is_stationary) OVER w AS prev_stationary,
      COALESCE(EXTRACT(EPOCH FROM (timestamp - LAG(timestamp, 1) OVER w)), 0) AS time_delta,
      AVG(load_weight) OVER (w ROWS BETWEEN 2 PRECEDING AND 2 FOLLOWING) AS smoothed_load_weight
    FROM temp_base_for_worker
    WINDOW w AS (PARTITION BY device_id, device_date ORDER BY timestamp)
  ),
  duration_prep AS (
    SELECT
      *,
      SUM(CASE WHEN is_stationary != prev_stationary THEN 1 ELSE 0 END) OVER w AS stationary_block_id
    FROM basic_windows
    WINDOW w AS (PARTITION BY device_id, device_date ORDER BY timestamp)
  )
SELECT
  -- Select all final columns and calculate the remaining window functions
  timestamp, ingested_at, raw_event_hash_id, device_id, device_date, system_engaged,
  parking_brake_applied, current_position, current_speed, load_weight, state,
  software_state, prndl, extras, location_type, is_stationary, altitude,
  (altitude - LAG(altitude, 1) OVER w) AS altitude_rate_of_change,
  AVG(current_speed) OVER (w ROWS BETWEEN 2 PRECEDING AND 2 FOLLOWING) AS speed_rolling_avg_5s,
  smoothed_load_weight AS load_weight_smoothed,
  (smoothed_load_weight - LAG(smoothed_load_weight, 1) OVER w) AS load_weight_rate_of_change,
  (STDDEV(load_weight) OVER (PARTITION BY device_id) > 1000) AS has_reliable_payload,
  CASE
    WHEN is_stationary THEN SUM(time_delta) OVER (PARTITION BY device_id, device_date, stationary_block_id ORDER BY timestamp)
    ELSE 0
  END AS time_in_stationary_state
FROM duration_prep
WINDOW w AS (PARTITION BY device_id, device_date ORDER BY timestamp);]
[parameters: {'chunk_list': ('lake-605-8-0898_2025-08-05', 'lake-605-8-0898_2025-08-06', 'lake-605-8-0898_2025-08-07', 'lake-605-8-0898_2025-08-08', 'lake-605-8-0898_2025-08-09', ... (1167 characters truncated) ... e-775g-2-2266_2025-08-08', 'lake-775g-2-2266_2025-08-09', 'lake-775g-2-2266_2025-08-10', 'lake-775g-2-2266_2025-08-11', 'lake-775g-2-2266_2025-08-12')}]
(Background on this error at: https://sqlalche.me/e/20/f405)
2025-09-03 06:32:13,881 - ERROR - ❌ [Worker-128130471687872] Failed during SQL execution: (psycopg2.errors.DatatypeMismatch) column "raw_event_hash_id" is of type bigint but expression is of type character
LINE 39:   timestamp, ingested_at, raw_event_hash_id, device_id, devi...
                                   ^
HINT:  You will need to rewrite or cast the expression.

[SQL: -- Step A: Create a temporary table with base features FOR THIS CHUNK ONLY.
CREATE TEMP TABLE temp_base_for_worker AS
SELECT
    t.*,
    ST_Z(t.current_position::geometry) AS altitude,
    (t.current_speed < 0.5) AS is_stationary,
    CASE
        WHEN ST_Z(t.current_position::geometry) < 200 THEN 'Loading Zone A'
        WHEN ST_Z(t.current_position::geometry) > 250 THEN 'Dump Zone B'
        ELSE 'Haul Road / Other'
    END AS location_type
FROM "02_raw_telemetry_transformed" t
WHERE t.device_date IN %(chunk_list)s;

-- Index the small temp table for the next step.
CREATE INDEX ON temp_base_for_worker (device_id, device_date, timestamp);

-- Step B: Insert the final calculated features for this chunk into the shared staging table.
INSERT INTO feature_engineering_staging
WITH
  basic_windows AS (
    SELECT
      *,
      LAG(is_stationary, 1, is_stationary) OVER w AS prev_stationary,
      COALESCE(EXTRACT(EPOCH FROM (timestamp - LAG(timestamp, 1) OVER w)), 0) AS time_delta,
      AVG(load_weight) OVER (w ROWS BETWEEN 2 PRECEDING AND 2 FOLLOWING) AS smoothed_load_weight
    FROM temp_base_for_worker
    WINDOW w AS (PARTITION BY device_id, device_date ORDER BY timestamp)
  ),
  duration_prep AS (
    SELECT
      *,
      SUM(CASE WHEN is_stationary != prev_stationary THEN 1 ELSE 0 END) OVER w AS stationary_block_id
    FROM basic_windows
    WINDOW w AS (PARTITION BY device_id, device_date ORDER BY timestamp)
  )
SELECT
  -- Select all final columns and calculate the remaining window functions
  timestamp, ingested_at, raw_event_hash_id, device_id, device_date, system_engaged,
  parking_brake_applied, current_position, current_speed, load_weight, state,
  software_state, prndl, extras, location_type, is_stationary, altitude,
  (altitude - LAG(altitude, 1) OVER w) AS altitude_rate_of_change,
  AVG(current_speed) OVER (w ROWS BETWEEN 2 PRECEDING AND 2 FOLLOWING) AS speed_rolling_avg_5s,
  smoothed_load_weight AS load_weight_smoothed,
  (smoothed_load_weight - LAG(smoothed_load_weight, 1) OVER w) AS load_weight_rate_of_change,
  (STDDEV(load_weight) OVER (PARTITION BY device_id) > 1000) AS has_reliable_payload,
  CASE
    WHEN is_stationary THEN SUM(time_delta) OVER (PARTITION BY device_id, device_date, stationary_block_id ORDER BY timestamp)
    ELSE 0
  END AS time_in_stationary_state
FROM duration_prep
WINDOW w AS (PARTITION BY device_id, device_date ORDER BY timestamp);]
[parameters: {'chunk_list': ('lake-605-10-0050_2025-07-30', 'lake-605-10-0050_2025-07-31', 'lake-605-10-0050_2025-08-01', 'lake-605-10-0050_2025-08-02', 'lake-605-10-0050_2025-08 ... (1154 characters truncated) ...  'lake-605-8-0898_2025-07-31', 'lake-605-8-0898_2025-08-01', 'lake-605-8-0898_2025-08-02', 'lake-605-8-0898_2025-08-03', 'lake-605-8-0898_2025-08-04')}]
(Background on this error at: https://sqlalche.me/e/20/f405)
2025-09-03 06:32:13,881 - ERROR - ❌ Pipeline failed: (psycopg2.errors.DatatypeMismatch) column "raw_event_hash_id" is of type bigint but expression is of type character
LINE 39:   timestamp, ingested_at, raw_event_hash_id, device_id, devi...
                                   ^
HINT:  You will need to rewrite or cast the expression.

[SQL: -- Step A: Create a temporary table with base features FOR THIS CHUNK ONLY.
CREATE TEMP TABLE temp_base_for_worker AS
SELECT
    t.*,
    ST_Z(t.current_position::geometry) AS altitude,
    (t.current_speed < 0.5) AS is_stationary,
    CASE
        WHEN ST_Z(t.current_position::geometry) < 200 THEN 'Loading Zone A'
        WHEN ST_Z(t.current_position::geometry) > 250 THEN 'Dump Zone B'
        ELSE 'Haul Road / Other'
    END AS location_type
FROM "02_raw_telemetry_transformed" t
WHERE t.device_date IN %(chunk_list)s;

-- Index the small temp table for the next step.
CREATE INDEX ON temp_base_for_worker (device_id, device_date, timestamp);

-- Step B: Insert the final calculated features for this chunk into the shared staging table.
INSERT INTO feature_engineering_staging
WITH
  basic_windows AS (
    SELECT
      *,
      LAG(is_stationary, 1, is_stationary) OVER w AS prev_stationary,
      COALESCE(EXTRACT(EPOCH FROM (timestamp - LAG(timestamp, 1) OVER w)), 0) AS time_delta,
      AVG(load_weight) OVER (w ROWS BETWEEN 2 PRECEDING AND 2 FOLLOWING) AS smoothed_load_weight
    FROM temp_base_for_worker
    WINDOW w AS (PARTITION BY device_id, device_date ORDER BY timestamp)
  ),
  duration_prep AS (
    SELECT
      *,
      SUM(CASE WHEN is_stationary != prev_stationary THEN 1 ELSE 0 END) OVER w AS stationary_block_id
    FROM basic_windows
    WINDOW w AS (PARTITION BY device_id, device_date ORDER BY timestamp)
  )
SELECT
  -- Select all final columns and calculate the remaining window functions
  timestamp, ingested_at, raw_event_hash_id, device_id, device_date, system_engaged,
  parking_brake_applied, current_position, current_speed, load_weight, state,
  software_state, prndl, extras, location_type, is_stationary, altitude,
  (altitude - LAG(altitude, 1) OVER w) AS altitude_rate_of_change,
  AVG(current_speed) OVER (w ROWS BETWEEN 2 PRECEDING AND 2 FOLLOWING) AS speed_rolling_avg_5s,
  smoothed_load_weight AS load_weight_smoothed,
  (smoothed_load_weight - LAG(smoothed_load_weight, 1) OVER w) AS load_weight_rate_of_change,
  (STDDEV(load_weight) OVER (PARTITION BY device_id) > 1000) AS has_reliable_payload,
  CASE
    WHEN is_stationary THEN SUM(time_delta) OVER (PARTITION BY device_id, device_date, stationary_block_id ORDER BY timestamp)
    ELSE 0
  END AS time_in_stationary_state
FROM duration_prep
WINDOW w AS (PARTITION BY device_id, device_date ORDER BY timestamp);]
[parameters: {'chunk_list': ('lake-605-10-0050_2025-07-30', 'lake-605-10-0050_2025-07-31', 'lake-605-10-0050_2025-08-01', 'lake-605-10-0050_2025-08-02', 'lake-605-10-0050_2025-08 ... (1154 characters truncated) ...  'lake-605-8-0898_2025-07-31', 'lake-605-8-0898_2025-08-01', 'lake-605-8-0898_2025-08-02', 'lake-605-8-0898_2025-08-03', 'lake-605-8-0898_2025-08-04')}]
(Background on this error at: https://sqlalche.me/e/20/f405)
2025-09-03 06:32:13,882 - INFO - Cleaning up staging table...
2025-09-03 06:32:13,891 - INFO - ✓ Staging table cleaned up successfully.
2025-09-03 06:32:53,393 - INFO - Connected to database with high-performance configuration
2025-09-03 06:32:53,393 - INFO - Initialized with 2 parallel workers.
2025-09-03 06:32:53,393 - INFO - ======================================================================
2025-09-03 06:32:53,393 - INFO - STARTING MASSIVELY PARALLEL FEATURE ENGINEERING
2025-09-03 06:32:53,393 - INFO - ======================================================================
2025-09-03 06:32:53,393 - INFO - Creating shared staging table...
2025-09-03 06:32:53,411 - INFO - ✓ Staging table created successfully.
2025-09-03 06:32:53,411 - INFO - Fetching all device-date chunks to be processed...
2025-09-03 06:32:53,852 - INFO - ✓ Divided 96 total chunks into 2 batches for parallel processing.
2025-09-03 06:32:53,852 - INFO - 🚀 Launching parallel workers...
2025-09-03 06:32:53,853 - INFO - [Worker-136480995407552] Started, assigned 48 chunks.
2025-09-03 06:32:53,854 - INFO - [Worker-136480987014848] Started, assigned 48 chunks.
2025-09-03 06:33:10,191 - ERROR - ❌ [Worker-136480987014848] Failed during SQL execution: (psycopg2.errors.DatatypeMismatch) column "current_position" is of type geometry but expression is of type geography
LINE 40:   parking_brake_applied, current_position, current_speed, lo...
                                  ^
HINT:  You will need to rewrite or cast the expression.

[SQL: -- Step A: Create a temporary table with base features FOR THIS CHUNK ONLY.
CREATE TEMP TABLE temp_base_for_worker AS
SELECT
    t.*,
    ST_Z(t.current_position::geometry) AS altitude,
    (t.current_speed < 0.5) AS is_stationary,
    CASE
        WHEN ST_Z(t.current_position::geometry) < 200 THEN 'Loading Zone A'
        WHEN ST_Z(t.current_position::geometry) > 250 THEN 'Dump Zone B'
        ELSE 'Haul Road / Other'
    END AS location_type
FROM "02_raw_telemetry_transformed" t
WHERE t.device_date IN %(chunk_list)s;

-- Index the small temp table for the next step.
CREATE INDEX ON temp_base_for_worker (device_id, device_date, timestamp);

-- Step B: Insert the final calculated features for this chunk into the shared staging table.
INSERT INTO feature_engineering_staging
WITH
  basic_windows AS (
    SELECT
      *,
      LAG(is_stationary, 1, is_stationary) OVER w AS prev_stationary,
      COALESCE(EXTRACT(EPOCH FROM (timestamp - LAG(timestamp, 1) OVER w)), 0) AS time_delta,
      AVG(load_weight) OVER (w ROWS BETWEEN 2 PRECEDING AND 2 FOLLOWING) AS smoothed_load_weight
    FROM temp_base_for_worker
    WINDOW w AS (PARTITION BY device_id, device_date ORDER BY timestamp)
  ),
  duration_prep AS (
    SELECT
      *,
      SUM(CASE WHEN is_stationary != prev_stationary THEN 1 ELSE 0 END) OVER w AS stationary_block_id
    FROM basic_windows
    WINDOW w AS (PARTITION BY device_id, device_date ORDER BY timestamp)
  )
SELECT
  -- Select all final columns and calculate the remaining window functions
  timestamp, ingested_at, raw_event_hash_id, device_id, device_date, system_engaged,
  parking_brake_applied, current_position, current_speed, load_weight, state,
  software_state, prndl, extras, location_type, is_stationary, altitude,
  (altitude - LAG(altitude, 1) OVER w) AS altitude_rate_of_change,
  AVG(current_speed) OVER (w ROWS BETWEEN 2 PRECEDING AND 2 FOLLOWING) AS speed_rolling_avg_5s,
  smoothed_load_weight AS load_weight_smoothed,
  (smoothed_load_weight - LAG(smoothed_load_weight, 1) OVER w) AS load_weight_rate_of_change,
  (STDDEV(load_weight) OVER (PARTITION BY device_id) > 1000) AS has_reliable_payload,
  CASE
    WHEN is_stationary THEN SUM(time_delta) OVER (PARTITION BY device_id, device_date, stationary_block_id ORDER BY timestamp)
    ELSE 0
  END AS time_in_stationary_state
FROM duration_prep
WINDOW w AS (PARTITION BY device_id, device_date ORDER BY timestamp);]
[parameters: {'chunk_list': ('lake-605-8-0898_2025-08-05', 'lake-605-8-0898_2025-08-06', 'lake-605-8-0898_2025-08-07', 'lake-605-8-0898_2025-08-08', 'lake-605-8-0898_2025-08-09', ... (1167 characters truncated) ... e-775g-2-2266_2025-08-08', 'lake-775g-2-2266_2025-08-09', 'lake-775g-2-2266_2025-08-10', 'lake-775g-2-2266_2025-08-11', 'lake-775g-2-2266_2025-08-12')}]
(Background on this error at: https://sqlalche.me/e/20/f405)
2025-09-03 06:33:10,858 - ERROR - ❌ [Worker-136480995407552] Failed during SQL execution: (psycopg2.errors.DatatypeMismatch) column "current_position" is of type geometry but expression is of type geography
LINE 40:   parking_brake_applied, current_position, current_speed, lo...
                                  ^
HINT:  You will need to rewrite or cast the expression.

[SQL: -- Step A: Create a temporary table with base features FOR THIS CHUNK ONLY.
CREATE TEMP TABLE temp_base_for_worker AS
SELECT
    t.*,
    ST_Z(t.current_position::geometry) AS altitude,
    (t.current_speed < 0.5) AS is_stationary,
    CASE
        WHEN ST_Z(t.current_position::geometry) < 200 THEN 'Loading Zone A'
        WHEN ST_Z(t.current_position::geometry) > 250 THEN 'Dump Zone B'
        ELSE 'Haul Road / Other'
    END AS location_type
FROM "02_raw_telemetry_transformed" t
WHERE t.device_date IN %(chunk_list)s;

-- Index the small temp table for the next step.
CREATE INDEX ON temp_base_for_worker (device_id, device_date, timestamp);

-- Step B: Insert the final calculated features for this chunk into the shared staging table.
INSERT INTO feature_engineering_staging
WITH
  basic_windows AS (
    SELECT
      *,
      LAG(is_stationary, 1, is_stationary) OVER w AS prev_stationary,
      COALESCE(EXTRACT(EPOCH FROM (timestamp - LAG(timestamp, 1) OVER w)), 0) AS time_delta,
      AVG(load_weight) OVER (w ROWS BETWEEN 2 PRECEDING AND 2 FOLLOWING) AS smoothed_load_weight
    FROM temp_base_for_worker
    WINDOW w AS (PARTITION BY device_id, device_date ORDER BY timestamp)
  ),
  duration_prep AS (
    SELECT
      *,
      SUM(CASE WHEN is_stationary != prev_stationary THEN 1 ELSE 0 END) OVER w AS stationary_block_id
    FROM basic_windows
    WINDOW w AS (PARTITION BY device_id, device_date ORDER BY timestamp)
  )
SELECT
  -- Select all final columns and calculate the remaining window functions
  timestamp, ingested_at, raw_event_hash_id, device_id, device_date, system_engaged,
  parking_brake_applied, current_position, current_speed, load_weight, state,
  software_state, prndl, extras, location_type, is_stationary, altitude,
  (altitude - LAG(altitude, 1) OVER w) AS altitude_rate_of_change,
  AVG(current_speed) OVER (w ROWS BETWEEN 2 PRECEDING AND 2 FOLLOWING) AS speed_rolling_avg_5s,
  smoothed_load_weight AS load_weight_smoothed,
  (smoothed_load_weight - LAG(smoothed_load_weight, 1) OVER w) AS load_weight_rate_of_change,
  (STDDEV(load_weight) OVER (PARTITION BY device_id) > 1000) AS has_reliable_payload,
  CASE
    WHEN is_stationary THEN SUM(time_delta) OVER (PARTITION BY device_id, device_date, stationary_block_id ORDER BY timestamp)
    ELSE 0
  END AS time_in_stationary_state
FROM duration_prep
WINDOW w AS (PARTITION BY device_id, device_date ORDER BY timestamp);]
[parameters: {'chunk_list': ('lake-605-10-0050_2025-07-30', 'lake-605-10-0050_2025-07-31', 'lake-605-10-0050_2025-08-01', 'lake-605-10-0050_2025-08-02', 'lake-605-10-0050_2025-08 ... (1154 characters truncated) ...  'lake-605-8-0898_2025-07-31', 'lake-605-8-0898_2025-08-01', 'lake-605-8-0898_2025-08-02', 'lake-605-8-0898_2025-08-03', 'lake-605-8-0898_2025-08-04')}]
(Background on this error at: https://sqlalche.me/e/20/f405)
2025-09-03 06:33:10,859 - ERROR - ❌ Pipeline failed: (psycopg2.errors.DatatypeMismatch) column "current_position" is of type geometry but expression is of type geography
LINE 40:   parking_brake_applied, current_position, current_speed, lo...
                                  ^
HINT:  You will need to rewrite or cast the expression.

[SQL: -- Step A: Create a temporary table with base features FOR THIS CHUNK ONLY.
CREATE TEMP TABLE temp_base_for_worker AS
SELECT
    t.*,
    ST_Z(t.current_position::geometry) AS altitude,
    (t.current_speed < 0.5) AS is_stationary,
    CASE
        WHEN ST_Z(t.current_position::geometry) < 200 THEN 'Loading Zone A'
        WHEN ST_Z(t.current_position::geometry) > 250 THEN 'Dump Zone B'
        ELSE 'Haul Road / Other'
    END AS location_type
FROM "02_raw_telemetry_transformed" t
WHERE t.device_date IN %(chunk_list)s;

-- Index the small temp table for the next step.
CREATE INDEX ON temp_base_for_worker (device_id, device_date, timestamp);

-- Step B: Insert the final calculated features for this chunk into the shared staging table.
INSERT INTO feature_engineering_staging
WITH
  basic_windows AS (
    SELECT
      *,
      LAG(is_stationary, 1, is_stationary) OVER w AS prev_stationary,
      COALESCE(EXTRACT(EPOCH FROM (timestamp - LAG(timestamp, 1) OVER w)), 0) AS time_delta,
      AVG(load_weight) OVER (w ROWS BETWEEN 2 PRECEDING AND 2 FOLLOWING) AS smoothed_load_weight
    FROM temp_base_for_worker
    WINDOW w AS (PARTITION BY device_id, device_date ORDER BY timestamp)
  ),
  duration_prep AS (
    SELECT
      *,
      SUM(CASE WHEN is_stationary != prev_stationary THEN 1 ELSE 0 END) OVER w AS stationary_block_id
    FROM basic_windows
    WINDOW w AS (PARTITION BY device_id, device_date ORDER BY timestamp)
  )
SELECT
  -- Select all final columns and calculate the remaining window functions
  timestamp, ingested_at, raw_event_hash_id, device_id, device_date, system_engaged,
  parking_brake_applied, current_position, current_speed, load_weight, state,
  software_state, prndl, extras, location_type, is_stationary, altitude,
  (altitude - LAG(altitude, 1) OVER w) AS altitude_rate_of_change,
  AVG(current_speed) OVER (w ROWS BETWEEN 2 PRECEDING AND 2 FOLLOWING) AS speed_rolling_avg_5s,
  smoothed_load_weight AS load_weight_smoothed,
  (smoothed_load_weight - LAG(smoothed_load_weight, 1) OVER w) AS load_weight_rate_of_change,
  (STDDEV(load_weight) OVER (PARTITION BY device_id) > 1000) AS has_reliable_payload,
  CASE
    WHEN is_stationary THEN SUM(time_delta) OVER (PARTITION BY device_id, device_date, stationary_block_id ORDER BY timestamp)
    ELSE 0
  END AS time_in_stationary_state
FROM duration_prep
WINDOW w AS (PARTITION BY device_id, device_date ORDER BY timestamp);]
[parameters: {'chunk_list': ('lake-605-10-0050_2025-07-30', 'lake-605-10-0050_2025-07-31', 'lake-605-10-0050_2025-08-01', 'lake-605-10-0050_2025-08-02', 'lake-605-10-0050_2025-08 ... (1154 characters truncated) ...  'lake-605-8-0898_2025-07-31', 'lake-605-8-0898_2025-08-01', 'lake-605-8-0898_2025-08-02', 'lake-605-8-0898_2025-08-03', 'lake-605-8-0898_2025-08-04')}]
(Background on this error at: https://sqlalche.me/e/20/f405)
2025-09-03 06:33:10,859 - INFO - Cleaning up staging table...
2025-09-03 06:33:10,867 - INFO - ✓ Staging table cleaned up successfully.
2025-09-03 06:33:45,849 - INFO - Connected to database with high-performance configuration
2025-09-03 06:33:45,850 - INFO - Initialized with 2 parallel workers.
2025-09-03 06:33:45,850 - INFO - ======================================================================
2025-09-03 06:33:45,850 - INFO - STARTING MASSIVELY PARALLEL FEATURE ENGINEERING
2025-09-03 06:33:45,850 - INFO - ======================================================================
2025-09-03 06:33:45,850 - INFO - Creating shared staging table...
2025-09-03 06:33:45,869 - INFO - ✓ Staging table created successfully.
2025-09-03 06:33:45,869 - INFO - Fetching all device-date chunks to be processed...
2025-09-03 06:33:46,307 - INFO - ✓ Divided 96 total chunks into 2 batches for parallel processing.
2025-09-03 06:33:46,308 - INFO - 🚀 Launching parallel workers...
2025-09-03 06:33:46,309 - INFO - [Worker-133643051726528] Started, assigned 48 chunks.
2025-09-03 06:33:46,309 - INFO - [Worker-133643043333824] Started, assigned 48 chunks.
2025-09-03 06:35:12,160 - INFO - ✓ [Worker-133643043333824] Successfully processed 48 chunks and inserted into staging table.
2025-09-03 06:35:12,656 - INFO - ✓ [Worker-133643051726528] Successfully processed 48 chunks and inserted into staging table.
2025-09-03 06:35:12,656 - INFO - ✓ All parallel workers completed successfully.
2025-09-03 06:35:12,657 - INFO - 🤝 Assembling final table from staging data...
2025-09-03 06:35:12,657 - INFO - ✓ Loaded SQL script: final_assembly.sql
2025-09-03 06:36:28,101 - ERROR - ❌ Pipeline failed: (psycopg2.errors.ActiveSqlTransaction) CREATE INDEX CONCURRENTLY cannot run inside a transaction block

[SQL: -- This script runs once at the end to assemble the results from the staging table.
DROP TABLE IF EXISTS "03_primary_feature_table";

CREATE TABLE "03_primary_feature_table" AS
SELECT
    -- Original & Engineered Features
    timestamp, ingested_at, raw_event_hash_id, device_id, device_date, system_engaged,
    parking_brake_applied, current_position, current_speed, load_weight, state,
    software_state, prndl, extras, location_type, is_stationary, altitude,
    altitude_rate_of_change, speed_rolling_avg_5s, load_weight_smoothed,
    load_weight_rate_of_change, has_reliable_payload, time_in_stationary_state,
    
    -- PRNDL one-hot encoding
    (prndl = 'park') AS prndl_park,
    (prndl = 'reverse') AS prndl_reverse,
    (prndl = 'neutral') AS prndl_neutral,
    (prndl = 'drive') AS prndl_drive,
    (prndl = 'unknown') AS prndl_unknown,
    
    -- Interaction Features
    (is_stationary AND prndl = 'neutral' AND parking_brake_applied) AS is_ready_for_load,
    (location_type = 'Haul Road / Other' AND NOT is_stationary) AS is_hauling,
    (is_stationary AND location_type LIKE '%%Loading%%') AS is_loading_position,
    (is_stationary AND location_type LIKE '%%Dump%%') AS is_dumping_position,
    (load_weight_smoothed > 50000) AS is_heavy_load

FROM feature_engineering_staging
ORDER BY device_id, timestamp;

-- Apply final optimizations
ALTER TABLE "03_primary_feature_table" ADD PRIMARY KEY (raw_event_hash_id);

CREATE INDEX CONCURRENTLY idx_03_primary_device_timestamp 
ON "03_primary_feature_table" (device_id, timestamp);

CREATE INDEX CONCURRENTLY idx_03_primary_stationary 
ON "03_primary_feature_table" (is_stationary) 
WHERE is_stationary = true;

CREATE INDEX CONCURRENTLY idx_03_primary_location 
ON "03_primary_feature_table" (location_type);

CREATE INDEX CONCURRENTLY idx_03_primary_heavy_load
ON "03_primary_feature_table" (is_heavy_load)
WHERE is_heavy_load = true;

CREATE INDEX CONCURRENTLY idx_03_primary_ready_for_load
ON "03_primary_feature_table" (is_ready_for_load)
WHERE is_ready_for_load = true;

ANALYZE "03_primary_feature_table";]
(Background on this error at: https://sqlalche.me/e/20/2j85)
2025-09-03 06:36:28,102 - INFO - Cleaning up staging table...
2025-09-03 06:36:29,736 - INFO - ✓ Staging table cleaned up successfully.
2025-09-03 06:36:57,235 - INFO - Connected to database with high-performance configuration
2025-09-03 06:36:57,236 - INFO - Initialized with 2 parallel workers.
2025-09-03 06:36:57,236 - INFO - ======================================================================
2025-09-03 06:36:57,236 - INFO - STARTING MASSIVELY PARALLEL FEATURE ENGINEERING
2025-09-03 06:36:57,236 - INFO - ======================================================================
2025-09-03 06:36:57,236 - INFO - Creating shared staging table...
2025-09-03 06:36:57,253 - INFO - ✓ Staging table created successfully.
2025-09-03 06:36:57,253 - INFO - Fetching all device-date chunks to be processed...
2025-09-03 06:36:57,686 - INFO - ✓ Divided 96 total chunks into 2 batches for parallel processing.
2025-09-03 06:36:57,686 - INFO - 🚀 Launching parallel workers...
2025-09-03 06:36:57,688 - INFO - [Worker-131127549359808] Started, assigned 48 chunks.
2025-09-03 06:36:57,688 - INFO - [Worker-131127540967104] Started, assigned 48 chunks.
2025-09-03 06:40:53,966 - INFO - Connected to database with high-performance configuration
2025-09-03 06:40:53,966 - INFO - Initialized with 32 parallel workers.
2025-09-03 06:40:53,966 - INFO - ======================================================================
2025-09-03 06:40:53,966 - INFO - STARTING MASSIVELY PARALLEL FEATURE ENGINEERING
2025-09-03 06:40:53,966 - INFO - ======================================================================
2025-09-03 06:40:53,966 - INFO - Creating shared staging table...
2025-09-03 06:40:53,984 - INFO - ✓ Staging table created successfully.
2025-09-03 06:40:53,984 - INFO - Fetching all device-date chunks to be processed...
2025-09-03 06:40:54,419 - INFO - ✓ Divided 96 total chunks into 32 batches for parallel processing.
2025-09-03 06:40:54,419 - INFO - 🚀 Launching parallel workers...
2025-09-03 06:40:54,420 - INFO - [Worker-134046365513408] Started, assigned 3 chunks.
2025-09-03 06:40:54,421 - INFO - [Worker-134046357120704] Started, assigned 3 chunks.
2025-09-03 06:40:54,422 - INFO - [Worker-134046348728000] Started, assigned 3 chunks.
2025-09-03 06:40:54,423 - INFO - [Worker-134046130632384] Started, assigned 3 chunks.
2025-09-03 06:40:54,423 - INFO - [Worker-134046122239680] Started, assigned 3 chunks.
2025-09-03 06:40:54,424 - INFO - [Worker-134046113846976] Started, assigned 3 chunks.
2025-09-03 06:40:54,426 - INFO - [Worker-134046105454272] Started, assigned 3 chunks.
2025-09-03 06:40:54,427 - INFO - [Worker-134046097061568] Started, assigned 3 chunks.
2025-09-03 06:40:54,428 - INFO - [Worker-134046088668864] Started, assigned 3 chunks.
2025-09-03 06:40:54,428 - INFO - [Worker-134046080276160] Started, assigned 3 chunks.
2025-09-03 06:40:54,429 - INFO - [Worker-134045593761472] Started, assigned 3 chunks.
2025-09-03 06:40:54,430 - INFO - [Worker-134045585368768] Started, assigned 3 chunks.
2025-09-03 06:40:54,431 - INFO - [Worker-134045576976064] Started, assigned 3 chunks.
2025-09-03 06:40:54,432 - INFO - [Worker-134045568583360] Started, assigned 3 chunks.
2025-09-03 06:40:54,433 - INFO - [Worker-134045560190656] Started, assigned 3 chunks.
2025-09-03 06:40:54,434 - INFO - [Worker-134045551797952] Started, assigned 3 chunks.
2025-09-03 06:40:54,436 - INFO - [Worker-134045543405248] Started, assigned 3 chunks.
2025-09-03 06:40:54,438 - INFO - [Worker-134045056890560] Started, assigned 3 chunks.
2025-09-03 06:40:54,439 - INFO - [Worker-134045048497856] Started, assigned 3 chunks.
2025-09-03 06:40:54,440 - INFO - [Worker-134045040105152] Started, assigned 3 chunks.
2025-09-03 06:40:54,442 - INFO - [Worker-134045031712448] Started, assigned 3 chunks.
2025-09-03 06:40:54,445 - INFO - [Worker-134045023319744] Started, assigned 3 chunks.
2025-09-03 06:40:54,447 - INFO - [Worker-134045014927040] Started, assigned 3 chunks.
2025-09-03 06:40:54,449 - INFO - [Worker-134045006534336] Started, assigned 3 chunks.
2025-09-03 06:40:54,456 - INFO - [Worker-134044520019648] Started, assigned 3 chunks.
2025-09-03 06:40:54,459 - INFO - [Worker-134044511626944] Started, assigned 3 chunks.
2025-09-03 06:40:54,464 - INFO - [Worker-134044503234240] Started, assigned 3 chunks.
2025-09-03 06:40:54,465 - INFO - [Worker-134044494841536] Started, assigned 3 chunks.
2025-09-03 06:40:54,468 - INFO - [Worker-134044486448832] Started, assigned 3 chunks.
2025-09-03 06:40:54,469 - INFO - [Worker-134044478056128] Started, assigned 3 chunks.
2025-09-03 06:40:54,471 - INFO - [Worker-134044469663424] Started, assigned 3 chunks.
2025-09-03 06:40:54,473 - INFO - [Worker-134043983148736] Started, assigned 3 chunks.
2025-09-03 06:40:58,357 - INFO - ✓ [Worker-134045040105152] Successfully processed 3 chunks and inserted into staging table.
2025-09-03 06:41:12,023 - INFO - ✓ [Worker-134043983148736] Successfully processed 3 chunks and inserted into staging table.
2025-09-03 06:41:19,284 - INFO - ✓ [Worker-134045568583360] Successfully processed 3 chunks and inserted into staging table.
2025-09-03 06:41:24,611 - INFO - ✓ [Worker-134046088668864] Successfully processed 3 chunks and inserted into staging table.
2025-09-03 06:41:25,776 - INFO - ✓ [Worker-134045023319744] Successfully processed 3 chunks and inserted into staging table.
2025-09-03 06:41:31,174 - INFO - ✓ [Worker-134045560190656] Successfully processed 3 chunks and inserted into staging table.
2025-09-03 06:41:33,475 - INFO - ✓ [Worker-134046105454272] Successfully processed 3 chunks and inserted into staging table.
2025-09-03 06:41:33,699 - INFO - ✓ [Worker-134046130632384] Successfully processed 3 chunks and inserted into staging table.
2025-09-03 06:41:35,332 - INFO - ✓ [Worker-134045048497856] Successfully processed 3 chunks and inserted into staging table.
2025-09-03 06:41:35,516 - INFO - ✓ [Worker-134044478056128] Successfully processed 3 chunks and inserted into staging table.
2025-09-03 06:41:35,739 - INFO - ✓ [Worker-134045014927040] Successfully processed 3 chunks and inserted into staging table.
2025-09-03 06:41:35,977 - INFO - ✓ [Worker-134046357120704] Successfully processed 3 chunks and inserted into staging table.
2025-09-03 06:41:36,769 - INFO - ✓ [Worker-134046122239680] Successfully processed 3 chunks and inserted into staging table.
2025-09-03 06:41:37,457 - INFO - ✓ [Worker-134046348728000] Successfully processed 3 chunks and inserted into staging table.
2025-09-03 06:41:37,546 - INFO - ✓ [Worker-134045551797952] Successfully processed 3 chunks and inserted into staging table.
2025-09-03 06:41:37,624 - INFO - ✓ [Worker-134045056890560] Successfully processed 3 chunks and inserted into staging table.
2025-09-03 06:41:37,685 - INFO - ✓ [Worker-134045543405248] Successfully processed 3 chunks and inserted into staging table.
2025-09-03 06:41:37,761 - INFO - ✓ [Worker-134044520019648] Successfully processed 3 chunks and inserted into staging table.
2025-09-03 06:41:37,761 - INFO - ✓ [Worker-134044486448832] Successfully processed 3 chunks and inserted into staging table.
2025-09-03 06:41:37,871 - INFO - ✓ [Worker-134046080276160] Successfully processed 3 chunks and inserted into staging table.
2025-09-03 06:41:38,249 - INFO - ✓ [Worker-134045031712448] Successfully processed 3 chunks and inserted into staging table.
2025-09-03 06:41:38,479 - INFO - ✓ [Worker-134046097061568] Successfully processed 3 chunks and inserted into staging table.
2025-09-03 06:41:38,707 - INFO - ✓ [Worker-134045593761472] Successfully processed 3 chunks and inserted into staging table.
2025-09-03 06:41:38,779 - INFO - ✓ [Worker-134046365513408] Successfully processed 3 chunks and inserted into staging table.
2025-09-03 06:41:38,883 - INFO - ✓ [Worker-134046113846976] Successfully processed 3 chunks and inserted into staging table.
2025-09-03 06:41:38,949 - INFO - ✓ [Worker-134044503234240] Successfully processed 3 chunks and inserted into staging table.
2025-09-03 06:41:38,950 - INFO - ✓ [Worker-134045006534336] Successfully processed 3 chunks and inserted into staging table.
2025-09-03 06:41:39,048 - INFO - ✓ [Worker-134044494841536] Successfully processed 3 chunks and inserted into staging table.
2025-09-03 06:41:39,469 - INFO - ✓ [Worker-134045585368768] Successfully processed 3 chunks and inserted into staging table.
2025-09-03 06:41:40,257 - INFO - ✓ [Worker-134044469663424] Successfully processed 3 chunks and inserted into staging table.
2025-09-03 06:41:40,388 - INFO - ✓ [Worker-134045576976064] Successfully processed 3 chunks and inserted into staging table.
2025-09-03 06:41:40,541 - INFO - ✓ [Worker-134044511626944] Successfully processed 3 chunks and inserted into staging table.
2025-09-03 06:41:40,546 - INFO - ✓ All parallel workers completed successfully.
2025-09-03 06:41:40,546 - INFO - 🤝 Assembling final table from staging data...
2025-09-03 06:41:40,546 - INFO - ✓ Loaded SQL script: final_assembly.sql
2025-09-03 06:45:14,951 - INFO - Connected to database with high-performance configuration
2025-09-03 06:45:14,951 - INFO - Initialized with 32 parallel workers.
2025-09-03 06:45:14,951 - INFO - ======================================================================
2025-09-03 06:45:14,952 - INFO - STARTING MASSIVELY PARALLEL FEATURE ENGINEERING
2025-09-03 06:45:14,952 - INFO - ======================================================================
2025-09-03 06:45:14,952 - INFO - Creating shared staging table...
2025-09-03 06:45:14,973 - INFO - ✓ Staging table created successfully.
2025-09-03 06:45:14,973 - INFO - Fetching all device-date chunks to be processed...
2025-09-03 06:45:15,477 - INFO - ✓ Divided 96 total chunks into 32 batches for parallel processing.
2025-09-03 06:45:15,477 - INFO - 🚀 Launching parallel workers...
2025-09-03 06:45:15,478 - INFO - [Worker-133241006716608] Started, assigned 3 chunks.
2025-09-03 06:45:15,479 - INFO - [Worker-133240998323904] Started, assigned 3 chunks.
2025-09-03 06:45:15,480 - INFO - [Worker-133240989931200] Started, assigned 3 chunks.
2025-09-03 06:45:15,481 - INFO - [Worker-133240981538496] Started, assigned 3 chunks.
2025-09-03 06:45:15,482 - INFO - [Worker-133240973145792] Started, assigned 3 chunks.
2025-09-03 06:45:15,483 - INFO - [Worker-133240622937792] Started, assigned 3 chunks.
2025-09-03 06:45:15,484 - INFO - [Worker-133240614545088] Started, assigned 3 chunks.
2025-09-03 06:45:15,485 - INFO - [Worker-133240606152384] Started, assigned 3 chunks.
2025-09-03 06:45:15,485 - INFO - [Worker-133240597759680] Started, assigned 3 chunks.
2025-09-03 06:45:15,486 - INFO - [Worker-133240589366976] Started, assigned 3 chunks.
2025-09-03 06:45:15,487 - INFO - [Worker-133240580974272] Started, assigned 3 chunks.
2025-09-03 06:45:15,488 - INFO - [Worker-133240572581568] Started, assigned 3 chunks.
2025-09-03 06:45:15,489 - INFO - [Worker-133240086066880] Started, assigned 3 chunks.
2025-09-03 06:45:15,489 - INFO - [Worker-133240077674176] Started, assigned 3 chunks.
2025-09-03 06:45:15,491 - INFO - [Worker-133240069281472] Started, assigned 3 chunks.
2025-09-03 06:45:15,492 - INFO - [Worker-133240060888768] Started, assigned 3 chunks.
2025-09-03 06:45:15,493 - INFO - [Worker-133240052496064] Started, assigned 3 chunks.
2025-09-03 06:45:15,494 - INFO - [Worker-133240044103360] Started, assigned 3 chunks.
2025-09-03 06:45:15,497 - INFO - [Worker-133240035710656] Started, assigned 3 chunks.
2025-09-03 06:45:15,499 - INFO - [Worker-133239549195968] Started, assigned 3 chunks.
2025-09-03 06:45:15,503 - INFO - [Worker-133239540803264] Started, assigned 3 chunks.
2025-09-03 06:45:15,511 - INFO - [Worker-133239532410560] Started, assigned 3 chunks.
2025-09-03 06:45:15,511 - INFO - [Worker-133239524017856] Started, assigned 3 chunks.
2025-09-03 06:45:15,514 - INFO - [Worker-133239515625152] Started, assigned 3 chunks.
2025-09-03 06:45:15,515 - INFO - [Worker-133239507232448] Started, assigned 3 chunks.
2025-09-03 06:45:15,518 - INFO - [Worker-133239498839744] Started, assigned 3 chunks.
2025-09-03 06:45:15,520 - INFO - [Worker-133239012325056] Started, assigned 3 chunks.
2025-09-03 06:45:15,521 - INFO - [Worker-133239003932352] Started, assigned 3 chunks.
2025-09-03 06:45:15,523 - INFO - [Worker-133238995539648] Started, assigned 3 chunks.
2025-09-03 06:45:15,525 - INFO - [Worker-133238987146944] Started, assigned 3 chunks.
2025-09-03 06:45:15,527 - INFO - [Worker-133238978754240] Started, assigned 3 chunks.
2025-09-03 06:45:15,529 - INFO - [Worker-133238970361536] Started, assigned 3 chunks.
2025-09-03 06:45:18,622 - INFO - ✓ [Worker-133239549195968] Successfully processed 3 chunks and inserted into staging table.
2025-09-03 06:45:31,347 - INFO - ✓ [Worker-133238970361536] Successfully processed 3 chunks and inserted into staging table.
2025-09-03 06:45:42,652 - INFO - ✓ [Worker-133240077674176] Successfully processed 3 chunks and inserted into staging table.
2025-09-03 06:45:48,410 - INFO - ✓ [Worker-133240597759680] Successfully processed 3 chunks and inserted into staging table.
2025-09-03 06:45:49,969 - INFO - ✓ [Worker-133239532410560] Successfully processed 3 chunks and inserted into staging table.
2025-09-03 06:45:54,053 - INFO - ✓ [Worker-133240069281472] Successfully processed 3 chunks and inserted into staging table.
2025-09-03 06:45:56,215 - INFO - ✓ [Worker-133240614545088] Successfully processed 3 chunks and inserted into staging table.
2025-09-03 06:45:58,170 - INFO - ✓ [Worker-133240981538496] Successfully processed 3 chunks and inserted into staging table.
2025-09-03 06:45:58,406 - INFO - ✓ [Worker-133238987146944] Successfully processed 3 chunks and inserted into staging table.
2025-09-03 06:45:58,549 - INFO - ✓ [Worker-133240035710656] Successfully processed 3 chunks and inserted into staging table.
2025-09-03 06:45:59,033 - INFO - ✓ [Worker-133240998323904] Successfully processed 3 chunks and inserted into staging table.
2025-09-03 06:45:59,160 - INFO - ✓ [Worker-133240973145792] Successfully processed 3 chunks and inserted into staging table.
2025-09-03 06:46:00,079 - INFO - ✓ [Worker-133240989931200] Successfully processed 3 chunks and inserted into staging table.
2025-09-03 06:46:00,080 - INFO - ✓ [Worker-133239524017856] Successfully processed 3 chunks and inserted into staging table.
2025-09-03 06:46:00,191 - INFO - ✓ [Worker-133240060888768] Successfully processed 3 chunks and inserted into staging table.
2025-09-03 06:46:00,550 - INFO - ✓ [Worker-133240052496064] Successfully processed 3 chunks and inserted into staging table.
2025-09-03 06:46:00,847 - INFO - ✓ [Worker-133239507232448] Successfully processed 3 chunks and inserted into staging table.
2025-09-03 06:46:00,956 - INFO - ✓ [Worker-133238995539648] Successfully processed 3 chunks and inserted into staging table.
2025-09-03 06:46:01,024 - INFO - ✓ [Worker-133240589366976] Successfully processed 3 chunks and inserted into staging table.
2025-09-03 06:46:01,438 - INFO - ✓ [Worker-133240044103360] Successfully processed 3 chunks and inserted into staging table.
2025-09-03 06:46:01,540 - INFO - ✓ [Worker-133239515625152] Successfully processed 3 chunks and inserted into staging table.
2025-09-03 06:46:01,612 - INFO - ✓ [Worker-133240622937792] Successfully processed 3 chunks and inserted into staging table.
2025-09-03 06:46:01,720 - INFO - ✓ [Worker-133239540803264] Successfully processed 3 chunks and inserted into staging table.
2025-09-03 06:46:01,779 - INFO - ✓ [Worker-133240580974272] Successfully processed 3 chunks and inserted into staging table.
2025-09-03 06:46:02,108 - INFO - ✓ [Worker-133239003932352] Successfully processed 3 chunks and inserted into staging table.
2025-09-03 06:46:02,109 - INFO - ✓ [Worker-133241006716608] Successfully processed 3 chunks and inserted into staging table.
2025-09-03 06:46:02,109 - INFO - ✓ [Worker-133239012325056] Successfully processed 3 chunks and inserted into staging table.
2025-09-03 06:46:02,165 - INFO - ✓ [Worker-133240606152384] Successfully processed 3 chunks and inserted into staging table.
2025-09-03 06:46:02,538 - INFO - ✓ [Worker-133240572581568] Successfully processed 3 chunks and inserted into staging table.
2025-09-03 06:46:03,451 - INFO - ✓ [Worker-133238978754240] Successfully processed 3 chunks and inserted into staging table.
2025-09-03 06:46:03,608 - INFO - ✓ [Worker-133240086066880] Successfully processed 3 chunks and inserted into staging table.
2025-09-03 06:46:03,779 - INFO - ✓ [Worker-133239498839744] Successfully processed 3 chunks and inserted into staging table.
2025-09-03 06:46:03,784 - INFO - ✓ All parallel workers completed successfully.
2025-09-03 06:46:03,784 - INFO - 🤝 Assembling final table from staging data...
2025-09-03 06:46:03,784 - INFO - ✓ Loaded SQL script: final_assembly.sql
2025-09-03 06:47:35,096 - INFO - ✓ Final feature table assembled successfully.
2025-09-03 06:47:35,097 - INFO - Cleaning up staging table...
2025-09-03 06:47:36,716 - INFO - ✓ Staging table cleaned up successfully.
2025-09-03 06:47:36,716 - INFO - ======================================================================
2025-09-03 06:47:36,716 - INFO - 🎉 PIPELINE COMPLETED SUCCESSFULLY IN 141.8 SECONDS
2025-09-03 06:47:36,716 - INFO - ======================================================================
2025-09-03 07:14:16,855 - INFO - Connected to database with high-performance configuration
2025-09-03 07:14:16,855 - INFO - Initialized with 1 parallel workers.
2025-09-03 07:14:16,855 - INFO - Fetching all device-date chunks to be processed...
2025-09-03 07:14:17,378 - INFO - ✓ Divided 96 total chunks into 1 batches for parallel processing.
2025-09-03 07:16:44,482 - INFO - Connected to database with high-performance configuration
2025-09-03 07:16:44,482 - INFO - Initialized with 32 parallel workers.
2025-09-03 07:16:44,482 - INFO - ======================================================================
2025-09-03 07:16:44,482 - INFO - STARTING MASSIVELY PARALLEL FEATURE ENGINEERING
2025-09-03 07:16:44,482 - INFO - ======================================================================
2025-09-03 07:16:44,482 - INFO - Creating shared staging table...
2025-09-03 07:16:44,500 - INFO - ✓ Staging table created successfully.
2025-09-03 07:16:44,500 - INFO - Fetching all device-date chunks to be processed...
2025-09-03 07:16:45,014 - INFO - ✓ Divided 96 total chunks into 32 batches for parallel processing.
2025-09-03 07:16:45,014 - INFO - 🚀 Launching parallel workers...
2025-09-03 07:16:45,015 - INFO - [Worker-135551801882304] Started, assigned 3 chunks.
2025-09-03 07:16:45,016 - INFO - [Worker-135551717996224] Started, assigned 3 chunks.
2025-09-03 07:16:45,017 - INFO - [Worker-135551709603520] Started, assigned 3 chunks.
2025-09-03 07:16:45,017 - INFO - [Worker-135551701210816] Started, assigned 3 chunks.
2025-09-03 07:16:45,019 - INFO - [Worker-135551692818112] Started, assigned 3 chunks.
2025-09-03 07:16:45,019 - INFO - [Worker-135551684425408] Started, assigned 3 chunks.
2025-09-03 07:16:45,020 - INFO - [Worker-135551676032704] Started, assigned 3 chunks.
2025-09-03 07:16:45,021 - INFO - [Worker-135551667640000] Started, assigned 3 chunks.
2025-09-03 07:16:45,022 - INFO - [Worker-135551181125312] Started, assigned 3 chunks.
2025-09-03 07:16:45,023 - INFO - [Worker-135551172732608] Started, assigned 3 chunks.
2025-09-03 07:16:45,023 - INFO - [Worker-135551164339904] Started, assigned 3 chunks.
2025-09-03 07:16:45,024 - INFO - [Worker-135551155947200] Started, assigned 3 chunks.
2025-09-03 07:16:45,025 - INFO - [Worker-135551147554496] Started, assigned 3 chunks.
2025-09-03 07:16:45,027 - INFO - [Worker-135551139161792] Started, assigned 3 chunks.
2025-09-03 07:16:45,028 - INFO - [Worker-135551130769088] Started, assigned 3 chunks.
2025-09-03 07:16:45,030 - INFO - [Worker-135550644254400] Started, assigned 3 chunks.
2025-09-03 07:16:45,032 - INFO - [Worker-135550635861696] Started, assigned 3 chunks.
2025-09-03 07:16:45,038 - INFO - [Worker-135550627468992] Started, assigned 3 chunks.
2025-09-03 07:16:45,039 - INFO - [Worker-135550619076288] Started, assigned 3 chunks.
2025-09-03 07:16:45,043 - INFO - [Worker-135550610683584] Started, assigned 3 chunks.
2025-09-03 07:16:45,047 - INFO - [Worker-135550602290880] Started, assigned 3 chunks.
2025-09-03 07:16:45,048 - INFO - [Worker-135550593898176] Started, assigned 3 chunks.
2025-09-03 07:16:45,049 - INFO - [Worker-135550107383488] Started, assigned 3 chunks.
2025-09-03 07:16:45,052 - INFO - [Worker-135550098990784] Started, assigned 3 chunks.
2025-09-03 07:16:45,054 - INFO - [Worker-135550090598080] Started, assigned 3 chunks.
2025-09-03 07:16:45,057 - INFO - [Worker-135550082205376] Started, assigned 3 chunks.
2025-09-03 07:16:45,058 - INFO - [Worker-135550073812672] Started, assigned 3 chunks.
2025-09-03 07:16:45,060 - INFO - [Worker-135550065419968] Started, assigned 3 chunks.
2025-09-03 07:16:45,062 - INFO - [Worker-135550057027264] Started, assigned 3 chunks.
2025-09-03 07:16:45,064 - INFO - [Worker-135549570512576] Started, assigned 3 chunks.
2025-09-03 07:16:45,066 - INFO - [Worker-135549562119872] Started, assigned 3 chunks.
2025-09-03 07:16:45,068 - INFO - [Worker-135549553727168] Started, assigned 3 chunks.
2025-09-03 07:16:49,305 - INFO - ✓ [Worker-135550610683584] Successfully processed 3 chunks and inserted into staging table.
2025-09-03 07:16:50,284 - INFO - ✓ [Worker-135549553727168] Successfully processed 3 chunks and inserted into staging table.
2025-09-03 07:17:05,781 - INFO - ✓ [Worker-135551139161792] Successfully processed 3 chunks and inserted into staging table.
2025-09-03 07:17:12,796 - INFO - ✓ [Worker-135550593898176] Successfully processed 3 chunks and inserted into staging table.
2025-09-03 07:17:14,991 - INFO - ✓ [Worker-135551181125312] Successfully processed 3 chunks and inserted into staging table.
2025-09-03 07:17:20,484 - INFO - ✓ [Worker-135551130769088] Successfully processed 3 chunks and inserted into staging table.
2025-09-03 07:17:27,278 - INFO - ✓ [Worker-135551676032704] Successfully processed 3 chunks and inserted into staging table.
2025-09-03 07:17:28,405 - INFO - ✓ [Worker-135551701210816] Successfully processed 3 chunks and inserted into staging table.
2025-09-03 07:17:28,487 - INFO - ✓ [Worker-135549570512576] Successfully processed 3 chunks and inserted into staging table.
2025-09-03 07:17:29,015 - INFO - ✓ [Worker-135551717996224] Successfully processed 3 chunks and inserted into staging table.
2025-09-03 07:17:29,450 - INFO - ✓ [Worker-135550619076288] Successfully processed 3 chunks and inserted into staging table.
2025-09-03 07:17:29,563 - INFO - ✓ [Worker-135550107383488] Successfully processed 3 chunks and inserted into staging table.
2025-09-03 07:17:29,887 - INFO - ✓ [Worker-135551692818112] Successfully processed 3 chunks and inserted into staging table.
2025-09-03 07:17:29,980 - INFO - ✓ [Worker-135551709603520] Successfully processed 3 chunks and inserted into staging table.
2025-09-03 07:17:30,095 - INFO - ✓ [Worker-135550644254400] Successfully processed 3 chunks and inserted into staging table.
2025-09-03 07:17:30,459 - INFO - ✓ [Worker-135550635861696] Successfully processed 3 chunks and inserted into staging table.
2025-09-03 07:17:30,857 - INFO - ✓ [Worker-135550090598080] Successfully processed 3 chunks and inserted into staging table.
2025-09-03 07:17:30,994 - INFO - ✓ [Worker-135550627468992] Successfully processed 3 chunks and inserted into staging table.
2025-09-03 07:17:31,101 - INFO - ✓ [Worker-135550057027264] Successfully processed 3 chunks and inserted into staging table.
2025-09-03 07:17:31,347 - INFO - ✓ [Worker-135550602290880] Successfully processed 3 chunks and inserted into staging table.
2025-09-03 07:17:31,443 - INFO - ✓ [Worker-135551172732608] Successfully processed 3 chunks and inserted into staging table.
2025-09-03 07:17:31,577 - INFO - ✓ [Worker-135550098990784] Successfully processed 3 chunks and inserted into staging table.
2025-09-03 07:17:31,746 - INFO - ✓ [Worker-135551684425408] Successfully processed 3 chunks and inserted into staging table.
2025-09-03 07:17:32,104 - INFO - ✓ [Worker-135551801882304] Successfully processed 3 chunks and inserted into staging table.
2025-09-03 07:17:32,147 - INFO - ✓ [Worker-135551667640000] Successfully processed 3 chunks and inserted into staging table.
2025-09-03 07:17:32,148 - INFO - ✓ [Worker-135550065419968] Successfully processed 3 chunks and inserted into staging table.
2025-09-03 07:17:32,231 - INFO - ✓ [Worker-135551164339904] Successfully processed 3 chunks and inserted into staging table.
2025-09-03 07:17:32,231 - INFO - ✓ [Worker-135550073812672] Successfully processed 3 chunks and inserted into staging table.
2025-09-03 07:17:32,898 - INFO - ✓ [Worker-135551155947200] Successfully processed 3 chunks and inserted into staging table.
2025-09-03 07:17:33,648 - INFO - ✓ [Worker-135549562119872] Successfully processed 3 chunks and inserted into staging table.
2025-09-03 07:17:33,786 - INFO - ✓ [Worker-135551147554496] Successfully processed 3 chunks and inserted into staging table.
2025-09-03 07:17:33,920 - INFO - ✓ [Worker-135550082205376] Successfully processed 3 chunks and inserted into staging table.
2025-09-03 07:17:33,924 - INFO - ✓ All parallel workers completed successfully.
2025-09-03 07:17:33,924 - INFO - 🤝 Assembling final table from staging data...
2025-09-03 07:17:33,925 - INFO - ✓ Loaded SQL script: final_assembly.sql
2025-09-03 07:19:03,585 - INFO - ✓ Final feature table assembled successfully.
2025-09-03 07:19:03,586 - INFO - Cleaning up staging table...
2025-09-03 07:19:04,110 - INFO - ✓ Staging table cleaned up successfully.
2025-09-03 07:19:04,110 - INFO - ======================================================================
2025-09-03 07:19:04,110 - INFO - 🎉 PIPELINE COMPLETED SUCCESSFULLY IN 139.6 SECONDS
2025-09-03 07:19:04,110 - INFO - ======================================================================
2025-09-03 17:44:38,961 - INFO - Connected to database with high-performance configuration
2025-09-03 17:44:38,961 - INFO - Initialized with 32 parallel workers.
2025-09-03 17:44:38,961 - INFO - ======================================================================
2025-09-03 17:44:38,961 - INFO - STARTING MASSIVELY PARALLEL FEATURE ENGINEERING
2025-09-03 17:44:38,961 - INFO - ======================================================================
2025-09-03 17:44:38,961 - INFO - Creating shared staging table...
2025-09-03 17:44:38,980 - INFO - ✓ Staging table created successfully.
2025-09-03 17:44:38,980 - INFO - Fetching all device-date chunks to be processed...
2025-09-03 17:44:39,448 - INFO - ✓ Divided 96 total chunks into 32 batches for parallel processing.
2025-09-03 17:44:39,448 - INFO - 🚀 Launching parallel workers...
2025-09-03 17:44:39,449 - INFO - [Worker-134993089132224] Started, assigned 3 chunks.
2025-09-03 17:44:39,449 - INFO - [Worker-134993080739520] Started, assigned 3 chunks.
2025-09-03 17:44:39,451 - INFO - [Worker-134993072346816] Started, assigned 3 chunks.
2025-09-03 17:44:39,451 - INFO - [Worker-134993063954112] Started, assigned 3 chunks.
2025-09-03 17:44:39,453 - INFO - [Worker-134993055561408] Started, assigned 3 chunks.
2025-09-03 17:44:39,453 - INFO - [Worker-134993047168704] Started, assigned 3 chunks.
2025-09-03 17:44:39,454 - INFO - [Worker-134992701159104] Started, assigned 3 chunks.
2025-09-03 17:44:39,455 - INFO - [Worker-134992692766400] Started, assigned 3 chunks.
2025-09-03 17:44:39,456 - INFO - [Worker-134992684373696] Started, assigned 3 chunks.
2025-09-03 17:44:39,457 - INFO - [Worker-134992675980992] Started, assigned 3 chunks.
2025-09-03 17:44:39,457 - INFO - [Worker-134992667588288] Started, assigned 3 chunks.
2025-09-03 17:44:39,458 - INFO - [Worker-134992659195584] Started, assigned 3 chunks.
2025-09-03 17:44:39,459 - INFO - [Worker-134992650802880] Started, assigned 3 chunks.
2025-09-03 17:44:39,460 - INFO - [Worker-134992097179328] Started, assigned 3 chunks.
2025-09-03 17:44:39,461 - INFO - [Worker-134992088786624] Started, assigned 3 chunks.
2025-09-03 17:44:39,462 - INFO - [Worker-134992080393920] Started, assigned 3 chunks.
2025-09-03 17:44:39,462 - INFO - [Worker-134992072001216] Started, assigned 3 chunks.
2025-09-03 17:44:39,464 - INFO - [Worker-134992063608512] Started, assigned 3 chunks.
2025-09-03 17:44:39,465 - INFO - [Worker-134992055215808] Started, assigned 3 chunks.
2025-09-03 17:44:39,470 - INFO - [Worker-134992046823104] Started, assigned 3 chunks.
2025-09-03 17:44:39,472 - INFO - [Worker-134991560308416] Started, assigned 3 chunks.
2025-09-03 17:44:39,475 - INFO - [Worker-134991551915712] Started, assigned 3 chunks.
2025-09-03 17:44:39,476 - INFO - [Worker-134991543523008] Started, assigned 3 chunks.
2025-09-03 17:44:39,481 - INFO - [Worker-134991535130304] Started, assigned 3 chunks.
2025-09-03 17:44:39,484 - INFO - [Worker-134991526737600] Started, assigned 3 chunks.
2025-09-03 17:44:39,487 - INFO - [Worker-134991518344896] Started, assigned 3 chunks.
2025-09-03 17:44:39,489 - INFO - [Worker-134991509952192] Started, assigned 3 chunks.
2025-09-03 17:44:39,492 - INFO - [Worker-134991023437504] Started, assigned 3 chunks.
2025-09-03 17:44:39,496 - INFO - [Worker-134991015044800] Started, assigned 3 chunks.
2025-09-03 17:44:39,498 - INFO - [Worker-134991006652096] Started, assigned 3 chunks.
2025-09-03 17:44:39,500 - INFO - [Worker-134990998259392] Started, assigned 3 chunks.
2025-09-03 17:44:39,503 - INFO - [Worker-134990989866688] Started, assigned 3 chunks.
2025-09-03 17:44:44,008 - INFO - ✓ [Worker-134992046823104] Successfully processed 3 chunks and inserted into staging table.
2025-09-03 17:44:47,990 - INFO - ✓ [Worker-134990989866688] Successfully processed 3 chunks and inserted into staging table.
2025-09-03 17:45:01,230 - INFO - ✓ [Worker-134992097179328] Successfully processed 3 chunks and inserted into staging table.
2025-09-03 17:45:07,113 - INFO - ✓ [Worker-134992684373696] Successfully processed 3 chunks and inserted into staging table.
2025-09-03 17:45:13,209 - INFO - ✓ [Worker-134991551915712] Successfully processed 3 chunks and inserted into staging table.
2025-09-03 17:45:14,652 - INFO - ✓ [Worker-134992088786624] Successfully processed 3 chunks and inserted into staging table.
2025-09-03 17:45:19,016 - INFO - ✓ [Worker-134992701159104] Successfully processed 3 chunks and inserted into staging table.
2025-09-03 17:45:20,312 - INFO - ✓ [Worker-134993063954112] Successfully processed 3 chunks and inserted into staging table.
2025-09-03 17:45:21,995 - INFO - ✓ [Worker-134991006652096] Successfully processed 3 chunks and inserted into staging table.
2025-09-03 17:45:22,234 - INFO - ✓ [Worker-134992055215808] Successfully processed 3 chunks and inserted into staging table.
2025-09-03 17:45:22,983 - INFO - ✓ [Worker-134991543523008] Successfully processed 3 chunks and inserted into staging table.
2025-09-03 17:45:23,212 - INFO - ✓ [Worker-134993072346816] Successfully processed 3 chunks and inserted into staging table.
2025-09-03 17:45:23,296 - INFO - ✓ [Worker-134993080739520] Successfully processed 3 chunks and inserted into staging table.
2025-09-03 17:45:23,354 - INFO - ✓ [Worker-134993055561408] Successfully processed 3 chunks and inserted into staging table.
2025-09-03 17:45:23,475 - INFO - ✓ [Worker-134992072001216] Successfully processed 3 chunks and inserted into staging table.
2025-09-03 17:45:23,681 - INFO - ✓ [Worker-134992063608512] Successfully processed 3 chunks and inserted into staging table.
2025-09-03 17:45:23,755 - INFO - ✓ [Worker-134992080393920] Successfully processed 3 chunks and inserted into staging table.
2025-09-03 17:45:24,274 - INFO - ✓ [Worker-134991015044800] Successfully processed 3 chunks and inserted into staging table.
2025-09-03 17:45:24,758 - INFO - ✓ [Worker-134992675980992] Successfully processed 3 chunks and inserted into staging table.
2025-09-03 17:45:24,775 - INFO - ✓ [Worker-134993047168704] Successfully processed 3 chunks and inserted into staging table.
2025-09-03 17:45:24,896 - INFO - ✓ [Worker-134991560308416] Successfully processed 3 chunks and inserted into staging table.
2025-09-03 17:45:24,896 - INFO - ✓ [Worker-134991526737600] Successfully processed 3 chunks and inserted into staging table.
2025-09-03 17:45:25,052 - INFO - ✓ [Worker-134991535130304] Successfully processed 3 chunks and inserted into staging table.
2025-09-03 17:45:25,053 - INFO - ✓ [Worker-134992692766400] Successfully processed 3 chunks and inserted into staging table.
2025-09-03 17:45:25,128 - INFO - ✓ [Worker-134993089132224] Successfully processed 3 chunks and inserted into staging table.
2025-09-03 17:45:25,128 - INFO - ✓ [Worker-134991509952192] Successfully processed 3 chunks and inserted into staging table.
2025-09-03 17:45:25,183 - INFO - ✓ [Worker-134992667588288] Successfully processed 3 chunks and inserted into staging table.
2025-09-03 17:45:25,347 - INFO - ✓ [Worker-134991023437504] Successfully processed 3 chunks and inserted into staging table.
2025-09-03 17:45:25,544 - INFO - ✓ [Worker-134992659195584] Successfully processed 3 chunks and inserted into staging table.
2025-09-03 17:45:26,258 - INFO - ✓ [Worker-134990998259392] Successfully processed 3 chunks and inserted into staging table.
2025-09-03 17:45:26,364 - INFO - ✓ [Worker-134992650802880] Successfully processed 3 chunks and inserted into staging table.
2025-09-03 17:45:26,562 - INFO - ✓ [Worker-134991518344896] Successfully processed 3 chunks and inserted into staging table.
2025-09-03 17:45:26,566 - INFO - ✓ All parallel workers completed successfully.
2025-09-03 17:45:26,566 - INFO - 🤝 Assembling final table from staging data...
2025-09-03 17:45:26,567 - INFO - ✓ Loaded SQL script: final_assembly.sql
2025-09-03 17:47:06,766 - INFO - ✓ Final feature table assembled successfully.
2025-09-03 17:47:06,766 - INFO - Cleaning up staging table...
2025-09-03 17:47:07,386 - INFO - ✓ Staging table cleaned up successfully.
2025-09-03 17:47:07,386 - INFO - ======================================================================
2025-09-03 17:47:07,386 - INFO - 🎉 PIPELINE COMPLETED SUCCESSFULLY IN 148.4 SECONDS
2025-09-03 17:47:07,386 - INFO - ======================================================================
