{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataMine v2 - Comprehensive Exploratory Data Analysis\n",
    "## Database: datamine_v2_db | Table: 02_raw_telemetry_transformed\n",
    "\n",
    "**Objective:** Comprehensive analysis of the entire telemetry dataset to understand data characteristics, patterns, and quality across all devices and time periods.\n",
    "\n",
    "**Key Considerations:**\n",
    "- **605 Trucks:** Working load sensors (5 devices, 6.18M records)\n",
    "- **775G Trucks:** Broken load sensors (2 devices, 2.65M records)\n",
    "- **Total Dataset:** 8.83M records across 7 devices\n",
    "\n",
    "This analysis will inform the hybrid framework for high-precision haul cycle event detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import psycopg2\n",
    "from sqlalchemy import create_engine\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "\n",
    "# Configure plotting\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Database Connection & Initial Data Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Database connection configuration\n",
    "DB_CONFIG = {\n",
    "    'host': 'localhost',\n",
    "    'port': 5432,\n",
    "    'database': 'datamine_v2_db',\n",
    "    'user': 'ahs_user',\n",
    "    'password': 'ahs_password'\n",
    "}\n",
    "\n",
    "# Create SQLAlchemy engine\n",
    "engine = create_engine(f\"postgresql://{DB_CONFIG['user']}:{DB_CONFIG['password']}@{DB_CONFIG['host']}:{DB_CONFIG['port']}/{DB_CONFIG['database']}\")\n",
    "\n",
    "print(\"Database connection established!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get dataset overview - device breakdown by truck type\n",
    "overview_query = \"\"\"\n",
    "SELECT \n",
    "    device_id,\n",
    "    CASE WHEN device_id LIKE '%775g%' THEN '775G (Broken Load Sensors)' \n",
    "         ELSE '605 (Working Load Sensors)' END as truck_type,\n",
    "    COUNT(*) as total_records,\n",
    "    MIN(timestamp) as earliest_record,\n",
    "    MAX(timestamp) as latest_record,\n",
    "    COUNT(DISTINCT DATE(timestamp)) as operational_days\n",
    "FROM \"02_raw_telemetry_transformed\" \n",
    "GROUP BY device_id, truck_type\n",
    "ORDER BY truck_type, total_records DESC;\n",
    "\"\"\"\n",
    "\n",
    "overview_df = pd.read_sql_query(overview_query, engine)\n",
    "print(\"Dataset Overview:\")\n",
    "display(overview_df)\n",
    "\n",
    "# Summary statistics\n",
    "print(\"\\n=== DATASET SUMMARY ===\")\n",
    "print(f\"Total Records: {overview_df['total_records'].sum():,}\")\n",
    "print(f\"Total Devices: {len(overview_df)}\")\n",
    "print(f\"\\n605 Trucks (Working Sensors): {len(overview_df[overview_df['truck_type'] == '605 (Working Load Sensors)'])} devices, {overview_df[overview_df['truck_type'] == '605 (Working Load Sensors)']['total_records'].sum():,} records\")\n",
    "print(f\"775G Trucks (Broken Sensors): {len(overview_df[overview_df['truck_type'] == '775G (Broken Load Sensors)'])} devices, {overview_df[overview_df['truck_type'] == '775G (Broken Load Sensors)']['total_records'].sum():,} records\")\n",
    "print(f\"\\nOperational Period: {overview_df['earliest_record'].min()} to {overview_df['latest_record'].max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Table Schema and Data Types Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get table schema information\n",
    "schema_query = \"\"\"\n",
    "SELECT \n",
    "    column_name,\n",
    "    data_type,\n",
    "    is_nullable,\n",
    "    column_default\n",
    "FROM information_schema.columns \n",
    "WHERE table_name = '02_raw_telemetry_transformed' \n",
    "    AND table_schema = 'public'\n",
    "ORDER BY ordinal_position;\n",
    "\"\"\"\n",
    "\n",
    "schema_df = pd.read_sql_query(schema_query, engine)\n",
    "print(\"Table Schema:\")\n",
    "display(schema_df)\n",
    "\n",
    "# Get enum values\n",
    "enum_queries = {\n",
    "    'state': \"SELECT unnest(enum_range(NULL::telemetry_state_enum)) as enum_value;\",\n",
    "    'software_state': \"SELECT unnest(enum_range(NULL::software_state_enum)) as enum_value;\",\n",
    "    'prndl': \"SELECT unnest(enum_range(NULL::prndl_enum)) as enum_value;\"\n",
    "}\n",
    "\n",
    "print(\"\\n=== ENUM VALUES ===\")\n",
    "for enum_name, query in enum_queries.items():\n",
    "    try:\n",
    "        enum_df = pd.read_sql_query(query, engine)\n",
    "        print(f\"\\n{enum_name.upper()} enum values:\")\n",
    "        print(enum_df['enum_value'].tolist())\n",
    "    except Exception as e:\n",
    "        print(f\"Error getting {enum_name} enum values: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Quality Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data quality analysis - null values, completeness by column\n",
    "quality_query = \"\"\"\n",
    "SELECT \n",
    "    COUNT(*) as total_records,\n",
    "    COUNT(device_id) as device_id_count,\n",
    "    COUNT(timestamp) as timestamp_count,\n",
    "    COUNT(system_engaged) as system_engaged_count,\n",
    "    COUNT(parking_brake_applied) as parking_brake_count,\n",
    "    COUNT(current_position) as position_count,\n",
    "    COUNT(current_speed) as speed_count,\n",
    "    COUNT(load_weight) as load_weight_count,\n",
    "    COUNT(state) as state_count,\n",
    "    COUNT(software_state) as software_state_count,\n",
    "    COUNT(prndl) as prndl_count,\n",
    "    COUNT(extras) as extras_count\n",
    "FROM \"02_raw_telemetry_transformed\";\n",
    "\"\"\"\n",
    "\n",
    "quality_df = pd.read_sql_query(quality_query, engine)\n",
    "total_records = quality_df['total_records'].iloc[0]\n",
    "\n",
    "print(\"=== DATA COMPLETENESS ANALYSIS ===\")\n",
    "for col in quality_df.columns:\n",
    "    if col != 'total_records':\n",
    "        count = quality_df[col].iloc[0]\n",
    "        completeness = (count / total_records) * 100\n",
    "        missing = total_records - count\n",
    "        print(f\"{col:25}: {count:>10,} records ({completeness:>6.2f}% complete, {missing:>8,} missing)\")\n",
    "\n",
    "print(f\"\\nTotal Records: {total_records:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data quality by truck type - especially focusing on load_weight\n",
    "quality_by_type_query = \"\"\"\n",
    "SELECT \n",
    "    CASE WHEN device_id LIKE '%775g%' THEN '775G (Broken Load Sensors)' \n",
    "         ELSE '605 (Working Load Sensors)' END as truck_type,\n",
    "    COUNT(*) as total_records,\n",
    "    COUNT(load_weight) as load_weight_count,\n",
    "    ROUND(COUNT(load_weight)::numeric / COUNT(*)::numeric * 100, 2) as load_weight_completeness,\n",
    "    COUNT(CASE WHEN load_weight > 0 THEN 1 END) as positive_load_weight,\n",
    "    ROUND(AVG(load_weight), 2) as avg_load_weight,\n",
    "    ROUND(STDDEV(load_weight), 2) as stddev_load_weight,\n",
    "    MIN(load_weight) as min_load_weight,\n",
    "    MAX(load_weight) as max_load_weight\n",
    "FROM \"02_raw_telemetry_transformed\" \n",
    "GROUP BY truck_type\n",
    "ORDER BY truck_type;\n",
    "\"\"\"\n",
    "\n",
    "quality_by_type_df = pd.read_sql_query(quality_by_type_query, engine)\n",
    "print(\"\\n=== LOAD WEIGHT DATA QUALITY BY TRUCK TYPE ===\")\n",
    "display(quality_by_type_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. State Distribution Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze state distribution across entire dataset and by truck type\n",
    "state_analysis_query = \"\"\"\n",
    "SELECT \n",
    "    state,\n",
    "    COUNT(*) as total_count,\n",
    "    ROUND(COUNT(*)::numeric / (SELECT COUNT(*) FROM \"02_raw_telemetry_transformed\")::numeric * 100, 2) as percentage,\n",
    "    COUNT(CASE WHEN device_id LIKE '%605%' THEN 1 END) as count_605,\n",
    "    COUNT(CASE WHEN device_id LIKE '%775g%' THEN 1 END) as count_775g\n",
    "FROM \"02_raw_telemetry_transformed\" \n",
    "GROUP BY state \n",
    "ORDER BY total_count DESC;\n",
    "\"\"\"\n",
    "\n",
    "state_df = pd.read_sql_query(state_analysis_query, engine)\n",
    "print(\"=== STATE DISTRIBUTION ANALYSIS ===\")\n",
    "display(state_df)\n",
    "\n",
    "# Visualize state distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Overall state distribution\n",
    "axes[0].pie(state_df['total_count'], labels=state_df['state'], autopct='%1.1f%%', startangle=90)\n",
    "axes[0].set_title('Overall State Distribution\\n(All Trucks)')\n",
    "\n",
    "# State comparison by truck type\n",
    "state_comparison = state_df.set_index('state')[['count_605', 'count_775g']]\n",
    "state_comparison.plot(kind='bar', ax=axes[1])\n",
    "axes[1].set_title('State Distribution by Truck Type')\n",
    "axes[1].set_xlabel('State')\n",
    "axes[1].set_ylabel('Count')\n",
    "axes[1].legend(['605 Trucks', '775G Trucks'])\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Load Weight Analysis (Critical for Framework)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed load weight analysis - focus on 605 trucks vs 775G trucks\n",
    "load_weight_query = \"\"\"\n",
    "SELECT \n",
    "    device_id,\n",
    "    CASE WHEN device_id LIKE '%775g%' THEN '775G' ELSE '605' END as truck_series,\n",
    "    COUNT(*) as total_records,\n",
    "    COUNT(load_weight) as load_weight_records,\n",
    "    COUNT(CASE WHEN load_weight IS NOT NULL AND load_weight > 0 THEN 1 END) as positive_weight_records,\n",
    "    ROUND(AVG(load_weight), 2) as avg_load_weight,\n",
    "    ROUND(STDDEV(load_weight), 2) as stddev_load_weight,\n",
    "    MIN(load_weight) as min_load_weight,\n",
    "    MAX(load_weight) as max_load_weight,\n",
    "    PERCENTILE_CONT(0.25) WITHIN GROUP (ORDER BY load_weight) as q25_load_weight,\n",
    "    PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY load_weight) as median_load_weight,\n",
    "    PERCENTILE_CONT(0.75) WITHIN GROUP (ORDER BY load_weight) as q75_load_weight,\n",
    "    PERCENTILE_CONT(0.95) WITHIN GROUP (ORDER BY load_weight) as q95_load_weight\n",
    "FROM \"02_raw_telemetry_transformed\" \n",
    "GROUP BY device_id, truck_series\n",
    "ORDER BY truck_series, device_id;\n",
    "\"\"\"\n",
    "\n",
    "load_weight_df = pd.read_sql_query(load_weight_query, engine)\n",
    "print(\"=== LOAD WEIGHT ANALYSIS BY DEVICE ===\")\n",
    "display(load_weight_df)\n",
    "\n",
    "# Summary by truck series\n",
    "print(\"\\n=== LOAD WEIGHT SUMMARY BY TRUCK SERIES ===\")\n",
    "series_summary = load_weight_df.groupby('truck_series').agg({\n",
    "    'total_records': 'sum',\n",
    "    'load_weight_records': 'sum',\n",
    "    'positive_weight_records': 'sum',\n",
    "    'avg_load_weight': 'mean',\n",
    "    'min_load_weight': 'min',\n",
    "    'max_load_weight': 'max'\n",
    "}).round(2)\n",
    "\n",
    "series_summary['weight_completeness_%'] = (series_summary['load_weight_records'] / series_summary['total_records'] * 100).round(2)\n",
    "series_summary['positive_weight_%'] = (series_summary['positive_weight_records'] / series_summary['load_weight_records'] * 100).round(2)\n",
    "\n",
    "display(series_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample load weight data for detailed analysis (working sensors only)\n",
    "sample_load_query = \"\"\"\n",
    "SELECT \n",
    "    load_weight,\n",
    "    current_speed,\n",
    "    state,\n",
    "    device_id\n",
    "FROM \"02_raw_telemetry_transformed\" \n",
    "WHERE device_id NOT LIKE '%775g%' \n",
    "    AND load_weight IS NOT NULL\n",
    "ORDER BY RANDOM()\n",
    "LIMIT 50000;\n",
    "\"\"\"\n",
    "\n",
    "sample_load_df = pd.read_sql_query(sample_load_query, engine)\n",
    "\n",
    "# Load weight distribution visualization (605 trucks only)\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# Load weight histogram\n",
    "axes[0,0].hist(sample_load_df['load_weight'], bins=100, alpha=0.7, edgecolor='black')\n",
    "axes[0,0].set_title('Load Weight Distribution (605 Trucks Only)')\n",
    "axes[0,0].set_xlabel('Load Weight (kg)')\n",
    "axes[0,0].set_ylabel('Frequency')\n",
    "\n",
    "# Load weight by state\n",
    "sample_load_df.boxplot(column='load_weight', by='state', ax=axes[0,1])\n",
    "axes[0,1].set_title('Load Weight by State (605 Trucks)')\n",
    "axes[0,1].set_xlabel('State')\n",
    "axes[0,1].set_ylabel('Load Weight (kg)')\n",
    "\n",
    "# Speed vs Load Weight scatter\n",
    "scatter_sample = sample_load_df.sample(n=5000) if len(sample_load_df) > 5000 else sample_load_df\n",
    "axes[1,0].scatter(scatter_sample['current_speed'], scatter_sample['load_weight'], alpha=0.5)\n",
    "axes[1,0].set_title('Speed vs Load Weight (605 Trucks)')\n",
    "axes[1,0].set_xlabel('Current Speed (m/s)')\n",
    "axes[1,0].set_ylabel('Load Weight (kg)')\n",
    "\n",
    "# Load weight by device\n",
    "sample_load_df.boxplot(column='load_weight', by='device_id', ax=axes[1,1])\n",
    "axes[1,1].set_title('Load Weight by Device (605 Trucks)')\n",
    "axes[1,1].set_xlabel('Device ID')\n",
    "axes[1,1].set_ylabel('Load Weight (kg)')\n",
    "axes[1,1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Sample size for visualization: {len(sample_load_df):,} records from 605 trucks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Speed and Position Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Speed analysis across all trucks\n",
    "speed_analysis_query = \"\"\"\n",
    "SELECT \n",
    "    CASE WHEN device_id LIKE '%775g%' THEN '775G' ELSE '605' END as truck_series,\n",
    "    COUNT(*) as total_records,\n",
    "    COUNT(current_speed) as speed_records,\n",
    "    ROUND(AVG(current_speed), 2) as avg_speed,\n",
    "    ROUND(STDDEV(current_speed), 2) as stddev_speed,\n",
    "    MIN(current_speed) as min_speed,\n",
    "    MAX(current_speed) as max_speed,\n",
    "    PERCENTILE_CONT(0.25) WITHIN GROUP (ORDER BY current_speed) as q25_speed,\n",
    "    PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY current_speed) as median_speed,\n",
    "    PERCENTILE_CONT(0.75) WITHIN GROUP (ORDER BY current_speed) as q75_speed,\n",
    "    PERCENTILE_CONT(0.95) WITHIN GROUP (ORDER BY current_speed) as q95_speed,\n",
    "    COUNT(CASE WHEN current_speed <= 0.5 THEN 1 END) as stationary_count\n",
    "FROM \"02_raw_telemetry_transformed\" \n",
    "GROUP BY truck_series\n",
    "ORDER BY truck_series;\n",
    "\"\"\"\n",
    "\n",
    "speed_df = pd.read_sql_query(speed_analysis_query, engine)\n",
    "print(\"=== SPEED ANALYSIS BY TRUCK SERIES ===\")\n",
    "display(speed_df)\n",
    "\n",
    "# Add stationary percentage\n",
    "speed_df['stationary_percentage'] = (speed_df['stationary_count'] / speed_df['total_records'] * 100).round(2)\n",
    "print(\"\\nStationary Analysis (Speed <= 0.5 m/s):\")\n",
    "display(speed_df[['truck_series', 'stationary_count', 'stationary_percentage']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Position analysis - extract lat/lon from PostGIS geography\n",
    "position_sample_query = \"\"\"\n",
    "SELECT \n",
    "    device_id,\n",
    "    CASE WHEN device_id LIKE '%775g%' THEN '775G' ELSE '605' END as truck_series,\n",
    "    ST_X(current_position::geometry) as longitude,\n",
    "    ST_Y(current_position::geometry) as latitude,\n",
    "    ST_Z(current_position::geometry) as altitude,\n",
    "    current_speed,\n",
    "    state,\n",
    "    timestamp\n",
    "FROM \"02_raw_telemetry_transformed\" \n",
    "WHERE current_position IS NOT NULL\n",
    "ORDER BY RANDOM()\n",
    "LIMIT 25000;\n",
    "\"\"\"\n",
    "\n",
    "position_sample_df = pd.read_sql_query(position_sample_query, engine)\n",
    "\n",
    "print(\"=== POSITION DATA SAMPLE ===\")\n",
    "print(f\"Sample size: {len(position_sample_df):,} records\")\n",
    "print(\"\\nPosition Statistics:\")\n",
    "display(position_sample_df[['longitude', 'latitude', 'altitude']].describe())\n",
    "\n",
    "# Visualize positions\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Geographic positions by truck series\n",
    "for series in position_sample_df['truck_series'].unique():\n",
    "    series_data = position_sample_df[position_sample_df['truck_series'] == series]\n",
    "    axes[0].scatter(series_data['longitude'], series_data['latitude'], \n",
    "                   label=f'{series} Trucks', alpha=0.6, s=1)\n",
    "\n",
    "axes[0].set_title('Geographic Distribution of Truck Operations')\n",
    "axes[0].set_xlabel('Longitude')\n",
    "axes[0].set_ylabel('Latitude')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Altitude distribution\n",
    "position_sample_df['altitude'].hist(bins=50, alpha=0.7, ax=axes[1])\n",
    "axes[1].set_title('Altitude Distribution')\n",
    "axes[1].set_xlabel('Altitude (m)')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. ENUM Analysis (State, Software State, PRNDL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive ENUM analysis\n",
    "enum_analysis_query = \"\"\"\n",
    "SELECT \n",
    "    CASE WHEN device_id LIKE '%775g%' THEN '775G' ELSE '605' END as truck_series,\n",
    "    state,\n",
    "    software_state,\n",
    "    prndl,\n",
    "    COUNT(*) as record_count,\n",
    "    ROUND(COUNT(*)::numeric / (SELECT COUNT(*) FROM \"02_raw_telemetry_transformed\")::numeric * 100, 3) as percentage\n",
    "FROM \"02_raw_telemetry_transformed\" \n",
    "GROUP BY truck_series, state, software_state, prndl\n",
    "ORDER BY record_count DESC\n",
    "LIMIT 20;\n",
    "\"\"\"\n",
    "\n",
    "enum_df = pd.read_sql_query(enum_analysis_query, engine)\n",
    "print(\"=== TOP 20 ENUM COMBINATIONS ===\")\n",
    "display(enum_df)\n",
    "\n",
    "# Individual ENUM distributions\n",
    "individual_enums = {}\n",
    "for enum_col in ['state', 'software_state', 'prndl']:\n",
    "    query = f\"\"\"\n",
    "    SELECT \n",
    "        {enum_col},\n",
    "        COUNT(*) as count,\n",
    "        ROUND(COUNT(*)::numeric / (SELECT COUNT(*) FROM \"02_raw_telemetry_transformed\")::numeric * 100, 2) as percentage\n",
    "    FROM \"02_raw_telemetry_transformed\" \n",
    "    GROUP BY {enum_col}\n",
    "    ORDER BY count DESC;\n",
    "    \"\"\"\n",
    "    individual_enums[enum_col] = pd.read_sql_query(query, engine)\n",
    "\n",
    "# Display individual distributions\n",
    "for enum_name, enum_data in individual_enums.items():\n",
    "    print(f\"\\n=== {enum_name.upper()} DISTRIBUTION ===\")\n",
    "    display(enum_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize ENUM distributions\n",
    "fig, axes = plt.subplots(3, 1, figsize=(12, 15))\n",
    "\n",
    "for i, (enum_name, enum_data) in enumerate(individual_enums.items()):\n",
    "    enum_data.plot(x=enum_name, y='percentage', kind='bar', ax=axes[i])\n",
    "    axes[i].set_title(f'{enum_name.upper()} Distribution (% of Total Records)')\n",
    "    axes[i].set_xlabel(enum_name.replace('_', ' ').title())\n",
    "    axes[i].set_ylabel('Percentage (%)')\n",
    "    axes[i].tick_params(axis='x', rotation=45)\n",
    "    axes[i].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Boolean Fields Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boolean fields analysis\n",
    "boolean_analysis_query = \"\"\"\n",
    "SELECT \n",
    "    CASE WHEN device_id LIKE '%775g%' THEN '775G' ELSE '605' END as truck_series,\n",
    "    COUNT(*) as total_records,\n",
    "    COUNT(CASE WHEN system_engaged = true THEN 1 END) as system_engaged_true,\n",
    "    COUNT(CASE WHEN system_engaged = false THEN 1 END) as system_engaged_false,\n",
    "    COUNT(CASE WHEN system_engaged IS NULL THEN 1 END) as system_engaged_null,\n",
    "    COUNT(CASE WHEN parking_brake_applied = true THEN 1 END) as parking_brake_true,\n",
    "    COUNT(CASE WHEN parking_brake_applied = false THEN 1 END) as parking_brake_false,\n",
    "    COUNT(CASE WHEN parking_brake_applied IS NULL THEN 1 END) as parking_brake_null\n",
    "FROM \"02_raw_telemetry_transformed\" \n",
    "GROUP BY truck_series\n",
    "ORDER BY truck_series;\n",
    "\"\"\"\n",
    "\n",
    "boolean_df = pd.read_sql_query(boolean_analysis_query, engine)\n",
    "print(\"=== BOOLEAN FIELDS ANALYSIS ===\")\n",
    "display(boolean_df)\n",
    "\n",
    "# Calculate percentages\n",
    "for series in boolean_df['truck_series']:\n",
    "    series_data = boolean_df[boolean_df['truck_series'] == series].iloc[0]\n",
    "    total = series_data['total_records']\n",
    "    \n",
    "    print(f\"\\n=== {series} TRUCKS BOOLEAN PERCENTAGES ===\")\n",
    "    print(f\"System Engaged - True: {series_data['system_engaged_true']/total*100:.1f}%, False: {series_data['system_engaged_false']/total*100:.1f}%, Null: {series_data['system_engaged_null']/total*100:.1f}%\")\n",
    "    print(f\"Parking Brake - True: {series_data['parking_brake_true']/total*100:.1f}%, False: {series_data['parking_brake_false']/total*100:.1f}%, Null: {series_data['parking_brake_null']/total*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Time-Series Patterns Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporal patterns analysis\n",
    "temporal_query = \"\"\"\n",
    "SELECT \n",
    "    DATE(timestamp) as operation_date,\n",
    "    EXTRACT(hour FROM timestamp) as hour,\n",
    "    EXTRACT(dow FROM timestamp) as day_of_week,\n",
    "    COUNT(*) as record_count,\n",
    "    COUNT(DISTINCT device_id) as active_devices,\n",
    "    AVG(CASE WHEN device_id NOT LIKE '%775g%' THEN load_weight END) as avg_load_weight_605,\n",
    "    AVG(current_speed) as avg_speed\n",
    "FROM \"02_raw_telemetry_transformed\" \n",
    "GROUP BY DATE(timestamp), EXTRACT(hour FROM timestamp), EXTRACT(dow FROM timestamp)\n",
    "ORDER BY operation_date, hour;\n",
    "\"\"\"\n",
    "\n",
    "temporal_df = pd.read_sql_query(temporal_query, engine)\n",
    "temporal_df['operation_date'] = pd.to_datetime(temporal_df['operation_date'])\n",
    "\n",
    "print(\"=== TEMPORAL PATTERNS ANALYSIS ===\")\n",
    "print(f\"Data spans {temporal_df['operation_date'].nunique()} days\")\n",
    "print(f\"From {temporal_df['operation_date'].min()} to {temporal_df['operation_date'].max()}\")\n",
    "\n",
    "# Daily activity patterns\n",
    "daily_summary = temporal_df.groupby('operation_date').agg({\n",
    "    'record_count': 'sum',\n",
    "    'active_devices': 'max',\n",
    "    'avg_load_weight_605': 'mean',\n",
    "    'avg_speed': 'mean'\n",
    "}).round(2)\n",
    "\n",
    "print(\"\\nDaily Activity Summary:\")\n",
    "display(daily_summary.head(10))\n",
    "\n",
    "# Hourly patterns\n",
    "hourly_summary = temporal_df.groupby('hour').agg({\n",
    "    'record_count': 'mean',\n",
    "    'active_devices': 'mean',\n",
    "    'avg_speed': 'mean'\n",
    "}).round(2)\n",
    "\n",
    "print(\"\\nHourly Activity Patterns:\")\n",
    "display(hourly_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize temporal patterns\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# Daily records over time\n",
    "daily_summary.plot(y='record_count', ax=axes[0,0], kind='line')\n",
    "axes[0,0].set_title('Daily Record Count Over Time')\n",
    "axes[0,0].set_xlabel('Date')\n",
    "axes[0,0].set_ylabel('Records per Day')\n",
    "axes[0,0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Hourly activity pattern\n",
    "hourly_summary.plot(y='record_count', ax=axes[0,1], kind='bar')\n",
    "axes[0,1].set_title('Average Hourly Activity Pattern')\n",
    "axes[0,1].set_xlabel('Hour of Day')\n",
    "axes[0,1].set_ylabel('Average Records per Hour')\n",
    "\n",
    "# Daily average speed\n",
    "daily_summary.plot(y='avg_speed', ax=axes[1,0], kind='line', color='orange')\n",
    "axes[1,0].set_title('Daily Average Speed')\n",
    "axes[1,0].set_xlabel('Date')\n",
    "axes[1,0].set_ylabel('Average Speed (m/s)')\n",
    "axes[1,0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Daily active devices\n",
    "daily_summary.plot(y='active_devices', ax=axes[1,1], kind='line', color='green')\n",
    "axes[1,1].set_title('Daily Active Devices')\n",
    "axes[1,1].set_xlabel('Date')\n",
    "axes[1,1].set_ylabel('Number of Active Devices')\n",
    "axes[1,1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get correlation analysis data (sample for performance)\n",
    "correlation_query = \"\"\"\n",
    "SELECT \n",
    "    CASE WHEN device_id LIKE '%775g%' THEN 1 ELSE 0 END as is_775g,\n",
    "    CASE WHEN system_engaged THEN 1 ELSE 0 END as system_engaged_num,\n",
    "    CASE WHEN parking_brake_applied THEN 1 ELSE 0 END as parking_brake_num,\n",
    "    ST_X(current_position::geometry) as longitude,\n",
    "    ST_Y(current_position::geometry) as latitude,\n",
    "    ST_Z(current_position::geometry) as altitude,\n",
    "    current_speed,\n",
    "    COALESCE(load_weight, 0) as load_weight,\n",
    "    EXTRACT(hour FROM timestamp) as hour_of_day,\n",
    "    EXTRACT(dow FROM timestamp) as day_of_week\n",
    "FROM \"02_raw_telemetry_transformed\" \n",
    "WHERE current_position IS NOT NULL\n",
    "ORDER BY RANDOM()\n",
    "LIMIT 10000;\n",
    "\"\"\"\n",
    "\n",
    "correlation_df = pd.read_sql_query(correlation_query, engine)\n",
    "\n",
    "# Calculate correlation matrix\n",
    "correlation_matrix = correlation_df.corr()\n",
    "\n",
    "print(\"=== CORRELATION ANALYSIS ===\")\n",
    "print(f\"Correlation analysis based on {len(correlation_df):,} sample records\\n\")\n",
    "\n",
    "# Display correlation matrix\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0, \n",
    "            square=True, fmt='.2f', cbar_kws={'shrink': 0.8})\n",
    "plt.title('Correlation Matrix of Numerical Features')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Highlight key correlations\n",
    "print(\"Key Correlations (|correlation| > 0.3):\")\n",
    "for i in range(len(correlation_matrix.columns)):\n",
    "    for j in range(i+1, len(correlation_matrix.columns)):\n",
    "        corr_val = correlation_matrix.iloc[i, j]\n",
    "        if abs(corr_val) > 0.3:\n",
    "            print(f\"{correlation_matrix.columns[i]} vs {correlation_matrix.columns[j]}: {corr_val:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Data Quality Issues and Anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify potential data quality issues\n",
    "quality_issues_query = \"\"\"\n",
    "SELECT \n",
    "    'Negative Speed' as issue_type,\n",
    "    COUNT(*) as issue_count,\n",
    "    ROUND(COUNT(*)::numeric / (SELECT COUNT(*) FROM \"02_raw_telemetry_transformed\")::numeric * 100, 4) as percentage\n",
    "FROM \"02_raw_telemetry_transformed\" \n",
    "WHERE current_speed < 0\n",
    "\n",
    "UNION ALL\n",
    "\n",
    "SELECT \n",
    "    'Extreme Speed (>50 m/s)' as issue_type,\n",
    "    COUNT(*) as issue_count,\n",
    "    ROUND(COUNT(*)::numeric / (SELECT COUNT(*) FROM \"02_raw_telemetry_transformed\")::numeric * 100, 4) as percentage\n",
    "FROM \"02_raw_telemetry_transformed\" \n",
    "WHERE current_speed > 50\n",
    "\n",
    "UNION ALL\n",
    "\n",
    "SELECT \n",
    "    'Negative Load Weight (605 trucks)' as issue_type,\n",
    "    COUNT(*) as issue_count,\n",
    "    ROUND(COUNT(*)::numeric / (SELECT COUNT(*) FROM \"02_raw_telemetry_transformed\" WHERE device_id NOT LIKE '%775g%')::numeric * 100, 4) as percentage\n",
    "FROM \"02_raw_telemetry_transformed\" \n",
    "WHERE device_id NOT LIKE '%775g%' AND load_weight < 0\n",
    "\n",
    "UNION ALL\n",
    "\n",
    "SELECT \n",
    "    'Extreme Load Weight >100000 kg (605 trucks)' as issue_type,\n",
    "    COUNT(*) as issue_count,\n",
    "    ROUND(COUNT(*)::numeric / (SELECT COUNT(*) FROM \"02_raw_telemetry_transformed\" WHERE device_id NOT LIKE '%775g%')::numeric * 100, 4) as percentage\n",
    "FROM \"02_raw_telemetry_transformed\" \n",
    "WHERE device_id NOT LIKE '%775g%' AND load_weight > 100000\n",
    "\n",
    "UNION ALL\n",
    "\n",
    "SELECT \n",
    "    'Missing Position Data' as issue_type,\n",
    "    COUNT(*) as issue_count,\n",
    "    ROUND(COUNT(*)::numeric / (SELECT COUNT(*) FROM \"02_raw_telemetry_transformed\")::numeric * 100, 4) as percentage\n",
    "FROM \"02_raw_telemetry_transformed\" \n",
    "WHERE current_position IS NULL;\n",
    "\"\"\"\n",
    "\n",
    "quality_issues_df = pd.read_sql_query(quality_issues_query, engine)\n",
    "print(\"=== DATA QUALITY ISSUES ===\")\n",
    "display(quality_issues_df)\n",
    "\n",
    "# Device-specific quality analysis\n",
    "device_quality_query = \"\"\"\n",
    "SELECT \n",
    "    device_id,\n",
    "    COUNT(*) as total_records,\n",
    "    COUNT(CASE WHEN current_speed IS NULL THEN 1 END) as null_speed,\n",
    "    COUNT(CASE WHEN load_weight IS NULL THEN 1 END) as null_load_weight,\n",
    "    COUNT(CASE WHEN current_position IS NULL THEN 1 END) as null_position,\n",
    "    COUNT(CASE WHEN current_speed < 0 THEN 1 END) as negative_speed,\n",
    "    COUNT(CASE WHEN load_weight < 0 AND device_id NOT LIKE '%775g%' THEN 1 END) as negative_weight_605,\n",
    "    MIN(timestamp) as first_record,\n",
    "    MAX(timestamp) as last_record\n",
    "FROM \"02_raw_telemetry_transformed\" \n",
    "GROUP BY device_id\n",
    "ORDER BY device_id;\n",
    "\"\"\"\n",
    "\n",
    "device_quality_df = pd.read_sql_query(device_quality_query, engine)\n",
    "print(\"\\n=== DEVICE-SPECIFIC QUALITY ANALYSIS ===\")\n",
    "display(device_quality_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Key Findings Summary and Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"                    COMPREHENSIVE EDA FINDINGS SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Dataset Overview Summary\n",
    "total_records = overview_df['total_records'].sum()\n",
    "total_devices = len(overview_df)\n",
    "date_range_days = (pd.to_datetime(overview_df['latest_record'].max()) - pd.to_datetime(overview_df['earliest_record'].min())).days\n",
    "\n",
    "print(f\"\\nðŸ“Š DATASET OVERVIEW:\")\n",
    "print(f\"   â€¢ Total Records: {total_records:,}\")\n",
    "print(f\"   â€¢ Total Devices: {total_devices}\")\n",
    "print(f\"   â€¢ Time Span: {date_range_days} days ({overview_df['earliest_record'].min()} to {overview_df['latest_record'].max()})\")\n",
    "print(f\"   â€¢ Average Records per Device: {total_records/total_devices:,.0f}\")\n",
    "\n",
    "# Truck Type Breakdown\n",
    "print(f\"\\nðŸš› TRUCK TYPE BREAKDOWN:\")\n",
    "working_sensors = overview_df[overview_df['truck_type'] == '605 (Working Load Sensors)']\n",
    "broken_sensors = overview_df[overview_df['truck_type'] == '775G (Broken Load Sensors)']\n",
    "\n",
    "print(f\"   â€¢ 605 Trucks (Working Load Sensors): {len(working_sensors)} devices, {working_sensors['total_records'].sum():,} records ({working_sensors['total_records'].sum()/total_records*100:.1f}%)\")\n",
    "print(f\"   â€¢ 775G Trucks (Broken Load Sensors): {len(broken_sensors)} devices, {broken_sensors['total_records'].sum():,} records ({broken_sensors['total_records'].sum()/total_records*100:.1f}%)\")\n",
    "\n",
    "# Data Quality Summary\n",
    "print(f\"\\nðŸ” DATA QUALITY ASSESSMENT:\")\n",
    "completeness_issues = quality_issues_df[quality_issues_df['issue_count'] > 0]\n",
    "if len(completeness_issues) > 0:\n",
    "    print(f\"   â€¢ Major Quality Issues Identified: {len(completeness_issues)}\")\n",
    "    for _, issue in completeness_issues.iterrows():\n",
    "        print(f\"     - {issue['issue_type']}: {issue['issue_count']:,} cases ({issue['percentage']:.3f}%)\")\n",
    "else:\n",
    "    print(\"   â€¢ No significant data quality issues detected\")\n",
    "\n",
    "# Load Weight Analysis Summary\n",
    "print(f\"\\nâš–ï¸ LOAD WEIGHT ANALYSIS:\")\n",
    "load_605 = load_weight_df[load_weight_df['truck_series'] == '605']\n",
    "load_775g = load_weight_df[load_weight_df['truck_series'] == '775G']\n",
    "\n",
    "avg_completeness_605 = (load_605['load_weight_records'].sum() / load_605['total_records'].sum() * 100)\n",
    "avg_completeness_775g = (load_775g['load_weight_records'].sum() / load_775g['total_records'].sum() * 100)\n",
    "\n",
    "print(f\"   â€¢ 605 Trucks Load Weight Completeness: {avg_completeness_605:.1f}%\")\n",
    "print(f\"   â€¢ 775G Trucks Load Weight Completeness: {avg_completeness_775g:.1f}%\")\n",
    "print(f\"   â€¢ Average Load Weight (605 trucks): {load_605['avg_load_weight'].mean():.1f} kg\")\n",
    "print(f\"   â€¢ Max Load Weight (605 trucks): {load_605['max_load_weight'].max():.1f} kg\")\n",
    "\n",
    "# State Distribution Summary\n",
    "print(f\"\\nðŸš¦ OPERATIONAL STATE DISTRIBUTION:\")\n",
    "top_states = state_df.head(3)\n",
    "for _, state_row in top_states.iterrows():\n",
    "    print(f\"   â€¢ {state_row['state']}: {state_row['total_count']:,} records ({state_row['percentage']:.1f}%)\")\n",
    "\n",
    "# Speed Analysis Summary\n",
    "print(f\"\\nðŸƒ SPEED CHARACTERISTICS:\")\n",
    "for _, speed_row in speed_df.iterrows():\n",
    "    print(f\"   â€¢ {speed_row['truck_series']} Trucks:\")\n",
    "    print(f\"     - Average Speed: {speed_row['avg_speed']:.2f} m/s\")\n",
    "    print(f\"     - Stationary Time: {speed_row['stationary_percentage']:.1f}% (speed â‰¤ 0.5 m/s)\")\n",
    "\n",
    "print(f\"\\n\" + \"=\"*80)\n",
    "print(\"                           RECOMMENDATIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nðŸŽ¯ KEY RECOMMENDATIONS FOR PIPELINE DEVELOPMENT:\")\n",
    "print(f\"\\n1. SENSOR RELIABILITY STRATEGY:\")\n",
    "print(f\"   â€¢ Implement separate processing pipelines for 605 vs 775G trucks\")\n",
    "print(f\"   â€¢ 605 trucks: Leverage high-quality load_weight data (primary signal)\")\n",
    "print(f\"   â€¢ 775G trucks: Use weight-agnostic approach with speed + position + state\")\n",
    "\n",
    "print(f\"\\n2. FEATURE ENGINEERING PRIORITIES:\")\n",
    "print(f\"   â€¢ Create 'is_stationary' feature (speed â‰¤ 0.5 m/s) - {speed_df['stationary_percentage'].mean():.1f}% of data\")\n",
    "print(f\"   â€¢ Extract lat/lon/altitude from PostGIS geography for geofencing\")\n",
    "print(f\"   â€¢ Engineer rate-of-change features for load_weight (605 trucks)\")\n",
    "print(f\"   â€¢ Leverage state transitions as secondary indicators\")\n",
    "\n",
    "print(f\"\\n3. DATA QUALITY HANDLING:\")\n",
    "if len(completeness_issues) > 0:\n",
    "    print(f\"   â€¢ Address data quality issues before processing\")\n",
    "    print(f\"   â€¢ Implement robust null handling for optional fields\")\n",
    "else:\n",
    "    print(f\"   â€¢ Data quality is excellent - minimal preprocessing required\")\n",
    "print(f\"   â€¢ Validate geographic bounds for position data\")\n",
    "print(f\"   â€¢ Apply reasonable bounds checking for speed and load weight\")\n",
    "\n",
    "print(f\"\\n4. HYBRID FRAMEWORK IMPLEMENTATION:\")\n",
    "print(f\"   â€¢ Stage 1: Clean and smooth signals, especially load_weight for 605 trucks\")\n",
    "print(f\"   â€¢ Stage 2: Apply CPD to load_weight (605) or multi-signal composite (775G)\")\n",
    "print(f\"   â€¢ Stage 3: Use rich feature vectors combining all available signals\")\n",
    "print(f\"   â€¢ Stage 4: State machine validation using operational state transitions\")\n",
    "\n",
    "print(f\"\\n5. SCALABILITY CONSIDERATIONS:\")\n",
    "print(f\"   â€¢ Process data in manageable chunks (by device_id and date range)\")\n",
    "print(f\"   â€¢ Implement efficient indexing strategies for timestamp-based queries\")\n",
    "print(f\"   â€¢ Cache geofence calculations for repeated position lookups\")\n",
    "print(f\"   â€¢ Design for {total_records//1000000:.1f}M+ records processing capability\")\n",
    "\n",
    "print(f\"\\n\" + \"=\"*80)\n",
    "print(f\"                    EDA ANALYSIS COMPLETE\")\n",
    "print(f\"        Ready to proceed with pipeline development\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}